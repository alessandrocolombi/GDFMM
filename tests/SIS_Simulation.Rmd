---
title: "SIS Simulation Study"
author: "Alessandro Colombi"
date: "31/01/2023"
output: html_document
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  out.width = "50%"
)
```


*Dati esperimento:*

* **K = 4**
* d = 3
* $n_1 = 200$, $n_2 = 150$, $n_3 = 150$
* $\mu_1 = -20$, $\mu_2 = -10$, $\mu_3 = 0$, $\mu_4 = 15$
* $\sigma_1 = \sigma_2 = \sigma_3 = \sigma_4 = 1$
* $\pi_{11} = 1/4, \pi_{12} = 1/4, \pi_{13} = 1/4, \pi_{14} = 1/4$
* $\pi_{21} = 1/3, \pi_{22} =   0, \pi_{23} = 1/3, \pi_{24} = 1/3$
* $\pi_{31} =   0, \pi_{22} = 1/2, \pi_{23} =   0, \pi_{24} =   0$

# Partizione Fissata

```{r}
suppressWarnings(suppressPackageStartupMessages(library(GDFMM)))
suppressWarnings(suppressPackageStartupMessages(library(ACutils)))
suppressWarnings(suppressPackageStartupMessages(library(tidyverse)))
suppressWarnings(suppressPackageStartupMessages(library(RColorBrewer)))
suppressWarnings(suppressPackageStartupMessages(library(salso)))
suppressWarnings(suppressPackageStartupMessages(library(wesanderson)))
suppressWarnings(suppressPackageStartupMessages(library(mcclust.ext)))
```


```{r}
# color palette -----------------------------------------------------------
mycol = hcl.colors(n=3,palette = "Zissou1")

# data generation ---------------------------------------------------------

d = 3                   # number of groups
K = 4                   # number of global clusters
mu = c(-20,-10,0, 15)   # vectors of means
sd = c(2,1,2.5,3)         # vector of sd
n_j = c(200,100,100)#rep(200, d)        # set cardinality of the groups
mix_probs = matrix(c(2/10,3/10,1/10,4/10,
                     3/10,   0,3/10,4/10,
                        0,2/10,   0,8/10), nrow = d, ncol = K, byrow = T)
seed = 20051131

genD = simulate_data(d = d, K = K, p = mix_probs, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition

mycol_cluster = brewer.pal(n=K, name = "Dark2")

```

```{r}
counter = 1
data_per_cluster = vector("list", length = K)
log_marginal_prior = vector("list", length = d)
for(j in 1:d){
  log_marginal_prior[[j]] = vector( length = n_j[j])
  for(i in 1:n_j[j]){
    c_ji = real_partition[counter]
    data_per_cluster[[c_ji]] = c(data_per_cluster[[c_ji]], data[j,i])
    log_marginal_prior[[j]][i] = log(ACutils::dnct(data[j,i], n0 = 10, mu0 = 1, gamma0 = sqrt(2)) )
    counter = counter + 1
  }
  
}
#data_per_cluster 
mean_data_per_cluster = lapply(data_per_cluster, mean)
mean_data_per_cluster
var_data_per_cluster  = lapply(data_per_cluster, var)
var_data_per_cluster
#log_marginal_prior
```
```{r}
# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]


l_grid = 200
grid = seq(-25,15,length.out = l_grid)

for(j in 1:d){

  print(
  tibble(value = data[j,1:n_j[j]],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,1:n_j[j]])), size = 2) +
    geom_histogram(data = tibble(value = data[j,1:n_j[j]]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
  )

  idx_start = idx_end + 1
  if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}
```





Scelta iperparametri del processo

```{r}
# fK Ã¨ la funzione da ottimizzare: now it takes 3 parameters, gamma_1, gamma_2 and Lambda
fK <- function(x){
  -p_distinct_prior(k = K, n_j = n_j, 
                    gamma = c(x[1],x[1],x[1]), 
                    prior = "Poisson", lambda = x[2] )
}



optK = optim(par = rep(0.1,2), #initial points
             method = "L-BFGS-B", # that's the only method that takes bounds
             fn  = fK, #function to be minimized
             lower = rep(0.001,2) #lower bound for variables
)
optK
```

Elicito la prior per i $\gamma$ e per $\Lambda$

```{r}
# gamma
mu_gamma  = optK$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma
x_range_gamma = c(  max(0, mu_gamma-2*sqrt(var_gamma) ),
                    mu_gamma+2*sqrt(var_gamma) )
#lambda
mu_lambda  = optK$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)
x_range_lambda = c(  max(0, mu_lambda-2*sqrt(var_lambda) ),
                     mu_lambda+2*sqrt(var_lambda) )


par(mfrow = c(1,2))
plot_density(shape = a_gamma, rate = b_gamma,
             dist = "gamma", main = "gamma prior", use_x11_device = F,
             xlim = x_range_gamma)
plot_density(shape = a_lambda, rate = b_lambda,
             dist = "gamma", main = "lambda prior", use_x11_device = F,
             xlim = x_range_lambda)
```



Con i parametri scelti, plotto un'approssimazione al $99.5\%$ della massa totale della distribuzione del numero di cluster
```{r}
prob_k = rep(0,sum(n_j))
threshold = 0.995
for(kk in 1:sum(n_j) ){
  prob_k[kk] = p_distinct_prior(k = kk, n_j = n_j, 
                                gamma = rep(optK$par[1],d),
                                prior = "Poisson", lambda = optK$par[2])
  if(sum(prob_k)>=threshold)
    break
}

plot(which(prob_k>0), prob_k[prob_k>0], 
     type = 'b', pch = 16, lty = 1, lwd = 3,
     main = "Prior number of clusters", xlab = "K", ylab = "prob")
```


## Iperparametri di default

```{r}
# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]

# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra

mu0 = 0;k0= 3/10;nu0=5;sigma0=1

l_grid = 200
grid = seq(-25,15,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)

for(j in 1:d){

  print(
  tibble(value = data[j,1:n_j[j]],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,1:n_j[j]])), size = 2) +
    geom_histogram(data = tibble(value = data[j,1:n_j[j]]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(  data = tibble(grid = grid, value = Prior_grid),
                color = 'black', aes(x=grid,y=value), size = 1.2,
                inherit.aes = F)  +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
  )

  idx_start = idx_end + 1
    if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}
```


### One-Shot-Run
```{r}
# Run  --------------------------------------------------------------------
niter  <-  2000
burnin <-  2000
thin   <- 1

# tau 
#P0_hyparam = empirical_bayes_normalinvgamma(data = data)


option = set_options_marginal(
             "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = optK$par[1],"Lambda0" = optK$par[2], 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = seq(1,sum(n_j))
        )

# P0 Hyperparameters:
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,

# Process parameters
# old set:  "gamma0" = 1,"Lambda0" = 3, 
#           "alpha_gamma"=5, "beta_gamma"=0.5, "alpha_lambda"=1, "beta_lambda"=5,


#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```

```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```


```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)

arandi(binder_sara$cl,real_partition, adjust = F)
arandi(VI_sara$cl,real_partition, adjust = T)


mean( apply(part_matrix, 1, FUN = function(part_it){arandi(part_it,real_partition, adjust = T)}) )
```



```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)

# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]
for(j in 1:d){

  print(
  tibble(value = data[j,1:n_j[j]],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,1:n_j[j]])), size = 2) +
    geom_histogram(data = tibble(value = data[j,1:n_j[j]]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(data = as_tibble(t(Pred_all[[j]])), color = 'black', aes(x=grid,y=`50%`), size = 1.2,
              inherit.aes = F) +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
    geom_ribbon(data = as_tibble(t(Pred_all[[j]])), fill = mycol[1],
                aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
                alpha = 0.5, inherit.aes = F)
  )

  idx_start = idx_end + 1
    if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}

```

## Repeated Runs

Define save objects
```{r}
Nrep = 50

VI_Kest     = rep(NA,Nrep)
Binder_Kest = rep(NA,Nrep)
MC_Kest     = rep(NA,Nrep)

VI_rand_idx     = rep(NA,Nrep)
Binder_rand_idx = rep(NA,Nrep)
MC_rand_idx     = rep(NA,Nrep)

```



```{r}
seed0 = 215324
change_seed = sample(1:(Nrep+10),size = Nrep,replace = F)
for(it in 1:Nrep){
  # Data Generation
  seed = seed0 * change_seed[it]
  genD = simulate_data(d = d, K = K, p = mix_probs, mu = mu, sd = sd, n_j = n_j, seed = seed)
  data = genD$data
  real_partition = genD$real_partition
  
  # Run
  niter  <-  2000
  burnin <-  2000
  thin   <-     1
  
  option = set_options_marginal(
               "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
               "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
               "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
               "gamma0" = optK$par[1],"Lambda0" = optK$par[2], 
               "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
               "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
               "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
               "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
               "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
               "partition" = seq(1,sum(n_j))
          )
  
  GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
  
  # Posterior Analysis of clustering
  part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix
  sim_matrix <- psm(part_matrix)
  
  binder_sara = minbinder(sim_matrix)
  VI_sara = minVI(sim_matrix)
  
  # Save
  
  # MC
  MC_Kest[it] = mean(GDFMM$K)
  MC_rand_idx[it] = apply(part_matrix, 1, FUN = function(part_it){arandi(part_it,real_partition, adjust = T)})
  
  # VI
  VI_Kest[it]     = length(table(VI_sara$cl))
  VI_rand_idx[it] = arandi(VI_sara$cl,real_partition, adjust = F)
    
  # Binder
  Binder_Kest[it]     = length(table(binder_sara$cl))
  Binder_rand_idx[it] = arandi(binder_sara$cl,real_partition, adjust = F)
}

SIS_simulation = list(
  "VI_Kest" = VI_Kest,
  "Binder_Kest" = Binder_Kest,
  "MC_Kest" = MC_Kest,
  "VI_rand_idx"=VI_rand_idx,
  "Binder_rand_idx"=Binder_rand_idx,
  "MC_rand_idx"=MC_rand_idx
)
SIS_simulation
#save(SIS_simulation, file = "D:\\FiniteMixture\\SIS\\SIS_simulation1.R")
lapply(SIS_simulation, mean)
```

## Plot
```{r}
boxplot(VI_rand_idx, ylim = c(0.9,1), ylab = "Rand Index", xlab = "VI", col = mycol[1])
```


```{r}
data_bxp = tibble("rand idx" = VI_rand_idx, "type" = as.factor(rep("VI",Nrep) )) %>%
            rbind( tibble("rand idx" = Binder_rand_idx, "type" = as.factor(rep("Binder Loss",Nrep)) ))

boxplot(data_bxp$`rand idx` ~ data_bxp$type, ylim = c(0.8,1), col = mycol, pch = 16,
        ylab = "Rand index", xlab = "type")
```


```{r}
data_bxp = tibble("rand idx" = VI_rand_idx, "type" = as.factor(rep("VI",Nrep) )) %>%
            rbind( tibble("rand idx" = Binder_rand_idx, "type" = as.factor(rep("Binder Loss",Nrep)) )) %>%
            rbind( tibble("rand idx" = MC_rand_idx, "type" = as.factor(rep("MC",Nrep)) ))

boxplot(data_bxp$`rand idx` ~ data_bxp$type, ylim = c(0.8,1), col = mycol, pch = 16,
        ylab = "Rand index", xlab = "type")
```



