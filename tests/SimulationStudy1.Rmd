---
title: "Simulation Study"
author: "Alessandro Colombi"
date: "Subtitle"
output: html_document
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  out.width = "50%"
)
```


# 0 - Osservazioni



* Le medie a posteriori sono

$$\mu_k \mid \sigma^2,X_1,\dots,X_n \sim \text{N}\left(\frac{n_k}{n_k + k_0}\bar{X_k} + \frac{k_0}{n_k + k_0}\mu_0, \frac{\sigma^2}{k_0 + n_k}  \right)$$

$$\sigma^2_k \mid X_1,\dots,X_n \sim \text{InvGamma}\left(\frac{\nu_0 + n_k}{2}, \frac{1}{2}(\nu_0\sigma_0^2 + (n_k-1)V_k + \frac{n_k k_0}{n_k + k_0}(\bar{X}_k - \mu_0)^2)\right)$$

dove $n_k$ è la numerosità, $\bar{X}_k$ è la media campionaria e $V_k$ è la varianza campionaria del cluster $k$.

* Con valori di $k_0$ alti, il peso legato alla prior nella media a posteriori è molto simile a quello dato all'evidenza dei dati $\frac{n_k}{n_k + k_0} \approx  \frac{k_0}{n_k + k_0}$.

* Quindi, dalla media a posteriori mi viene da dire che sarebbe meglio avere $k_0$ basso in modo da avere la media a posteriori guidata dalla media nel cluster. 
  - In Empirical Bayes mettiamo $k_0 = \frac{\bar{\sigma^2}}{\bar{V_\mu}}$, quindi è meglio aumentare $\bar{V_\mu}$. 
  - Bello che $k_0$ sia l'unico valore a dipendere da $\bar{V_{\mu}}$, quindi posso regolarlo direttamente.
    
* Per la varianza, non voglio  $\nu_0\sigma^2_0$ troppo alto altrimenti conta solo la parte dalla prior
  - Per contenere $\nu_0$ posso aumentare $\bar{V_\sigma}$, ha anche un po' di impatto su $\sigma_0^2$. 
  - Per ridurre tutto, si può lavorare anche sul parametro di correzione: per fare matching dei momenti, metto
    $E[\sigma^2] = \bar{\sigma^2}/correction$. Seguendo il lavoro di Raf avevo sempre considerato $correction = 3$ 
    ma penso sia meglio alzarlo.
  

# 1 - Casarin-Bassetti

**Dati esperimento:**

* **K = 4**
* d = 3
* $n_1 = 100$, $n_2 = 50$, $n_3 = 50$
* $\mu_1 = -5$, $\mu_2 = 0$, $\mu_3 = 5$
* $\sigma_1 = \sigma_2 = \sigma_3 = 1$
* $\pi_{11} = 3/10, \pi_{12} = 3/10, \pi_{13} = 4/10$
* $\pi_{21} = 3/10, \pi_{22} = 7/10, \pi_{23} = 0$
* $\pi_{31} = 8/10, \pi_{32} = 1/10, \pi_{33} = 1/10$

## Data Generation

```{r}
suppressWarnings(suppressPackageStartupMessages(library(GDFMM)))
suppressWarnings(suppressPackageStartupMessages(library(ACutils)))
suppressWarnings(suppressPackageStartupMessages(library(tidyverse)))
suppressWarnings(suppressPackageStartupMessages(library(RColorBrewer)))
suppressWarnings(suppressPackageStartupMessages(library(salso)))
suppressWarnings(suppressPackageStartupMessages(library(wesanderson)))
suppressWarnings(suppressPackageStartupMessages(library(mcclust.ext)))
```


```{r}
mycol = hcl.colors(n=3,palette = "Zissou1")

d = 3                   # number of groups
K = 3                   # number of global clusters
mu = c(-5,0,5)          # vectors of means
sd = c(1,1,1)           # vector of sd
n_j = c(100,50,50)      # set cardinality of the groups

mix_probs = matrix(c(3/10,3/10,4/10,
                     3/10,7/10,   0,
                     8/10,1/10,1/10), nrow = d, ncol = K, byrow = T)
seed = 200532121

genD = simulate_data(d = d, K = K, p = mix_probs, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition

mycol_cluster = brewer.pal(n=K, name = "Dark2")

```

## Data Visualization

```{r}
idx_start = 1
idx_end = n_j[1]
xrange = c(-10,10)
l_grid = 200
grid = seq(xrange[1],xrange[2],length.out = l_grid)
           

par(mfrow = c(1,d), mar = c(2,2,1,1), bty = "l")
for(j in 1:d){
  mycol_cluster = brewer.pal(n=K, name = "Dark2")
  mycol_points  = as.factor(real_partition[idx_start:idx_end])
  levels(mycol_points) = unique(mycol_cluster)
  mycol_points = as.character(mycol_points)
  
  #dj = density(data[j,1:n_j[j]])
  
  hist(data[j,1:n_j[j]], freq = F, main = paste0("Level = ",j), 
       xlim = xrange, ylim = c(0,0.33), 
       nclass = "fd")
  
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points)
  #lines(dj$x,dj$y, lwd = 2)
  points(grid, GDFMM::dmix(x = grid, w_j = mix_probs[j,], mu_vec = mu, sigma_vec = sd),
         col = "red", lwd = 2, type = "l")
    
  idx_start = idx_end + 1
  if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}
```


## Iperparametri $P_0$

```{r}
# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra
varsig2 = 100 * var(as.vector(data), na.rm = T)^2 
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 100, varsig2 = varsig2, correction = 100)
mu0 = P0_hyparam$mu0
k0= P0_hyparam$k0
nu0=P0_hyparam$nu0
sigma0=P0_hyparam$sigma0
  
c("mu0"=mu0,"k0"=k0,"nu0"=nu0,"sigma0"=sigma0)

scale = sqrt( (k0 + 1)/(k0) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

mean_marginal
var_marginal
```


```{r}
xrange = c(-10,10)
l_grid = 200
grid = seq(xrange[1],xrange[2],length.out = l_grid)
# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)

for(j in 1:d){
  mycol_cluster = brewer.pal(n=K, name = "Dark2")
  mycol_points  = as.factor(real_partition[idx_start:idx_end])
  levels(mycol_points) = unique(mycol_cluster)
  mycol_points = as.character(mycol_points)
  
  par(mar = c(2,2,1,1), bty = "l")
  hist(data[j,1:n_j[j]], freq = F, main = paste0("Level = ",j), 
       xlim = xrange, ylim = c(0,0.5), 
       nclass = "fd")
  
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points)
  points(grid, Prior_grid, col = "black", lwd = 2, type = "l")
  
    
  idx_start = idx_end + 1
  if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}
```


### Marginali nei cluster

Fissato il valore degli iperparametri, voglio visualizzare non solo la marginale dei dati ma anche le marginali nei cluster


Calcolo media e varianza nei cluster
```{r}
counter = 1
data_per_cluster = vector("list", length = K)
log_marginal_prior = vector("list", length = d)
for(j in 1:d){
  log_marginal_prior[[j]] = vector( length = n_j[j])
  for(i in 1:n_j[j]){
    c_ji = real_partition[counter]
    data_per_cluster[[c_ji]] = c(data_per_cluster[[c_ji]], data[j,i])
    log_marginal_prior[[j]][i] = log(ACutils::dnct(data[j,i], n0 = 10, mu0 = 1, gamma0 = sqrt(2)) )
    counter = counter + 1
  }
  
}
#data_per_cluster 
mean_data_per_cluster = unlist(lapply(data_per_cluster, mean))
mean_data_per_cluster
var_data_per_cluster  = unlist(lapply(data_per_cluster, var))
var_data_per_cluster

ndata_per_cluster = unlist(lapply(data_per_cluster, length))
ndata_per_cluster
#log_marginal_prior


```

```{r}
dof_post = nu0 + ndata_per_cluster
k0_post  = k0 + ndata_per_cluster
location_post = (k0*mu0 + mean_data_per_cluster*ndata_per_cluster)/(k0 + ndata_per_cluster)
sigma0_post   = 1/dof_post*( (ndata_per_cluster - 1)*var_data_per_cluster + 
                              nu0*sigma0 + 
                              (k0*ndata_per_cluster)/(k0 + ndata_per_cluster) * (mu0 - mean_data_per_cluster)^2  
                           )

scale_post = sqrt( (sigma0_post*(k0_post + 1))/(k0_post)  )

mean_marginal_per_cluster = location_post
var_marginal_per_cluster = scale_post^2 * (dof_post)/(dof_post - 2)

mean_marginal_per_cluster
var_marginal_per_cluster
```

```{r}
xrange = c(-10,10)
l_grid = 200
grid = seq(xrange[1],xrange[2],length.out = l_grid)
idx_start = 1
idx_end = n_j[1]

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)



for(j in 1:d){
  mycol_cluster = brewer.pal(n=K, name = "Dark2")
  mycol_points  = as.factor(real_partition[idx_start:idx_end])
  levels(mycol_points) = unique(mycol_cluster)
  mycol_points = as.character(mycol_points)
  
  par(mar = c(2,2,1,1), bty = "l")
  plot(grid, Prior_grid, col = "black", lwd = 2, type = "l",
          main = paste0("Level = ",j), 
          xlim = xrange, ylim = c(0,0.5))
  
  for(kk in 1:K){
    Marginal_per_cluster_grid = GDFMM:::dnct(x = grid, 
                                         n0 = dof_post[kk], 
                                         mu0 = location_post[kk], 
                                         gamma0 = scale_post[kk])
  
    points(grid, Marginal_per_cluster_grid, col = mycol_cluster[kk], lwd = 2, type = "l")

  }
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points )
  
  idx_start = idx_end + 1
  if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}
```



## Run

```{r}
# Run  --------------------------------------------------------------------
niter  <-  10000
burnin <-  6000
thin   <-  1

# tau 
a_gamma <-   1; b_gamma  <- 1
a_lambda <- 10; b_lambda <- 2

option = set_options_marginal(
             "mu0" = mu0,"sigma0"= sigma0, "k0"= k0, "nu0"=nu0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = 3,"Lambda0" = 10, 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = real_partition#seq(1,sum(n_j))
        )

# P0 Hyperparameters:
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,

# Process parameters
# old set:  "gamma0" = 1,"Lambda0" = 3, 
#           "alpha_gamma"=5, "beta_gamma"=0.5, "alpha_lambda"=1, "beta_lambda"=5,


#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```


### Chains
```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

par(mfrow = c(1,d), mar = c(2,2,1,1))
for(j in 1:d){
  acf(GDFMM$gamma[j,], main = paste0("ACF - ",expression(gamma), "_",j))
}

par(mfrow = c(1,3), mar = c(2,2,1,1))
plot(GDFMM$gamma[1,], GDFMM$gamma[2,], pch = 16)
plot(GDFMM$gamma[1,], GDFMM$gamma[3,], pch = 16)
plot(GDFMM$gamma[2,], GDFMM$gamma[3,], pch = 16)

cor(GDFMM$gamma[1,], GDFMM$gamma[2,])
cor(GDFMM$gamma[1,], GDFMM$gamma[3,])
cor(GDFMM$gamma[2,], GDFMM$gamma[3,])

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,1), bty = "l")
plot(GDFMM$K, type = 'l', main = "K")


par(mfrow = c(1,1), bty = "l")
barplot(table(GDFMM$K), main = "Hist - K")

# ARI

part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix
ARI = apply( part_matrix, 1, 
             FUN = function(part_it){
                      arandi(part_it,real_partition, adjust = T)
                  }
            )

plot(ARI, type = 'l', main = "ARI")

```

### Clustering
```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)

arandi(binder_sara$cl,real_partition)
arandi(VI_sara$cl,real_partition)



```



### Density estimation
```{r}
xrange = c(-10,10)
l_grid = 200
grid = seq(xrange[1],xrange[2],length.out = l_grid)
# Predictive in all groups
Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)

# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]


#par(mfrow = c(1,d), mar = c(2,2,1,1))
par(mar = c(2,2,1,1), bty = "l")
for(j in 1:d){
  
  # set colors
  mycol_cluster = brewer.pal(n=K, name = "Dark2")
  mycol_points  = as.factor(real_partition[idx_start:idx_end])
  levels(mycol_points) = unique(mycol_cluster)
  mycol_points = as.character(mycol_points)
  
  
  # frequentist density
  dj = density(data[j,1:n_j[j]])
  yrange = range(Pred_all[[j]][3,]) + 0.001*max(Pred_all[[j]][3,])
  
  # Trick to add the grid below the plot
  plot.new()
  grid(lty = 1,lwd = 2,col = "gray90" )
  par(new = TRUE)
  
  # plot
  hist(data[j,1:n_j[j]], freq = F, breaks = n_j[j]/5,
       main = paste0("Level = ",j), 
       ylim = c(0,0.33), xlim = xrange,
       ylab = "y-axis", xlab = "x-axis")
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points)
  #lines(dj$x,dj$y, lwd = 3)
  #lines(x = grid, y = Pred_all[[j]][1,], col = "blue", lwd = 2, lty = 2) #0.025
  #lines(x = grid, y = Pred_all[[j]][3,], col = "blue", lwd = 2, lty = 2) #0.975
  polygon( c(grid, rev(grid)),
           c(Pred_all[[j]][1,], rev(Pred_all[[j]][3,])),
           col = ACutils::t_col(mycol[1], percent = 45))
  lines(x = grid, y = Pred_all[[j]][2,], col = mycol[1], lwd = 3) #0.5
  
  points(grid, dmix(x = grid, w_j = mix_probs[j,], mu_vec = mu, sigma_vec = sd),
         col = "red", lwd = 2, type = "l")

  idx_start = idx_end + 1
    if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}

```




## Indexes

Compute the co-clustering errors

$$
\text{CN} = \frac{1}{n} \sum_{l=1}^n\sum_{k=1}^n \ \lvert \ \pi_{lk} - \hat{\pi}_{lk} \ \rvert
$$
and the co-clustering errors star

$$
\text{CN}^* = \frac{1}{n} \sum_{l=1}^n\sum_{k=1}^n \ \lvert \ \pi_{lk} - \mathbb{I}(\hat{\pi}_{lk}>0.5) \ \rvert
$$

where $\hat{\pi}_{lk}$ is the element $l,k$ of the posterior similarity matrix and $\pi_{lk}\in\{0,1\}$ is equal to 1 if $y_l$ and $y_k$ belong to the same cluster, 0 otherwise.

```{r}
Compute_coclust_error(real_partition, sim_matrix)
```

Compute the $L_1$ distance 

$$
  SC = \frac{1}{d}\ \sum_{j=1}^{d} \ \int \ \lvert f(Y_{j,n_j+1}) - \hat{f}(Y_{j,n_j+1}\mid \mathbf{Y})  \rvert \ dY_{j,n_j+1} \
$$
```{r}

Pred_median = vector("list", length = d)
for(j in 1:d){
  Pred_median[[j]] = Pred_all[[j]][2,]
}

Compute_L1_dist(Pred = Pred_median, 
                p_mix = mix_probs, mu = mu, sigma = sd,
                grid = grid)
```


# 2 - Cremaschi

**Dati esperimento:**

* **K = 3**
* d = 2
* $n_1 = 100$, $n_2 = 100$
* $\mu_1 = -3$, $\mu_2 = 0$, $\mu_3 = 1$
* $\sigma_1 = \sqrt{0.1}$, $\sigma_2 = \sqrt{0.5}$, $\sigma_3 = \sqrt{1.5}$
* $\pi_{11} = 2/10, \pi_{12} = 8/10, \pi_{13} = 0$
* $\pi_{21} =    0, \pi_{22} = 1/10, \pi_{23} = 9/10$


## Data Generation
```{r}
mycol = hcl.colors(n=3,palette = "Zissou1")


d = 2               # number of groups
K = 3               # number of global clusters
mu = c(-3,0,1)      # vectors of means
sd = c(sqrt(0.1), sqrt(0.5), sqrt(1.5) )     # vector of sd
prob <- matrix(c(0.2,0.8,  0,
                   0,0.1,0.9),nrow = d, ncol = K, byrow = T)
  
n_j = rep(100, d)  # set cardinality of the groups
seed = 20051131

#genD = generate_data_prob(d=d,p=prob, K=K, mu = mu, sd = sd, n_j = n_j, seed = seed)
genD = simulate_data(d=d,prob=prob, K=K, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition


table(real_partition)
table(real_partition[1:n_j[1]])
table(real_partition[(n_j[1]+1):(n_j[1] + n_j[2])])


mycol_cluster = brewer.pal(n=K, name = "Accent")
```


## Data Visualization

```{r}
idx_start = 1
idx_end = n_j[1]
xrange = c(-6,6)
l_grid = 200
grid = seq(xrange[1],xrange[2],length.out = l_grid)
           

par(mfrow = c(1,d), mar = c(2,2,1,1), bty = "l")
for(j in 1:d){
  mycol_cluster = brewer.pal(n=K, name = "Dark2")
  mycol_points  = as.factor(real_partition[idx_start:idx_end])
  levels(mycol_points) = unique(mycol_cluster)
  mycol_points = as.character(mycol_points)
  #dj = density(data[j,1:n_j[j]])
  
  hist(data[j,1:n_j[j]], freq = F, main = paste0("Level = ",j), 
       xlim = xrange, ylim = c(0,0.5), 
       nclass = "fd")
  
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points)
  #lines(dj$x,dj$y, lwd = 2)
  points(grid, GDFMM::dmix(x = grid, w_j = prob[j,], mu_vec = mu, sigma_vec = sd),
         col = "red", lwd = 2, type = "l")
    
  idx_start = idx_end + 1
  if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}
```

## Iperparametri $P_0$

```{r}
xrange = c(-6,6)
l_grid = 200
grid = seq(xrange[1],xrange[2],length.out = l_grid)
# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]



# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra
P0_hyparam = empirical_bayes_normalinvgamma(data = data )
mu0 = P0_hyparam$mu0;k0= P0_hyparam$k0;nu0=P0_hyparam$nu0;sigma0=P0_hyparam$sigma0
k0 = 0.1

  
scale = sqrt( (k0 + 1)/(k0) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)

for(j in 1:d){
  mycol_cluster = brewer.pal(n=K, name = "Dark2")
  mycol_points  = as.factor(real_partition[idx_start:idx_end])
  levels(mycol_points) = unique(mycol_cluster)
  mycol_points = as.character(mycol_points)
  
  par(mar = c(2,2,1,1), bty = "l")
  hist(data[j,1:n_j[j]], freq = F, main = paste0("Level = ",j), 
       xlim = xrange, ylim = c(0,0.5), 
       nclass = "fd")
  
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points)
  points(grid, Prior_grid, col = "black", lwd = 2, type = "l")
  
    
  idx_start = idx_end + 1
  if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}
```

## Run

```{r}
# Run  --------------------------------------------------------------------
niter  <-  10000
burnin <-  6000
thin   <- 1

# tau 
a_gamma <-   1; b_gamma  <- 1
a_lambda <- 10; b_lambda <- 2

option = set_options_marginal(
             "mu0" = mu0,"sigma0"= sigma0, "k0"= k0, "nu0"=nu0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = 3,"Lambda0" = 10, 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = seq(1,sum(n_j))
        )

# P0 Hyperparameters:
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,

# Process parameters
# old set:  "gamma0" = 1,"Lambda0" = 3, 
#           "alpha_gamma"=5, "beta_gamma"=0.5, "alpha_lambda"=1, "beta_lambda"=5,


#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```


### Chains
```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

par(mfrow = c(1,d), mar = c(2,2,1,1))
for(j in 1:d){
  acf(GDFMM$gamma[j,], main = paste0("ACF - ",expression(gamma), "_",j))
}

par(mar = c(2,2,1,1), bty = "l")
plot(GDFMM$gamma[1,], GDFMM$gamma[2,], pch = 16)

cor(GDFMM$gamma[1,], GDFMM$gamma[2,])

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(bty = "l")
plot(GDFMM$K, type = 'l', main = "K")

par(mfrow = c(1,1), bty = "l")
barplot(table(GDFMM$K), main = "Hist - K")

# ARI

part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix
ARI = apply( part_matrix, 1, 
             FUN = function(part_it){
                      arandi(part_it,real_partition, adjust = T)
                  }
            )

plot(ARI, type = 'l', main = "ARI")

```

### Clustering
```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)

arandi(binder_sara$cl,real_partition)
arandi(VI_sara$cl,real_partition)


```



### Density estimation
```{r}
xrange = c(-6,6)
l_grid = 200
grid = seq(xrange[1],xrange[2],length.out = l_grid)
# Predictive in all groups
Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)

# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]


#par(mfrow = c(1,d), mar = c(2,2,1,1))
par(mar = c(2,2,1,1), bty = "l")
for(j in 1:d){
  
  # set colors
  mycol_cluster = brewer.pal(n=K, name = "Dark2")
  mycol_points  = as.factor(real_partition[idx_start:idx_end])
  levels(mycol_points) = unique(mycol_cluster)
  
  # frequentist density
  dj = density(data[j,1:n_j[j]])
  yrange = range(Pred_all[[j]][3,]) + 0.001*max(Pred_all[[j]][3,])
  
  # Trick to add the grid below the plot
  plot.new()
  grid(lty = 1,lwd = 2,col = "gray90" )
  par(new = TRUE)
  
  # plot
  hist(data[j,1:n_j[j]], freq = F, breaks = n_j[j]/5,
       main = paste0("Level = ",j), 
       ylim = c(0,0.5), xlim = xrange,
       ylab = "y-axis", xlab = "x-axis")
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points)
  #lines(dj$x,dj$y, lwd = 3)
  #lines(x = grid, y = Pred_all[[j]][1,], col = "blue", lwd = 2, lty = 2) #0.025
  #lines(x = grid, y = Pred_all[[j]][3,], col = "blue", lwd = 2, lty = 2) #0.975
  polygon( c(grid, rev(grid)),
           c(Pred_all[[j]][1,], rev(Pred_all[[j]][3,])),
           col = ACutils::t_col(mycol[1], percent = 45))
  lines(x = grid, y = Pred_all[[j]][2,], col = mycol[1], lwd = 3) #0.5
  
  points(grid, GDFMM::dmix(x = grid, w_j = prob[j,], mu_vec = mu, sigma_vec = sd),
         col = "red", lwd = 2, type = "l")

  idx_start = idx_end + 1
    if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}

```

## Analisi per gruppo

### Primo livello
```{r}
part_matrix_lv1 = part_matrix[,1:n_j[1]]

# Compute similarity matrix
sim_matrix_lv1 <- psm(part_matrix_lv1)

Kest_lv1 = apply(part_matrix_lv1, 1, FUN = function(x){
  x = as.factor(x)
  length(levels(x))
})
#table(Kest_lv1)


heatmap(sim_matrix_lv1)

par(mfrow = c(1,1), bty = "l")
barplot(table(Kest_lv1), main = "Hist - K")




```

### Secondo livello
```{r}
part_matrix_lv2 = part_matrix[,(n_j[1]+1):sum(n_j)]

Kest_lv2 = apply(part_matrix_lv2, 1, FUN = function(x){
  x = as.factor(x)
  length(levels(x))
})
#table(Kest_lv2)


# Compute similarity matrix
sim_matrix_lv2 <- psm(part_matrix_lv2)


heatmap(sim_matrix_lv2)

par(mfrow = c(1,1), bty = "l")
barplot(table(Kest_lv2), main = "Hist - K")



```

# 3 - High Dimensional


```{r}
suppressWarnings(suppressPackageStartupMessages(library(GDFMM)))
suppressWarnings(suppressPackageStartupMessages(library(ACutils)))
suppressWarnings(suppressPackageStartupMessages(library(tidyverse)))
suppressWarnings(suppressPackageStartupMessages(library(RColorBrewer)))
suppressWarnings(suppressPackageStartupMessages(library(salso)))
suppressWarnings(suppressPackageStartupMessages(library(wesanderson)))
suppressWarnings(suppressPackageStartupMessages(library(mcclust.ext)))
```

## Data Generation

```{r}
d = 100
K = 10

set.seed(27121996)
n_j = rpois(n=d, lambda = 20)
mu  = c(-5,-4,-3,-2,0,1,2.5,3,4.5,5)
sd  = sqrt(c(1,0.25,0.1,0.1,0.5,1.5,0.5,0.5,1.5,1.5))
K_j = sample(1:K,size = d, replace = T)
components_j = lapply(as.list(K_j),
                      FUN = function(kj){
                        sort(sample(1:K, size = kj, replace = F))
                      })


mix_probs = matrix(0,nrow = d, ncol = K)
for(j in 1:d){
  gammas = rgamma(n=K_j[j],shape = 0.1, rate = 1)
  mix_probs[ j,components_j[[j]]  ] = gammas/sum(gammas)
}


seed = 512541

genD = simulate_data(d = d, K = K, p = mix_probs, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition


```

## Data Visualization

Per prima cosa, voglio visualizzare le $K$ componenti

```{r}
xrange = c(-10,10)
yrange = c(0,1.25)
l_grid = 200
grid = seq(xrange[1],xrange[2],length.out = l_grid)
   

mycol_cluster = hcl.colors(n=K, palette = "Berlin")
mycol_levels  = hcl.colors(n=d, palette = "Zissou 1")
```


```{r}
par(mar = c(2,2,2,1), bty = "l")
plot(1,1,ylim = yrange, xlim = xrange, type = "n",
     main = "True component mixtures")
grid(lty = 1,lwd = 2,col = "gray90" )


for(kk in 1:K){
  comp_grid = dnorm(x=grid, mean = mu[kk], sd = sd[kk])
  points(x = grid, y = comp_grid, col = mycol_cluster[kk], 
       type = "l", lwd = 2)
}


```

```{r}

par(mar = c(2,2,2,1), bty = "l")
plot(1,1,ylim = yrange, xlim = xrange, type = "n",
     main = "True densities - all levels")
grid(lty = 1,lwd = 2,col = "gray90" )

for(j in 1:d){
  
  mix_grid = GDFMM::dmix(x = grid, w_j = mix_probs[j,], mu_vec = mu, sigma_vec = sd)
  points(x = grid, y = mix_grid, col = mycol_levels[j], 
       type = "l", lwd = 2)

}



```


## Iperparametri $P_0$

```{r}
# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra
varsig2 = 100 * var(as.vector(data), na.rm = T)^2 
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 100, varsig2 = varsig2, correction = 100)
mu0 = P0_hyparam$mu0
k0= P0_hyparam$k0
nu0=P0_hyparam$nu0
sigma0=P0_hyparam$sigma0
  
c("mu0"=mu0,"k0"=k0,"nu0"=nu0,"sigma0"=sigma0)

scale = sqrt( (k0 + 1)/(k0) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

mean_marginal
var_marginal
```

### Marginal density visualization

```{r}
Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)


par(mar = c(2,2,2,1), bty = "l")
plot(1,1,ylim = c(0,0.2), xlim = xrange, type = "n",
     main = "Marginal data Density")
grid(lty = 1,lwd = 2,col = "gray90" )


points(x = na.omit(as.numeric(data)), y = rep(0, length(na.omit(as.numeric(data)))), pch = 16)
points(grid, Prior_grid, col = "black", lwd = 2, type = "l")
  

```


### Marginali nei cluster

Fissato il valore degli iperparametri, voglio visualizzare non solo la marginale dei dati ma anche le marginali nei cluster


Calcolo media e varianza nei cluster
```{r}
counter = 1
data_per_cluster = vector("list", length = K)
log_marginal_prior = vector("list", length = d)
for(j in 1:d){
  log_marginal_prior[[j]] = vector( length = n_j[j])
  for(i in 1:n_j[j]){
    c_ji = real_partition[counter]
    data_per_cluster[[c_ji]] = c(data_per_cluster[[c_ji]], data[j,i])
    log_marginal_prior[[j]][i] = log(ACutils::dnct(data[j,i], n0 = 10, mu0 = 1, gamma0 = sqrt(2)) )
    counter = counter + 1
  }
  
}
#data_per_cluster 
mean_data_per_cluster = unlist(lapply(data_per_cluster, mean))
mean_data_per_cluster
var_data_per_cluster  = unlist(lapply(data_per_cluster, var))
var_data_per_cluster

ndata_per_cluster = unlist(lapply(data_per_cluster, length))
ndata_per_cluster
#log_marginal_prior


```

```{r}
dof_post = nu0 + ndata_per_cluster
k0_post  = k0 + ndata_per_cluster
location_post = (k0*mu0 + mean_data_per_cluster*ndata_per_cluster)/(k0 + ndata_per_cluster)
sigma0_post   = 1/dof_post*( (ndata_per_cluster - 1)*var_data_per_cluster + 
                              nu0*sigma0 + 
                              (k0*ndata_per_cluster)/(k0 + ndata_per_cluster) * (mu0 - mean_data_per_cluster)^2  
                           )

scale_post = sqrt( (sigma0_post*(k0_post + 1))/(k0_post)  )

mean_marginal_per_cluster = location_post
var_marginal_per_cluster = scale_post^2 * (dof_post)/(dof_post - 2)

mean_marginal_per_cluster
var_marginal_per_cluster
```

## Run

```{r}
# Run  --------------------------------------------------------------------
niter  <-  10000
burnin <-  1#6000
thin   <-  1

# tau 
a_gamma <-   1; b_gamma  <- 1
a_lambda <- 10; b_lambda <- 2

option = set_options_marginal(
             "mu0" = mu0,"sigma0"= sigma0, "k0"= k0, "nu0"=nu0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = 3,"Lambda0" = 10, 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = real_partition#seq(1,sum(n_j))
        )

# P0 Hyperparameters:
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,

# Process parameters
# old set:  "gamma0" = 1,"Lambda0" = 3, 
#           "alpha_gamma"=5, "beta_gamma"=0.5, "alpha_lambda"=1, "beta_lambda"=5,


#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```


### Chains
```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# K
par(mfrow = c(1,1), bty = "l")
plot(GDFMM$K, type = 'l', main = "K")


par(mfrow = c(1,1), bty = "l")
barplot(table(GDFMM$K), main = "Hist - K")

# ARI

part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix
ARI = apply( part_matrix, 1, 
             FUN = function(part_it){
                      arandi(part_it,real_partition, adjust = T)
                  }
            )

plot(ARI, type = 'l', main = "ARI")

```

### Clustering
```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)

arandi(binder_sara$cl,real_partition)
arandi(VI_sara$cl,real_partition)



```

