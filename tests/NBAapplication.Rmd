---
title: "NBA Application"
author: "Alessandro Colombi"
date: "15/6/2022"
output: pdf_document
---


```{r 11_setup, include=FALSE, purl=FALSE}
knitr::opts_chunk$set(eval = T)
```

```{r message=FALSE, include=TRUE}
library(GDFMM)
library(ACutils)
library(tidyverse)
```
# Load data

Set directory and read data data
```{r}
setwd('C:/Users/colom/OneDrive/GDFMM/tests') 
data <- read.csv("C:/Users/colom/OneDrive/GDFMM/tests/nba_salaries_with_position.csv")
```


```{r}
data = as_tibble(data)

# Divido per 10^6
# data = data %>%
#        mutate(position = as.factor(position)) %>%
#        mutate(salary = as.double(salary)/10^6) %>%
#        filter(player %in% unique(player)) #perdo i fratelli

#log scale
data = data %>%
       mutate(position = as.factor(position)) %>%
       mutate(salary = log(as.double(salary))) %>%
       filter(player %in% unique(player)) #perdo i fratelli

# get levels
data_PG = filter(data, position == "PG") %>% select(player, salary)
data_SG = filter(data, position == "SG") %>% select(player, salary)
data_SF = filter(data, position == "SF") %>% select(player, salary)
data_PF = filter(data, position == "PF") %>% select(player, salary)
data_C = filter(data, position == "C") %>% select(player, salary)

n = nrow(data)
n_j = c( nrow(data_PG), nrow(data_SG), nrow(data_SF), nrow(data_PF), nrow(data_C) )
d = length(n_j) #5

# jittering
data_PG$salary = data_PG$salary  + rnorm(n = n_j[1],sd=0.001)
data_SG$salary  = data_SG$salary  + rnorm(n = n_j[2],sd=0.001)
data_SF$salary  = data_SF$salary  + rnorm(n = n_j[3],sd=0.001)
data_PF$salary  = data_PF$salary  + rnorm(n = n_j[4],sd=0.001)
data_C$salary  = data_C$salary  + rnorm(n = n_j[5],sd=0.001)

n_max = max(n_j)
data_matrix = matrix(NA, nrow = d, ncol = n_max) # d x max(n_j) matrix

data_matrix[1,1:n_j[1]] = data_PG$salary
data_matrix[2,1:n_j[2]] = data_SG$salary
data_matrix[3,1:n_j[3]] = data_SF$salary
data_matrix[4,1:n_j[4]] = data_PF$salary
data_matrix[5,1:n_j[5]] = data_C$salary
```

Data visualization
```{r}
d_PG = density(data_PG$salary)
hist(data_PG$salary, freq = F, breaks = 30, col = ACutils::t_col("darkred", 70), 
     xlim = range(data_PG$salary), main = "PG")
points(x = data_PG$salary, y = rep(0, length(data_PG$salary)), pch = 16, col = ACutils::t_col("darkred", 10))
points(x = d_PG$x, y = d_PG$y, col = "darkred", type = 'l', lty = 1, lwd = 2)


d_SG = density(data_SG$salary)
hist(data_SG$salary, freq = F, breaks = 30, col = ACutils::t_col("darkred", 70), 
     xlim = range(data_SG$salary), main = "SG")
points(x = data_SG$salary, y = rep(0, length(data_SG$salary)), pch = 16, col = ACutils::t_col("darkred", 10))
points(x = d_SG$x, y = d_SG$y, col = "darkred", type = 'l', lty = 1, lwd = 2)

d_SF = density(data_SF$salary)
hist(data_SF$salary, freq = F, breaks = 30, col = ACutils::t_col("darkred", 70), 
     xlim = range(data_SF$salary), main = "SF")
points(x = data_SF$salary, y = rep(0, length(data_SF$salary)), pch = 16, col = ACutils::t_col("darkred", 10))
points(x = d_SF$x, y = d_SF$y, col = "darkred", type = 'l', lty = 1, lwd = 2)

d_PF = density(data_PF$salary)
hist(data_PF$salary, freq = F, breaks = 30, col = ACutils::t_col("darkred", 70), 
     xlim = range(data_PF$salary), main = "PF")
points(x = data_PF$salary, y = rep(0, length(data_PF$salary)), pch = 16, col = ACutils::t_col("darkred", 10))
points(x = d_PF$x, y = d_PF$y, col = "darkred", type = 'l', lty = 1, lwd = 2)

d_C = density(data_C$salary)
hist(data_C$salary, freq = F, breaks = 30, col = ACutils::t_col("darkred", 70), 
     xlim = range(data_C$salary), main = "C")
points(x = data_C$salary, y = rep(0, length(data_C$salary)), pch = 16, col = ACutils::t_col("darkred", 10))
points(x = d_PG$x, y = d_C$y, col = "darkred", type = 'l', lty = 1, lwd = 2)

```
# RUN - Marginal sampler


Seguo le indicazioni della sezione precedente
```{r}
data = data_matrix

idx_start = 1
idx_end = n_j[1]

# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 100, varsig2 = 100, correction = 10)
mu0 = P0_hyparam$mu0;k0= P0_hyparam$k0;nu0=P0_hyparam$nu0;sigma0=P0_hyparam$sigma0

l_grid = 200
grid = seq(2,3,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)

for(j in 1:d){

  print(
  tibble(value = data[j,1:n_j[j]]
         ) %>%
    ggplot(aes(x=value)) +
    geom_point(y = rep(0.005,length(data[j,1:n_j[j]])), size = 2) +
    geom_histogram(data = tibble(value = data[j,1:n_j[j]]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    geom_path(  data = tibble(grid = grid, value = Prior_grid),
                color = 'black', aes(x=grid,y=value), size = 1.2,
                inherit.aes = F)  +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
  )

  idx_start = idx_end + 1
  if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}
```


Scelta iperparametri del processo

```{r}
# fK Ã¨ la funzione da ottimizzare: now it takes 3 parameters, gamma_1, gamma_2 and Lambda
fK <- function(x){
  -p_distinct_prior(k = 3, n_j = n_j, 
                    gamma = c(x[1],x[1],x[1],x[1],x[1]), 
                    prior = "Poisson", lambda = x[2] )
}



optK = optim(par = rep(0.1,2), #initial points
             method = "L-BFGS-B", # that's the only method that takes bounds
             fn  = fK, #function to be minimized
             lower = rep(0.001,2) #lower bound for variables
)
optK
```

Elicito la prior per i $\gamma$ e per $\Lambda$

```{r}
# gamma
mu_gamma  = optK$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma
x_range_gamma = c(  max(0, mu_gamma-2*sqrt(var_gamma) ),
                    mu_gamma+2*sqrt(var_gamma) )
#lambda
mu_lambda  = optK$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)
x_range_lambda = c(  max(0, mu_lambda-2*sqrt(var_lambda) ),
                     mu_lambda+2*sqrt(var_lambda) )


par(mfrow = c(1,2))
plot_density(shape = a_gamma, rate = b_gamma,
             dist = "gamma", main = "gamma prior", use_x11_device = F,
             xlim = x_range_gamma)
plot_density(shape = a_lambda, rate = b_lambda,
             dist = "gamma", main = "lambda prior", use_x11_device = F,
             xlim = x_range_lambda)
```



Con i parametri scelti, plotto un'approssimazione al $99.5\%$ della massa totale della distribuzione del numero di cluster
```{r}
prob_k = rep(0,sum(n_j))
threshold = 0.995
for(kk in 1:sum(n_j) ){
  prob_k[kk] = p_distinct_prior(k = kk, n_j = n_j, 
                                gamma = rep(optK$par[1],d),
                                prior = "Poisson", lambda = optK$par[2])
  if(sum(prob_k)>=threshold)
    break
}

plot(which(prob_k>0), prob_k[prob_k>0], 
     type = 'b', pch = 16, lty = 1, lwd = 3,
     main = "Prior number of clusters", xlab = "K", ylab = "prob")
```

```{r}
# Run  --------------------------------------------------------------------
niter  <-  2000
burnin <-  2000
thin   <- 1

# tau 
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 100, varsig2 = 100, correction = 15)


# gamma
mu_gamma  = optK$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma

#lambda
mu_lambda  = optK$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)


option = set_options_marginal(
             "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = optK$par[1],"Lambda0" = optK$par[2], 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = seq(1:sum(n_j))#real_partition
        )

# P0 Hyperparameters:
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,

# Process parameters
# old set:  "gamma0" = 1,"Lambda0" = 3, 
#           "alpha_gamma"=5, "beta_gamma"=0.5, "alpha_lambda"=1, "beta_lambda"=5,


#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```

```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```

```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)
```


```{r}
mycol = hcl.colors(n=3,palette = "Zissou1")

l_grid = 200
grid = seq(2,4,length.out = l_grid)

# Predictive in all groups
Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)

# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]
for(j in 1:d){

  print(
  tibble(value = data[j,1:n_j[j]]
         ) %>%
    ggplot(aes(x=value)) +
    geom_point(y = rep(0.005,length(data[j,1:n_j[j]])), size = 2) +
    geom_histogram(data = tibble(value = data[j,1:n_j[j]]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    geom_path(data = as_tibble(t(Pred_all[[j]])), color = 'black', aes(x=grid,y=`50%`), size = 1.2,
              inherit.aes = F) +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
    geom_ribbon(data = as_tibble(t(Pred_all[[j]])), fill = mycol[1],
                aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
                alpha = 0.5, inherit.aes = F)
  )

  idx_start = idx_end + 1
    if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}

```

**Riassunto**




# RUN - Conditional sampler
```{r}

niter  <- 5000
burnin <- 0
thin   <- 1


option = set_options(
             "nu" = 1, "Mstar0" = 2, "Lambda0" = 3, "mu0" = 0,"sigma0"= 1, "gamma0" = 1,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, "Adapt_MH_power_lim"=10, "Adapt_MH_var0"=1,
             "k0"= 1/10, "nu0"=10, "alpha_gamma"=1,
             "beta_gamma"=1, "alpha_lambda"=100, "beta_lambda"=1/100,
             "UpdateU" = T, "UpdateM" = T, "UpdateGamma" = T, "UpdateS" = T,
             "UpdateTau" = T, "UpdateLambda" = T
        )

GDFMM = GDFMM_sampler(data_matrix, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```
Or load data
```{r}
load('GDFMM_NBA_test1.Rdat')
```




```{r}
#K
plot(GDFMM$K[3000:5000], type = 'l', main = "K")
hist(GDFMM$K[3000:5000], main = "K")

#Mstar
plot(GDFMM$Mstar[3000:5000], type = 'l', main = "Mstar")

# Predictive --------------------------------------------------------------


l_grid = 100
grid = seq(0,20,length.out = l_grid)

# Predictive in all groups
Pred_all = predictive_all_groups(grid = grid, fit = GDFMM, burnin = 4000)
# load 
load('Pred_all_NBA_test1.Rdat')



par(mfrow = c(2,3))
for(j in 1:d){
  hist(data_matrix[j,], freq = F, breaks = l_grid/10, col = ACutils::t_col("darkred", 70), xlim = c(10,17), #xlim = range(grid),
             main = paste0("level = ",j))
  matplot(x = grid, y = t(Pred_all[[j]]), type = 'l', col = 'black', lty = 1, lwd = 2, add = T)
  points(x = data_matrix[j,], y = rep(0, length(data_matrix[j,])), pch = 16, col = ACutils::t_col("darkred", 10))

}
```












