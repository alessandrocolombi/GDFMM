---
title: "Simulation Study - Cremaschi"
author: "Alessandro Colombi"
date: "27/1/2023"
output: html_document
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  out.width = "50%"
)
```

# 00 - Empirical Bayes

We use an Empirical Bayes strategy to choose $P_0$ hyperparameters $(\mu_0,\kappa_0,\nu_0,\sigma^2_0)$. Let $\hat{y}_n$ be the sample mean of all data points and let $\hat{s}^2_n$ be their variance.

The strategy we proposed requires 3 additional parameters to be specified

* $\bar{\text{V}}_\mu$: a priori variance for $\mu$
* $\bar{\text{V}}_{\sigma^2}$: a priori variance for $\sigma^2$
* $\text{c}$: extra parameter I can not interpret

$$
\text{E}[\mu] = \hat{y}_n \\
\text{Var}(\mu) = \bar{\text{V}}_\mu \\
\text{E}[\sigma^2] = \hat{s}^2_n / \text{c}\\
\text{Var}[\sigma^2] = \bar{\text{V}}_{\sigma^2} 
$$


# 1 - Reference example

**Dati esperimento:**

* **K = 3**
* d = 2
* $n_1 = 100$, $n_2 = 100$
* $\mu_1 = -3$, $\mu_2 = 0$, $\mu_3 = 1$
* $\sigma_1 = \sqrt{0.1}$, $\sigma_2 = \sqrt{0.5}$, $\sigma_3 = \sqrt{1.5}$
* $\pi_{11} = 2/10, \pi_{12} = 8/10, \pi_{13} = 0$
* $\pi_{21} =    0, \pi_{22} = 1/10, \pi_{23} = 9/10$

## Data Generation

```{r}
suppressWarnings(suppressPackageStartupMessages(library(GDFMM)))
suppressWarnings(suppressPackageStartupMessages(library(ACutils)))
suppressWarnings(suppressPackageStartupMessages(library(tidyverse)))
suppressWarnings(suppressPackageStartupMessages(library(RColorBrewer)))
suppressWarnings(suppressPackageStartupMessages(library(salso)))
suppressWarnings(suppressPackageStartupMessages(library(wesanderson)))
suppressWarnings(suppressPackageStartupMessages(library(mcclust.ext)))
```

```{r}
d = 2               # number of groups
K = 3               # number of global clusters
mu = c(-3,0,1)      # vectors of means
sd = c(sqrt(0.1), sqrt(0.5), sqrt(1.5) )     # vector of sd
prob <- matrix(c(0.2,0.8,  0,
                   0,0.1,0.9),nrow = d, ncol = K, byrow = T)
  
n_j = rep(100, d)  # set cardinality of the groups
seed = 20051131

#genD = generate_data_prob(d=d,p=prob, K=K, mu = mu, sd = sd, n_j = n_j, seed = seed)
genD = simulate_data(d=d,prob=prob, K=K, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition


#table(real_partition)
#table(real_partition[1:n_j[1]])
#table(real_partition[(n_j[1]+1):(n_j[1] + n_j[2])])


mycol = hcl.colors(n=3,palette = "Zissou1")
mycol_cluster = brewer.pal(n=K, name = "Dark2")
```


## Data Visualization

```{r}
idx_start = 1
idx_end = n_j[1]
xrange = c(-7,7)
l_grid = 200
grid = seq(xrange[1],xrange[2],length.out = l_grid)
           

par(mfrow = c(1,d), mar = c(2,2,1,1), bty = "l")
for(j in 1:d){
  mycol_cluster = brewer.pal(n=K, name = "Dark2")
  mycol_points  = as.factor(real_partition[idx_start:idx_end])
  levels(mycol_points) = unique(mycol_cluster)
  mycol_points = as.character(mycol_points)
  
  #dj = density(data[j,1:n_j[j]])
  
  hist(data[j,1:n_j[j]], freq = F, main = paste0("Level = ",j), 
       xlim = xrange, ylim = c(0,0.5), 
       nclass = "fd")
  
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points)
  #lines(dj$x,dj$y, lwd = 2)
  points(grid, GDFMM::dmix(x = grid, w_j = prob[j,], mu_vec = mu, sigma_vec = sd),
         col = "red", lwd = 2, type = "l")
    
  idx_start = idx_end + 1
  if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}
```


## Iperparametri $P_0$

```{r}
# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra
vardata = var(as.vector(data), na.rm = T)
P0_hyparam = empirical_bayes_normalinvgamma(data = data )

mu0 = P0_hyparam$mu0
k0 = 0.1
#k0= P0_hyparam$k0
nu0=P0_hyparam$nu0
sigma0=P0_hyparam$sigma0
  
c("mu0"=mu0,"k0"=k0,"nu0"=nu0,"sigma0"=sigma0)

scale = sqrt( (k0 + 1)/(k0) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

mean_marginal
var_marginal
```


```{r}
xrange = c(-7,7)
l_grid = 200
grid = seq(xrange[1],xrange[2],length.out = l_grid)
# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)

for(j in 1:d){
  mycol_cluster = brewer.pal(n=K, name = "Dark2")
  mycol_points  = as.factor(real_partition[idx_start:idx_end])
  levels(mycol_points) = unique(mycol_cluster)
  mycol_points = as.character(mycol_points)
  
  par(mar = c(2,2,1,1), bty = "l")
  hist(data[j,1:n_j[j]], freq = F, main = paste0("Level = ",j), 
       xlim = xrange, ylim = c(0,0.5), 
       nclass = "fd")
  
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points)
  points(grid, Prior_grid, col = "black", lwd = 2, type = "l")
  
    
  idx_start = idx_end + 1
  if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}
```


### Marginali nei cluster

Fissato il valore degli iperparametri, voglio visualizzare non solo la marginale dei dati ma anche le marginali nei cluster


Calcolo media e varianza nei cluster
```{r}
counter = 1
data_per_cluster = vector("list", length = K)
log_marginal_prior = vector("list", length = d)
for(j in 1:d){
  log_marginal_prior[[j]] = vector( length = n_j[j])
  for(i in 1:n_j[j]){
    c_ji = real_partition[counter]
    data_per_cluster[[c_ji]] = c(data_per_cluster[[c_ji]], data[j,i])
    log_marginal_prior[[j]][i] = log(ACutils::dnct(data[j,i], n0 = 10, mu0 = 1, gamma0 = sqrt(2)) )
    counter = counter + 1
  }
  
}
#data_per_cluster 
mean_data_per_cluster = unlist(lapply(data_per_cluster, mean))
mean_data_per_cluster
var_data_per_cluster  = unlist(lapply(data_per_cluster, var))
var_data_per_cluster

ndata_per_cluster = unlist(lapply(data_per_cluster, length))
ndata_per_cluster
#log_marginal_prior


```

```{r}
dof_post = nu0 + ndata_per_cluster
k0_post  = k0 + ndata_per_cluster
location_post = (k0*mu0 + mean_data_per_cluster*ndata_per_cluster)/(k0 + ndata_per_cluster)
sigma0_post   = 1/dof_post*( (ndata_per_cluster - 1)*var_data_per_cluster + 
                              nu0*sigma0 + 
                              (k0*ndata_per_cluster)/(k0 + ndata_per_cluster) * (mu0 - mean_data_per_cluster)^2  
                           )

scale_post = sqrt( (sigma0_post*(k0_post + 1))/(k0_post)  )

mean_marginal_per_cluster = location_post
var_marginal_per_cluster = scale_post^2 * (dof_post)/(dof_post - 2)

mean_marginal_per_cluster
var_marginal_per_cluster
```

```{r}
xrange = c(-7,7)
l_grid = 200
grid = seq(xrange[1],xrange[2],length.out = l_grid)
idx_start = 1
idx_end = n_j[1]

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)



for(j in 1:d){
  mycol_cluster = brewer.pal(n=K, name = "Dark2")
  mycol_points  = as.factor(real_partition[idx_start:idx_end])
  levels(mycol_points) = unique(mycol_cluster)
  mycol_points = as.character(mycol_points)
  
  par(mar = c(2,2,1,1), bty = "l")
  plot(grid, Prior_grid, col = "black", lwd = 2, type = "l",
          main = paste0("Level = ",j), 
          xlim = xrange, ylim = c(0,1))
  
  for(kk in 1:K){
    Marginal_per_cluster_grid = GDFMM:::dnct(x = grid, 
                                         n0 = dof_post[kk], 
                                         mu0 = location_post[kk], 
                                         gamma0 = scale_post[kk])
  
    points(grid, Marginal_per_cluster_grid, col = mycol_cluster[kk], lwd = 2, type = "l")

  }
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points )
  
  idx_start = idx_end + 1
  if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}
```



## Run

```{r}
# Run  --------------------------------------------------------------------
niter  <-  10000
burnin <-  6000
thin   <-  1

# tau 
a_gamma <-   1; b_gamma  <- 1
a_lambda <- 10; b_lambda <- 2

option = set_options_marginal(
             "mu0" = mu0,"sigma0"= sigma0, "k0"= k0, "nu0"=nu0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = 3,"Lambda0" = 10, 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = real_partition#seq(1,sum(n_j))
        )

# P0 Hyperparameters:
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,

# Process parameters
# old set:  "gamma0" = 1,"Lambda0" = 3, 
#           "alpha_gamma"=5, "beta_gamma"=0.5, "alpha_lambda"=1, "beta_lambda"=5,


#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```


### Chains
```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

par(mfrow = c(1,d), mar = c(2,2,1,1))
for(j in 1:d){
  acf(GDFMM$gamma[j,], main = paste0("ACF - ",expression(gamma), "_",j))
}

par(mar = c(2,2,1,1))
plot(GDFMM$gamma[1,], GDFMM$gamma[2,], pch = 16)

cor(GDFMM$gamma[1,], GDFMM$gamma[2,])

par(mfrow = c(1,d), mar = c(2,2,1,1))
for(j in 1:d){
  hist(GDFMM$gamma[j,], nclass = "fd", freq = FALSE, main = paste0("gamma_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,1), bty = "l")
plot(GDFMM$K, type = 'l', main = "K")


par(mfrow = c(1,1), bty = "l")
barplot(table(GDFMM$K), main = "Hist - K")

# ARI

part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix
ARI = apply( part_matrix, 1, 
             FUN = function(part_it){
                      arandi(part_it,real_partition, adjust = T)
                  }
            )

plot(ARI, type = 'l', main = "ARI")

```

### Clustering
```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)

arandi(binder_sara$cl,real_partition)
arandi(VI_sara$cl,real_partition)



```



### Density estimation
```{r}
xrange = c(-7,7)
l_grid = 200
grid = seq(xrange[1],xrange[2],length.out = l_grid)
# Predictive in all groups
Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)

# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]


#par(mfrow = c(1,d), mar = c(2,2,1,1))
par(mar = c(2,2,1,1), bty = "l")
for(j in 1:d){
  
  # set colors
  mycol_cluster = brewer.pal(n=K, name = "Dark2")
  mycol_points  = as.factor(real_partition[idx_start:idx_end])
  levels(mycol_points) = unique(mycol_cluster)
  mycol_points = as.character(mycol_points)
  
  
  # frequentist density
  dj = density(data[j,1:n_j[j]])
  yrange = range(Pred_all[[j]][3,]) + 0.001*max(Pred_all[[j]][3,])
  
  # Trick to add the grid below the plot
  plot.new()
  grid(lty = 1,lwd = 2,col = "gray90" )
  par(new = TRUE)
  
  # plot
  hist(data[j,1:n_j[j]], freq = F, breaks = n_j[j]/5,
       main = paste0("Level = ",j), 
       ylim = c(0,0.5), xlim = xrange,
       ylab = "y-axis", xlab = "x-axis")
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points)
  #lines(dj$x,dj$y, lwd = 3)
  #lines(x = grid, y = Pred_all[[j]][1,], col = "blue", lwd = 2, lty = 2) #0.025
  #lines(x = grid, y = Pred_all[[j]][3,], col = "blue", lwd = 2, lty = 2) #0.975
  polygon( c(grid, rev(grid)),
           c(Pred_all[[j]][1,], rev(Pred_all[[j]][3,])),
           col = ACutils::t_col(mycol[1], percent = 45))
  lines(x = grid, y = Pred_all[[j]][2,], col = mycol[1], lwd = 3) #0.5
  
  points(grid, dmix(x = grid, w_j = prob[j,], mu_vec = mu, sigma_vec = sd),
         col = "red", lwd = 2, type = "l")

  idx_start = idx_end + 1
    if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}

```




## Indexes

Compute the co-clustering errors

$$
\text{CN} = \frac{1}{n} \sum_{l=1}^n\sum_{k=1}^n \ \lvert \ \pi_{lk} - \hat{\pi}_{lk} \ \rvert
$$
and the co-clustering errors star

$$
\text{CN}^* = \frac{1}{n} \sum_{l=1}^n\sum_{k=1}^n \ \lvert \ \pi_{lk} - \mathbb{I}(\hat{\pi}_{lk}>0.5) \ \rvert
$$

where $\hat{\pi}_{lk}$ is the element $l,k$ of the posterior similarity matrix and $\pi_{lk}\in\{0,1\}$ is equal to 1 if $y_l$ and $y_k$ belong to the same cluster, 0 otherwise.

```{r}
Compute_coclust_error(real_partition, sim_matrix)
```

Compute the $L_1$ distance 

$$
  SC = \frac{1}{d}\ \sum_{j=1}^{d} \ \int \ \lvert f(Y_{j,n_j+1}) - \hat{f}(Y_{j,n_j+1}\mid \mathbf{Y})  \rvert \ dY_{j,n_j+1} \
$$
```{r}

Pred_median = vector("list", length = d)
for(j in 1:d){
  Pred_median[[j]] = Pred_all[[j]][2,]
}

Compute_L1_dist(Pred = Pred_median, 
                p_mix = prob, mu = mu, sigma = sd,
                grid = grid)
```

Casarin calcola anche mediana e varianza della catena del numero di cluster
```{r}
median(GDFMM$K)
var(GDFMM$K)
```

# 2 - Tests

## Iperparametri $P_0$

```{r}
# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra
vardata = var(as.vector(data), na.rm = T)
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 100, correction = 3, varsig2 = 100  )

mu0 = P0_hyparam$mu0
k0= P0_hyparam$k0
nu0=P0_hyparam$nu0
sigma0=P0_hyparam$sigma0
  
c("mu0"=mu0,"k0"=k0,"nu0"=nu0,"sigma0"=sigma0)

scale = sqrt( (k0 + 1)/(k0) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

mean_marginal
var_marginal
```


```{r}
xrange = c(-7,7)
l_grid = 200
grid = seq(xrange[1],xrange[2],length.out = l_grid)
# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)

for(j in 1:d){
  mycol_cluster = brewer.pal(n=K, name = "Dark2")
  mycol_points  = as.factor(real_partition[idx_start:idx_end])
  levels(mycol_points) = unique(mycol_cluster)
  mycol_points = as.character(mycol_points)
  
  par(mar = c(2,2,1,1), bty = "l")
  hist(data[j,1:n_j[j]], freq = F, main = paste0("Level = ",j), 
       xlim = xrange, ylim = c(0,0.5), 
       nclass = "fd")
  
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points)
  points(grid, Prior_grid, col = "black", lwd = 2, type = "l")
  
    
  idx_start = idx_end + 1
  if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}
```


### Marginali nei cluster

Fissato il valore degli iperparametri, voglio visualizzare non solo la marginale dei dati ma anche le marginali nei cluster


Calcolo media e varianza nei cluster
```{r}
counter = 1
data_per_cluster = vector("list", length = K)
log_marginal_prior = vector("list", length = d)
for(j in 1:d){
  log_marginal_prior[[j]] = vector( length = n_j[j])
  for(i in 1:n_j[j]){
    c_ji = real_partition[counter]
    data_per_cluster[[c_ji]] = c(data_per_cluster[[c_ji]], data[j,i])
    log_marginal_prior[[j]][i] = log(ACutils::dnct(data[j,i], n0 = 10, mu0 = 1, gamma0 = sqrt(2)) )
    counter = counter + 1
  }
  
}
#data_per_cluster 
mean_data_per_cluster = unlist(lapply(data_per_cluster, mean))
mean_data_per_cluster
var_data_per_cluster  = unlist(lapply(data_per_cluster, var))
var_data_per_cluster

ndata_per_cluster = unlist(lapply(data_per_cluster, length))
ndata_per_cluster
#log_marginal_prior


```

```{r}
dof_post = nu0 + ndata_per_cluster
k0_post  = k0 + ndata_per_cluster
location_post = (k0*mu0 + mean_data_per_cluster*ndata_per_cluster)/(k0 + ndata_per_cluster)
sigma0_post   = 1/dof_post*( (ndata_per_cluster - 1)*var_data_per_cluster + 
                              nu0*sigma0 + 
                              (k0*ndata_per_cluster)/(k0 + ndata_per_cluster) * (mu0 - mean_data_per_cluster)^2  
                           )

scale_post = sqrt( (sigma0_post*(k0_post + 1))/(k0_post)  )

mean_marginal_per_cluster = location_post
var_marginal_per_cluster = scale_post^2 * (dof_post)/(dof_post - 2)

mean_marginal_per_cluster
var_marginal_per_cluster
```

```{r}
xrange = c(-7,7)
l_grid = 200
grid = seq(xrange[1],xrange[2],length.out = l_grid)
idx_start = 1
idx_end = n_j[1]

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)



for(j in 1:d){
  mycol_cluster = brewer.pal(n=K, name = "Dark2")
  mycol_points  = as.factor(real_partition[idx_start:idx_end])
  levels(mycol_points) = unique(mycol_cluster)
  mycol_points = as.character(mycol_points)
  
  par(mar = c(2,2,1,1), bty = "l")
  plot(grid, Prior_grid, col = "black", lwd = 2, type = "l",
          main = paste0("Level = ",j), 
          xlim = xrange, ylim = c(0,1))
  
  for(kk in 1:K){
    Marginal_per_cluster_grid = GDFMM:::dnct(x = grid, 
                                         n0 = dof_post[kk], 
                                         mu0 = location_post[kk], 
                                         gamma0 = scale_post[kk])
  
    points(grid, Marginal_per_cluster_grid, col = mycol_cluster[kk], lwd = 2, type = "l")

  }
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points )
  
  idx_start = idx_end + 1
  if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}
```



## Run

```{r}
# Run  --------------------------------------------------------------------
niter  <-  10000
burnin <-   6000
thin   <-  1

# tau 
a_gamma <-   1; b_gamma  <- 100

# Lambda hyperparameters
mu_lambda  = 3
var_lambda = 1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)
#a_lambda <- 10; b_lambda <- 2

option = set_options_marginal(
             "mu0" = mu0,"sigma0"= sigma0, "k0"= k0, "nu0"=nu0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = 3,"Lambda0" = mu_lambda, 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = NULL#seq(1,sum(n_j))#real_partition
        )

# P0 Hyperparameters:
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,

# Process parameters
# old set:  "gamma0" = 1,"Lambda0" = 3, 
#           "alpha_gamma"=5, "beta_gamma"=0.5, "alpha_lambda"=1, "beta_lambda"=5,


#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```


### Chains
```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

par(mfrow = c(1,d), mar = c(2,2,1,1))
for(j in 1:d){
  acf(GDFMM$gamma[j,], main = paste0("ACF - ",expression(gamma), "_",j))
}

par(mfrow = c(1,1), mar = c(2,2,1,1))
plot(GDFMM$gamma[1,], GDFMM$gamma[2,], pch = 16)

cor(GDFMM$gamma[1,], GDFMM$gamma[2,])

par(mfrow = c(1,d), mar = c(2,2,1,1))
for(j in 1:d){
  hist(GDFMM$gamma[j,], nclass = "fd", freq = FALSE, main = paste0("gamma_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,1), bty = "l")
plot(GDFMM$K, type = 'l', main = "K")


par(mfrow = c(1,1), bty = "l")
barplot(table(GDFMM$K), main = "Hist - K")

# ARI

part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix
ARI = apply( part_matrix, 1, 
             FUN = function(part_it){
                      arandi(part_it,real_partition)
                  }
            )

plot(ARI, type = 'l', main = "ARI")

```

### Clustering
```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)

arandi(binder_sara$cl,real_partition)
arandi(VI_sara$cl,real_partition)



```



### Density estimation
```{r}
xrange = c(-7,7)
l_grid = 200
grid = seq(xrange[1],xrange[2],length.out = l_grid)
# Predictive in all groups
Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)

# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]


#par(mfrow = c(1,d), mar = c(2,2,1,1))
par(mar = c(2,2,1,1), bty = "l")
for(j in 1:d){
  
  # set colors
  mycol_cluster = brewer.pal(n=K, name = "Dark2")
  mycol_points  = as.factor(real_partition[idx_start:idx_end])
  levels(mycol_points) = unique(mycol_cluster)
  mycol_points = as.character(mycol_points)
  
  
  # frequentist density
  dj = density(data[j,1:n_j[j]])
  yrange = range(Pred_all[[j]][3,]) + 0.001*max(Pred_all[[j]][3,])
  
  # Trick to add the grid below the plot
  plot.new()
  grid(lty = 1,lwd = 2,col = "gray90" )
  par(new = TRUE)
  
  # plot
  hist(data[j,1:n_j[j]], freq = F, breaks = n_j[j]/5,
       main = paste0("Level = ",j), 
       ylim = c(0,0.5), xlim = xrange,
       ylab = "y-axis", xlab = "x-axis")
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points)
  #lines(dj$x,dj$y, lwd = 3)
  #lines(x = grid, y = Pred_all[[j]][1,], col = "blue", lwd = 2, lty = 2) #0.025
  #lines(x = grid, y = Pred_all[[j]][3,], col = "blue", lwd = 2, lty = 2) #0.975
  polygon( c(grid, rev(grid)),
           c(Pred_all[[j]][1,], rev(Pred_all[[j]][3,])),
           col = ACutils::t_col(mycol[1], percent = 45))
  lines(x = grid, y = Pred_all[[j]][2,], col = mycol[1], lwd = 3) #0.5
  
  points(grid, dmix(x = grid, w_j = prob[j,], mu_vec = mu, sigma_vec = sd),
         col = "red", lwd = 2, type = "l")

  idx_start = idx_end + 1
    if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}

```




## Indexes

```{r}
Compute_coclust_error(real_partition, sim_matrix)
```

Compute the $L_1$ distance 

```{r}

Pred_median = vector("list", length = d)
for(j in 1:d){
  Pred_median[[j]] = Pred_all[[j]][2,]
}

Compute_L1_dist(Pred = Pred_median, 
                p_mix = prob, mu = mu, sigma = sd,
                grid = grid)
```

Casarin calcola anche mediana e varianza della catena del numero di cluster
```{r}
median(GDFMM$K)
var(GDFMM$K)
```

## Commenti

* In questo esempio, più che gli iperparametri di $P_0$ mi sembra fondamentale la scelta di $\Lambda$
* $\Lambda$ alto mixxa molto ma troppi cluster
* Con $\Lambda$ centrato su 3 e varianza bassa, ho i risultati migliori
* Ho visto che quando non funziona, spesso mi ritrovo $\gamma_1$ troppo alto. Quindi ho ridotto a priori mettendo $b_\gamma=10$ e va molto meglio
* $b_\gamma=10$ compensa bene la variabilità di $\Lambda$, così va bene anche lasciando varianza 1
* $b_\gamma=100$ fa oscillare molto meno il RI
* Però mettendo $varmu = 10, correction = 3, varsig2 = vardata^2$ e $b_\gamma=1$ i risultati finale erano come quelli con $b_\gamma=1$. Quello che cambia è che l'ARI è meno ballerino, si schiaccia più verso valori alti

# 3 - Sensibility wrt $P_0$

```{r}
suppressWarnings(suppressPackageStartupMessages(library(GDFMM)))
suppressWarnings(suppressPackageStartupMessages(library(ACutils)))
suppressWarnings(suppressPackageStartupMessages(library(tidyverse)))
suppressWarnings(suppressPackageStartupMessages(library(RColorBrewer)))
suppressWarnings(suppressPackageStartupMessages(library(salso)))
suppressWarnings(suppressPackageStartupMessages(library(wesanderson)))
suppressWarnings(suppressPackageStartupMessages(library(mcclust.ext)))
```

## 3.1 - Experiment Setup 

Inizio variando un solo parametro alla volta. Qui, vario $\bar{\text{V}}_\mu$. La variabile di riferimento è $\texttt{varmu}$.

* $\bar{V}_\mu $ variabile
* $\bar{V}_{\sigma^2} = \text{Var}(data)^2$
* $\text{c} = 3$
* $b_\gamma = 1$
* $\text{E}[\Lambda] = 3$
* $\text{Var}(\Lambda) = 1$

```{r}
# Data generation
d = 2               # number of groups
K = 3               # number of global clusters
mu = c(-3,0,1)      # vectors of means
sd = c(sqrt(0.1), sqrt(0.5), sqrt(1.5) )     # vector of sd
prob <- matrix(c(0.2,0.8,  0,
                   0,0.1,0.9),nrow = d, ncol = K, byrow = T)
  
n_j = rep(100, d)  # set cardinality of the groups
seed = 20051131

#genD = generate_data_prob(d=d,p=prob, K=K, mu = mu, sd = sd, n_j = n_j, seed = seed)
genD = simulate_data(d=d,prob=prob, K=K, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition


#table(real_partition)
#table(real_partition[1:n_j[1]])
#table(real_partition[(n_j[1]+1):(n_j[1] + n_j[2])])


mycol = hcl.colors(n=3,palette = "Zissou1")
mycol_cluster = brewer.pal(n=K, name = "Dark2")


## Common parameters
vardata = var(as.vector(data), na.rm = T)
niter  <-  10000
burnin <-  6000
thin   <-  1


a_gamma <-   1; b_gamma  <- 1
mu_lambda  = 3
var_lambda = 1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)


varmu_grid = c(1,vardata,vardata^2,10,50,100)
N_varmu = length(varmu_grid)
nomi_varmu_grid = c("Vmu=1","Vmu=Vdata","Vmu=Vdata^2","Vmu=10","Vmu=50","Vmu=100")

FinalResults = vector("list", length = N_varmu)

```


### Run simulations

```{r message=TRUE, warning=TRUE, include=FALSE}
nsim = 1
for(jj in 1:N_varmu){
    
    cat("\n Sim Number ",nsim,"\n")
    # Define hyperparameters
    P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = varmu_grid[jj], 
                                                varsig2 = vardata^2, 
                                                correction = 3
                                                )
    mu0 = P0_hyparam$mu0
    k0= P0_hyparam$k0
    nu0=P0_hyparam$nu0
    sigma0=P0_hyparam$sigma0
    
    # Compute mean and variance marginals
    scale = sqrt( (k0 + 1)/(k0) * sigma0 )
    mean_marginal = mu0
    var_marginal  = nu0/(nu0-2) * scale^2
    
    
    counter = 1
    data_per_cluster = vector("list", length = K)
    log_marginal_prior = vector("list", length = d)
    for(j in 1:d){
      log_marginal_prior[[j]] = vector( length = n_j[j])
      for(i in 1:n_j[j]){
        c_ji = real_partition[counter]
        data_per_cluster[[c_ji]] = c(data_per_cluster[[c_ji]], data[j,i])
        log_marginal_prior[[j]][i] = log(ACutils::dnct(data[j,i], n0 = 10, mu0 = 1, gamma0 = sqrt(2)) )
        counter = counter + 1
      }
      
    }
    mean_data_per_cluster = unlist(lapply(data_per_cluster, mean))
    var_data_per_cluster  = unlist(lapply(data_per_cluster, var))
    ndata_per_cluster = unlist(lapply(data_per_cluster, length))
    
    
    dof_post = nu0 + ndata_per_cluster
    k0_post  = k0 + ndata_per_cluster
    location_post = (k0*mu0 + mean_data_per_cluster*ndata_per_cluster)/(k0 + ndata_per_cluster)
    sigma0_post   = 1/dof_post*( (ndata_per_cluster - 1)*var_data_per_cluster + 
                                  nu0*sigma0 + 
                                  (k0*ndata_per_cluster)/(k0 + ndata_per_cluster) * (mu0 - mean_data_per_cluster)^2  
                               )
    
    scale_post = sqrt( (sigma0_post*(k0_post + 1))/(k0_post)  )
    
    mean_marginal_per_cluster = location_post
    var_marginal_per_cluster = scale_post^2 * (dof_post)/(dof_post - 2)
    
    
    # Run 
    
    option = set_options_marginal(
                 "mu0" = mu0,"sigma0"= sigma0, "k0"= k0, "nu0"=nu0,
                 "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
                 "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
                 "gamma0" = 3,"Lambda0" = 3, 
                 "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
                 "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
                 "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
                 "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
                 "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
                 "partition" = seq(1,sum(n_j))#real_partition
            )
    
    GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
    
    
    
    # RI
    
    part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix
    RI = apply( part_matrix, 1, 
                 FUN = function(part_it){
                          arandi(part_it,real_partition)
                      }
                )
    
    # COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 
    part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix
    
    # Compute similarity matrix
    sim_matrix <- psm(part_matrix)
    
    
    binder_sara = minbinder(sim_matrix)
    VI_sara = minVI(sim_matrix)
    
    
    Bin = arandi(binder_sara$cl,real_partition, )
    VI  = arandi(VI_sara$cl,real_partition)
    
    
    CoClust = Compute_coclust_error(real_partition, sim_matrix)
    
    
    xrange = c(-6,6)
    l_grid = 200
    grid = seq(xrange[1],xrange[2],length.out = l_grid)
    # Predictive in all groups
    Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)
    
    
    Pred_median = vector("list", length = d)
    for(j in 1:d){
      Pred_median[[j]] = Pred_all[[j]][2,]
    }
    
    L1err = Compute_L1_dist(Pred = Pred_median, 
                             p_mix = prob, mu = mu, sigma = sd,
                             grid = grid)
    
    
    res = list("P0_hyparam"= c("mu0"=mu0,"k0"=k0,"nu0"=nu0,"sigma0"=sigma0),
               "Prior_marginal"=c("mean_marginal"=mean_marginal,"var_marginal"=var_marginal),
               "Post_marg_in_cluster" = c("mean"=mean_marginal_per_cluster, "var" = var_marginal_per_cluster),
               "RI" = c("MCmean" = mean(RI), "MCvar" = var(RI), "Binder"=Bin, "VI"=VI ),
               "CoClust"=c("err" = CoClust$coclust_err, "star" = CoClust$coclust_err_star), 
               "L1err" = L1err$L1err_average, 
               "Kstats" = c("MCmedian" = median(GDFMM$K), "MCvar" = var(GDFMM$K), 
                            "Binder"=length(table(binder_sara$cl)), "VI"=length(table(VI_sara$cl)) ),
               "Lambda" = c("mean" = mean(GDFMM$lambda), "median" = median(GDFMM$lambda), "var" = var(GDFMM$lambda)),
               "gamma" = colMeans(t(GDFMM$gamma))
               )
    
    FinalResults[[jj]] = res
    
    #Update simulation number
    nsim = nsim + 1
  }
```



```{r}
# Save
#save(FinalResults, file = "G:\\.shortcut-targets-by-id\\1Ck2MctcmCBWOueSMeW4Zr8IOvOaOzdqz\\BicoccaDrive\\TutoratoPoli2121_Group-dependent\\Test&Results\\SimCasarin_varsig2fix_exp1.Rdat")
```


### Leggo

```{r}
nu0s = unlist(lapply(FinalResults, FUN = function(x){
  x$P0_hyparam[3]
}))

k0s = unlist(lapply(FinalResults, FUN = function(x){
  x$P0_hyparam[2]
}))

sigma0s = unlist(lapply(FinalResults, FUN = function(x){
  x$P0_hyparam[4]
}))


names(nu0s) <- names(k0s) <- names(sigma0s) <- nomi_varmu_grid

round(nu0s, digits = 4)
round(k0s, digits = 4)
round(sigma0s, digits = 4)

```

```{r}
PriorVar = unlist(lapply(FinalResults, FUN = function(x){
  x$Prior_marginal[2]
}))
  

names(PriorVar) <- nomi_varmu_grid
round(PriorVar, digits = 4)
```

```{r}
Var_pred_in_cluster = matrix(0,nrow = N_varmu, ncol = K)
for(it in 1:N_varmu){
  Var_pred_in_cluster[it,] = FinalResults[[it]]$Post_marg_in_cluster[(K+1):(2*K)]
}

rownames(Var_pred_in_cluster) <- nomi_varmu_grid
colnames(Var_pred_in_cluster) <- paste0("Clus=", 1:K)

Var_pred_in_cluster
```



```{r}
Lambda_mean = unlist(lapply(FinalResults, FUN = function(x){
  x$Lambda[1]
}))

Lambda_var = unlist(lapply(FinalResults, FUN = function(x){
  x$Lambda[2]
}))

names(Lambda_mean) <- names(Lambda_mean) <- nomi_varmu_grid

gamma_means = matrix(0,nrow = N_varmu, ncol = d)
for(it in 1:N_varmu){
  gamma_means[it,] = FinalResults[[it]]$gamma
}

rownames(gamma_means) <- nomi_varmu_grid
colnames(gamma_means) <- paste0("Lev=", 1:d)

Lambda_mean
Lambda_var
gamma_means
```

```{r}

RI_Bin = unlist(lapply(FinalResults, FUN = function(x){
  x$RI[3]
}))

RI_VI = unlist(lapply(FinalResults, FUN = function(x){
  x$RI[4]
}))

CoClustering_error = unlist(lapply(FinalResults, FUN = function(x){
  x$CoClust[1]
}))

L1 = unlist(lapply(FinalResults, FUN = function(x){
  x$L1err
}))

Kest = unlist(lapply(FinalResults, FUN = function(x){
  x$Kstats[4]
}))

MC_Kest = unlist(lapply(FinalResults, FUN = function(x){
  x$Kstats[1]
}))

Kest_var = unlist(lapply(FinalResults, FUN = function(x){
  x$Kstats[2]
}))


names(RI_Bin) <- names(RI_VI) <- names(L1) <- names(Kest) <- names(Kest_var) <- names(MC_Kest) <- names(CoClustering_error) <- nomi_varmu_grid

RI_Bin
RI_VI
CoClustering_error
L1
Kest
MC_Kest
Kest_var
```

## 3.2 - Experiment Setup 

Inizio variando un solo parametro alla volta. Qui, vario $\bar{\text{V}}_{\sigma^2}$. La variabile di riferimento è $\texttt{varsig2}$.

* $\bar{V}_\mu = 10$ 
* $\bar{V}_{\sigma^2} $ variabile
* $\text{c} = 3$
* $b_\gamma = 1$
* $\text{E}[\Lambda] = 3$
* $\text{Var}(\Lambda) = 1$

```{r}
# Data generation
d = 2               # number of groups
K = 3               # number of global clusters
mu = c(-3,0,1)      # vectors of means
sd = c(sqrt(0.1), sqrt(0.5), sqrt(1.5) )     # vector of sd
prob <- matrix(c(0.2,0.8,  0,
                   0,0.1,0.9),nrow = d, ncol = K, byrow = T)
  
n_j = rep(100, d)  # set cardinality of the groups
seed = 20051131

#genD = generate_data_prob(d=d,p=prob, K=K, mu = mu, sd = sd, n_j = n_j, seed = seed)
genD = simulate_data(d=d,prob=prob, K=K, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition


#table(real_partition)
#table(real_partition[1:n_j[1]])
#table(real_partition[(n_j[1]+1):(n_j[1] + n_j[2])])


mycol = hcl.colors(n=3,palette = "Zissou1")
mycol_cluster = brewer.pal(n=K, name = "Dark2")


## Common parameters
vardata = var(as.vector(data), na.rm = T)
niter  <-  10000
burnin <-  6000
thin   <-  1


a_gamma <-   1; b_gamma  <- 1
mu_lambda  = 3
var_lambda = 1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)


varsig2_grid = c(1,vardata,vardata^2,10*vardata,100)
N_varsig = length(varsig2_grid)
nomi_varsig2_grid = c("Vsig=1","Vsig=Vdata","Vsig=Vdata^2","Vsig=10Vdata","Vsig=100")

FinalResults = vector("list", length = N_varsig)

```


### Run simulations

```{r message=TRUE, warning=TRUE, include=FALSE}
nsim = 1
for(jj in 1:N_varsig){
    
    cat("\n Sim Number ",nsim,"\n")
    # Define hyperparameters
    P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 10, 
                                                varsig2 = varsig2_grid[jj], 
                                                correction = 3
                                                )
    mu0 = P0_hyparam$mu0
    k0= P0_hyparam$k0
    nu0=P0_hyparam$nu0
    sigma0=P0_hyparam$sigma0
    
    # Compute mean and variance marginals
    scale = sqrt( (k0 + 1)/(k0) * sigma0 )
    mean_marginal = mu0
    var_marginal  = nu0/(nu0-2) * scale^2
    
    
    counter = 1
    data_per_cluster = vector("list", length = K)
    log_marginal_prior = vector("list", length = d)
    for(j in 1:d){
      log_marginal_prior[[j]] = vector( length = n_j[j])
      for(i in 1:n_j[j]){
        c_ji = real_partition[counter]
        data_per_cluster[[c_ji]] = c(data_per_cluster[[c_ji]], data[j,i])
        log_marginal_prior[[j]][i] = log(ACutils::dnct(data[j,i], n0 = 10, mu0 = 1, gamma0 = sqrt(2)) )
        counter = counter + 1
      }
      
    }
    mean_data_per_cluster = unlist(lapply(data_per_cluster, mean))
    var_data_per_cluster  = unlist(lapply(data_per_cluster, var))
    ndata_per_cluster = unlist(lapply(data_per_cluster, length))
    
    
    dof_post = nu0 + ndata_per_cluster
    k0_post  = k0 + ndata_per_cluster
    location_post = (k0*mu0 + mean_data_per_cluster*ndata_per_cluster)/(k0 + ndata_per_cluster)
    sigma0_post   = 1/dof_post*( (ndata_per_cluster - 1)*var_data_per_cluster + 
                                  nu0*sigma0 + 
                                  (k0*ndata_per_cluster)/(k0 + ndata_per_cluster) * (mu0 - mean_data_per_cluster)^2  
                               )
    
    scale_post = sqrt( (sigma0_post*(k0_post + 1))/(k0_post)  )
    
    mean_marginal_per_cluster = location_post
    var_marginal_per_cluster = scale_post^2 * (dof_post)/(dof_post - 2)
    
    
    # Run 
    
    option = set_options_marginal(
                 "mu0" = mu0,"sigma0"= sigma0, "k0"= k0, "nu0"=nu0,
                 "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
                 "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
                 "gamma0" = 3,"Lambda0" = 3, 
                 "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
                 "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
                 "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
                 "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
                 "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
                 "partition" = seq(1,sum(n_j))#real_partition
            )
    
    GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
    
    
    
    # RI
    
    part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix
    RI = apply( part_matrix, 1, 
                 FUN = function(part_it){
                          arandi(part_it,real_partition)
                      }
                )
    
    # COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 
    part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix
    
    # Compute similarity matrix
    sim_matrix <- psm(part_matrix)
    
    
    binder_sara = minbinder(sim_matrix)
    VI_sara = minVI(sim_matrix)
    
    
    Bin = arandi(binder_sara$cl,real_partition, )
    VI  = arandi(VI_sara$cl,real_partition)
    
    
    CoClust = Compute_coclust_error(real_partition, sim_matrix)
    
    
    xrange = c(-10,10)
    l_grid = 200
    grid = seq(xrange[1],xrange[2],length.out = l_grid)
    # Predictive in all groups
    Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)
    
    
    Pred_median = vector("list", length = d)
    for(j in 1:d){
      Pred_median[[j]] = Pred_all[[j]][2,]
    }
    
    L1err = Compute_L1_dist(Pred = Pred_median, 
                             p_mix = prob, mu = mu, sigma = sd,
                             grid = grid)
    
    
    res = list("P0_hyparam"= c("mu0"=mu0,"k0"=k0,"nu0"=nu0,"sigma0"=sigma0),
               "Prior_marginal"=c("mean_marginal"=mean_marginal,"var_marginal"=var_marginal),
               "Post_marg_in_cluster" = c("mean"=mean_marginal_per_cluster, "var" = var_marginal_per_cluster),
               "RI" = c("MCmean" = mean(RI), "MCvar" = var(RI), "Binder"=Bin, "VI"=VI ),
               "CoClust"=c("err" = CoClust$coclust_err, "star" = CoClust$coclust_err_star), 
               "L1err" = L1err$L1err_average, 
               "Kstats" = c("MCmedian" = median(GDFMM$K), "MCvar" = var(GDFMM$K), 
                            "Binder"=length(table(binder_sara$cl)), "VI"=length(table(VI_sara$cl)) ),
               "Lambda" = c("mean" = mean(GDFMM$lambda), "median" = median(GDFMM$lambda), "var" = var(GDFMM$lambda)),
               "gamma" = colMeans(t(GDFMM$gamma))
               )
    
    FinalResults[[jj]] = res
    
    #Update simulation number
    nsim = nsim + 1
  }
```



```{r}
# Save
#save(FinalResults, file = "G:\\.shortcut-targets-by-id\\1Ck2MctcmCBWOueSMeW4Zr8IOvOaOzdqz\\BicoccaDrive\\TutoratoPoli2121_Group-dependent\\Test&Results\\SimCasarin_varsig2fix_exp1.Rdat")
```


### Leggo

```{r}
nu0s = unlist(lapply(FinalResults, FUN = function(x){
  x$P0_hyparam[3]
}))

k0s = unlist(lapply(FinalResults, FUN = function(x){
  x$P0_hyparam[2]
}))

sigma0s = unlist(lapply(FinalResults, FUN = function(x){
  x$P0_hyparam[4]
}))


names(nu0s) <- names(k0s) <- names(sigma0s) <- nomi_varsig2_grid

round(nu0s, digits = 4)
round(k0s, digits = 4)
round(sigma0s, digits = 4)

```

```{r}
PriorVar = unlist(lapply(FinalResults, FUN = function(x){
  x$Prior_marginal[2]
}))
  

names(PriorVar) <- nomi_varsig2_grid
round(PriorVar, digits = 4)
```

```{r}
Var_pred_in_cluster = matrix(0,nrow = N_varsig, ncol = K)
for(it in 1:N_varsig){
  Var_pred_in_cluster[it,] = FinalResults[[it]]$Post_marg_in_cluster[(K+1):(2*K)]
}

rownames(Var_pred_in_cluster) <- nomi_varsig2_grid
colnames(Var_pred_in_cluster) <- paste0("Clus=", 1:K)

Var_pred_in_cluster
```



```{r}
Lambda_mean = unlist(lapply(FinalResults, FUN = function(x){
  x$Lambda[1]
}))

Lambda_var = unlist(lapply(FinalResults, FUN = function(x){
  x$Lambda[2]
}))

names(Lambda_mean) <- names(Lambda_var) <- nomi_varsig2_grid

gamma_means = matrix(0,nrow = N_varsig, ncol = d)
for(it in 1:N_varsig){
  gamma_means[it,] = FinalResults[[it]]$gamma
}

rownames(gamma_means) <- nomi_varsig2_grid
colnames(gamma_means) <- paste0("Lev=", 1:d)

Lambda_mean
Lambda_var
gamma_means
```

```{r}

RI_Bin = unlist(lapply(FinalResults, FUN = function(x){
  x$RI[3]
}))

RI_VI = unlist(lapply(FinalResults, FUN = function(x){
  x$RI[4]
}))

CoClustering_error = unlist(lapply(FinalResults, FUN = function(x){
  x$CoClust[1]
}))

L1 = unlist(lapply(FinalResults, FUN = function(x){
  x$L1err
}))

Kest = unlist(lapply(FinalResults, FUN = function(x){
  x$Kstats[4]
}))

MC_Kest = unlist(lapply(FinalResults, FUN = function(x){
  x$Kstats[1]
}))

Kest_var = unlist(lapply(FinalResults, FUN = function(x){
  x$Kstats[2]
}))


names(RI_Bin) <- names(RI_VI) <- names(L1) <- names(Kest) <- names(Kest_var) <- names(MC_Kest) <- names(CoClustering_error) <- nomi_varsig2_grid

RI_Bin
RI_VI
CoClustering_error
L1
Kest
MC_Kest
Kest_var
```


## 3.3 - Experiment Setup 

Inizio variando un solo parametro alla volta. Qui, vario $\text{c}$. La variabile di riferimento è $\texttt{correction}$.

* $\bar{V}_\mu = 10$ 
* $\bar{V}_{\sigma^2} = \text{Var}(data)^2$
* $\text{c} $ variabile
* $b_\gamma = 1$
* $\text{E}[\Lambda] = 3$
* $\text{Var}(\Lambda) = 1$

```{r}
# Data generation
d = 2               # number of groups
K = 3               # number of global clusters
mu = c(-3,0,1)      # vectors of means
sd = c(sqrt(0.1), sqrt(0.5), sqrt(1.5) )     # vector of sd
prob <- matrix(c(0.2,0.8,  0,
                   0,0.1,0.9),nrow = d, ncol = K, byrow = T)
  
n_j = rep(100, d)  # set cardinality of the groups
seed = 20051131

#genD = generate_data_prob(d=d,p=prob, K=K, mu = mu, sd = sd, n_j = n_j, seed = seed)
genD = simulate_data(d=d,prob=prob, K=K, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition


#table(real_partition)
#table(real_partition[1:n_j[1]])
#table(real_partition[(n_j[1]+1):(n_j[1] + n_j[2])])


mycol = hcl.colors(n=3,palette = "Zissou1")
mycol_cluster = brewer.pal(n=K, name = "Dark2")


## Common parameters
vardata = var(as.vector(data), na.rm = T)
niter  <-  10000
burnin <-  6000
thin   <-  1


a_gamma <-   1; b_gamma  <- 1
mu_lambda  = 3
var_lambda = 1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)



correction_grid = c(1,3,5,10,50,100,sum(n_j))
N_correction = length(correction_grid)
nomi_correction_grid = c("c=1","c=3","c=5","c=10","c=50","c=100","c=200")

FinalResults = vector("list", length = N_correction)

```


### Run simulations

```{r message=TRUE, warning=TRUE, include=FALSE}
nsim = 1
for(jj in 1:N_correction){
    
    cat("\n Sim Number ",nsim,"\n")
    # Define hyperparameters
    P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 10, 
                                                varsig2 = vardata^2, 
                                                correction = correction_grid[jj]
                                                )
    mu0 = P0_hyparam$mu0
    k0= P0_hyparam$k0
    nu0=P0_hyparam$nu0
    sigma0=P0_hyparam$sigma0
    
    # Compute mean and variance marginals
    scale = sqrt( (k0 + 1)/(k0) * sigma0 )
    mean_marginal = mu0
    var_marginal  = nu0/(nu0-2) * scale^2
    
    
    counter = 1
    data_per_cluster = vector("list", length = K)
    log_marginal_prior = vector("list", length = d)
    for(j in 1:d){
      log_marginal_prior[[j]] = vector( length = n_j[j])
      for(i in 1:n_j[j]){
        c_ji = real_partition[counter]
        data_per_cluster[[c_ji]] = c(data_per_cluster[[c_ji]], data[j,i])
        log_marginal_prior[[j]][i] = log(ACutils::dnct(data[j,i], n0 = 10, mu0 = 1, gamma0 = sqrt(2)) )
        counter = counter + 1
      }
      
    }
    mean_data_per_cluster = unlist(lapply(data_per_cluster, mean))
    var_data_per_cluster  = unlist(lapply(data_per_cluster, var))
    ndata_per_cluster = unlist(lapply(data_per_cluster, length))
    
    
    dof_post = nu0 + ndata_per_cluster
    k0_post  = k0 + ndata_per_cluster
    location_post = (k0*mu0 + mean_data_per_cluster*ndata_per_cluster)/(k0 + ndata_per_cluster)
    sigma0_post   = 1/dof_post*( (ndata_per_cluster - 1)*var_data_per_cluster + 
                                  nu0*sigma0 + 
                                  (k0*ndata_per_cluster)/(k0 + ndata_per_cluster) * (mu0 - mean_data_per_cluster)^2  
                               )
    
    scale_post = sqrt( (sigma0_post*(k0_post + 1))/(k0_post)  )
    
    mean_marginal_per_cluster = location_post
    var_marginal_per_cluster = scale_post^2 * (dof_post)/(dof_post - 2)
    
    
    # Run 
    
    option = set_options_marginal(
                 "mu0" = mu0,"sigma0"= sigma0, "k0"= k0, "nu0"=nu0,
                 "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
                 "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
                 "gamma0" = 3,"Lambda0" = 3, 
                 "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
                 "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
                 "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
                 "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
                 "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
                 "partition" = seq(1,sum(n_j))#real_partition
            )
    
    GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
    
    
    
    # RI
    
    part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix
    RI = apply( part_matrix, 1, 
                 FUN = function(part_it){
                          arandi(part_it,real_partition)
                      }
                )
    
    # COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 
    part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix
    
    # Compute similarity matrix
    sim_matrix <- psm(part_matrix)
    
    
    binder_sara = minbinder(sim_matrix)
    VI_sara = minVI(sim_matrix)
    
    
    Bin = arandi(binder_sara$cl,real_partition, )
    VI  = arandi(VI_sara$cl,real_partition)
    
    
    CoClust = Compute_coclust_error(real_partition, sim_matrix)
    
    
    xrange = c(-6,6)
    l_grid = 200
    grid = seq(xrange[1],xrange[2],length.out = l_grid)
    # Predictive in all groups
    Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)
    
    
    Pred_median = vector("list", length = d)
    for(j in 1:d){
      Pred_median[[j]] = Pred_all[[j]][2,]
    }
    
    L1err = Compute_L1_dist(Pred = Pred_median, 
                             p_mix = prob, mu = mu, sigma = sd,
                             grid = grid)
    
    
    res = list("P0_hyparam"= c("mu0"=mu0,"k0"=k0,"nu0"=nu0,"sigma0"=sigma0),
               "Prior_marginal"=c("mean_marginal"=mean_marginal,"var_marginal"=var_marginal),
               "Post_marg_in_cluster" = c("mean"=mean_marginal_per_cluster, "var" = var_marginal_per_cluster),
               "RI" = c("MCmean" = mean(RI), "MCvar" = var(RI), "Binder"=Bin, "VI"=VI ),
               "CoClust"=c("err" = CoClust$coclust_err, "star" = CoClust$coclust_err_star), 
               "L1err" = L1err$L1err_average, 
               "Kstats" = c("MCmedian" = median(GDFMM$K), "MCvar" = var(GDFMM$K), 
                            "Binder"=length(table(binder_sara$cl)), "VI"=length(table(VI_sara$cl)) ),
               "Lambda" = c("mean" = mean(GDFMM$lambda), "median" = median(GDFMM$lambda), "var" = var(GDFMM$lambda)),
               "gamma" = colMeans(t(GDFMM$gamma))
               )
    
    FinalResults[[jj]] = res
    
    #Update simulation number
    nsim = nsim + 1
  }
```



```{r}
# Save
#save(FinalResults, file = "G:\\.shortcut-targets-by-id\\1Ck2MctcmCBWOueSMeW4Zr8IOvOaOzdqz\\BicoccaDrive\\TutoratoPoli2121_Group-dependent\\Test&Results\\SimCasarin_varsig2fix_exp1.Rdat")
```


### Leggo

```{r}
nu0s = unlist(lapply(FinalResults, FUN = function(x){
  x$P0_hyparam[3]
}))

k0s = unlist(lapply(FinalResults, FUN = function(x){
  x$P0_hyparam[2]
}))

sigma0s = unlist(lapply(FinalResults, FUN = function(x){
  x$P0_hyparam[4]
}))


names(nu0s) <- names(k0s) <- names(sigma0s) <- nomi_correction_grid

round(nu0s, digits = 4)
round(k0s, digits = 4)
round(sigma0s, digits = 4)

```

```{r}
PriorVar = unlist(lapply(FinalResults, FUN = function(x){
  x$Prior_marginal[2]
}))
  

names(PriorVar) <- nomi_correction_grid
round(PriorVar, digits = 4)
```

```{r}
Var_pred_in_cluster = matrix(0,nrow = N_correction, ncol = K)
for(it in 1:N_correction){
  Var_pred_in_cluster[it,] = FinalResults[[it]]$Post_marg_in_cluster[(K+1):(2*K)]
}

rownames(Var_pred_in_cluster) <- nomi_correction_grid
colnames(Var_pred_in_cluster) <- paste0("Clus=", 1:K)

Var_pred_in_cluster
```



```{r}
Lambda_mean = unlist(lapply(FinalResults, FUN = function(x){
  x$Lambda[1]
}))

Lambda_var = unlist(lapply(FinalResults, FUN = function(x){
  x$Lambda[2]
}))

names(Lambda_mean) <- names(Lambda_var) <- nomi_correction_grid

gamma_means = matrix(0,nrow = N_correction, ncol = d)
for(it in 1:N_correction){
  gamma_means[it,] = FinalResults[[it]]$gamma
}

rownames(gamma_means) <- nomi_correction_grid
colnames(gamma_means) <- paste0("Lev=", 1:d)

Lambda_mean
Lambda_var
gamma_means
```

```{r}

RI_Bin = unlist(lapply(FinalResults, FUN = function(x){
  x$RI[3]
}))

RI_VI = unlist(lapply(FinalResults, FUN = function(x){
  x$RI[4]
}))

CoClustering_error = unlist(lapply(FinalResults, FUN = function(x){
  x$CoClust[1]
}))

L1 = unlist(lapply(FinalResults, FUN = function(x){
  x$L1err
}))

Kest = unlist(lapply(FinalResults, FUN = function(x){
  x$Kstats[4]
}))

MC_Kest = unlist(lapply(FinalResults, FUN = function(x){
  x$Kstats[1]
}))

Kest_var = unlist(lapply(FinalResults, FUN = function(x){
  x$Kstats[2]
}))


names(RI_Bin) <- names(RI_VI) <- names(L1) <- names(Kest) <- names(Kest_var) <- names(MC_Kest) <- names(CoClustering_error) <- nomi_correction_grid

RI_Bin
RI_VI
CoClustering_error
L1
Kest
MC_Kest
Kest_var
```




## 3.4 - Experiment Setup 

Faccio variare la media di $\Lambda$

* $\bar{V}_\mu = 10$ 
* $\bar{V}_{\sigma^2} = \text{Var}(data)^2$
* $\text{c} = 3$
* $b_\gamma = 1$
* $\text{E}[\Lambda]$ variabile
* $\text{Var}(\Lambda) = 1$

```{r}
# Data generation
d = 2               # number of groups
K = 3               # number of global clusters
mu = c(-3,0,1)      # vectors of means
sd = c(sqrt(0.1), sqrt(0.5), sqrt(1.5) )     # vector of sd
prob <- matrix(c(0.2,0.8,  0,
                   0,0.1,0.9),nrow = d, ncol = K, byrow = T)
  
n_j = rep(100, d)  # set cardinality of the groups
seed = 20051131

#genD = generate_data_prob(d=d,p=prob, K=K, mu = mu, sd = sd, n_j = n_j, seed = seed)
genD = simulate_data(d=d,prob=prob, K=K, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition


#table(real_partition)
#table(real_partition[1:n_j[1]])
#table(real_partition[(n_j[1]+1):(n_j[1] + n_j[2])])


mycol = hcl.colors(n=3,palette = "Zissou1")
mycol_cluster = brewer.pal(n=K, name = "Dark2")


## Common parameters
vardata = var(as.vector(data), na.rm = T)
niter  <-  10000
burnin <-  6000
thin   <-  1

a_gamma <-   1; b_gamma  <- 1
mean_Lambda_grid = c(1,3,5,10)
var_lambda = 1
nomi_meanL_grid = c("E[L]=1","E[L]=3","E[L]=5","E[L]=10")


N_Lambda = length(mean_Lambda_grid)

FinalResults = vector("list", length = N_Lambda)



```





### Run simulations

```{r message=TRUE, warning=TRUE, include=FALSE}
nsim = 1
for(jj in 1:N_Lambda){
    
    cat("\n Sim Number ",nsim,"\n")
    # Define hyperparameters
    P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 10, 
                                                varsig2 = var(as.vector(data), na.rm = T)^2, 
                                                correction = 3
                                                )
    mu0 = P0_hyparam$mu0
    k0= P0_hyparam$k0
    nu0=P0_hyparam$nu0
    sigma0=P0_hyparam$sigma0
    
    # Compute mean and variance marginals
    scale = sqrt( (k0 + 1)/(k0) * sigma0 )
    mean_marginal = mu0
    var_marginal  = nu0/(nu0-2) * scale^2
    
    
    counter = 1
    data_per_cluster = vector("list", length = K)
    log_marginal_prior = vector("list", length = d)
    for(j in 1:d){
      log_marginal_prior[[j]] = vector( length = n_j[j])
      for(i in 1:n_j[j]){
        c_ji = real_partition[counter]
        data_per_cluster[[c_ji]] = c(data_per_cluster[[c_ji]], data[j,i])
        log_marginal_prior[[j]][i] = log(ACutils::dnct(data[j,i], n0 = 10, mu0 = 1, gamma0 = sqrt(2)) )
        counter = counter + 1
      }
      
    }
    mean_data_per_cluster = unlist(lapply(data_per_cluster, mean))
    var_data_per_cluster  = unlist(lapply(data_per_cluster, var))
    ndata_per_cluster = unlist(lapply(data_per_cluster, length))
    
    
    dof_post = nu0 + ndata_per_cluster
    k0_post  = k0 + ndata_per_cluster
    location_post = (k0*mu0 + mean_data_per_cluster*ndata_per_cluster)/(k0 + ndata_per_cluster)
    sigma0_post   = 1/dof_post*( (ndata_per_cluster - 1)*var_data_per_cluster + 
                                  nu0*sigma0 + 
                                  (k0*ndata_per_cluster)/(k0 + ndata_per_cluster) * (mu0 - mean_data_per_cluster)^2  
                               )
    
    scale_post = sqrt( (sigma0_post*(k0_post + 1))/(k0_post)  )
    
    mean_marginal_per_cluster = location_post
    var_marginal_per_cluster = scale_post^2 * (dof_post)/(dof_post - 2)
    
    
    # Lambda hyperparameters
    mu_lambda  = mean_Lambda_grid[jj]
    var_lambda = 1
    a_lambda = (mu_lambda*mu_lambda)/var_lambda
    b_lambda = mu_lambda/(var_lambda)

    
    # Run 
    
    option = set_options_marginal(
                 "mu0" = mu0,"sigma0"= sigma0, "k0"= k0, "nu0"=nu0,
                 "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
                 "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
                 "gamma0" = 3,"Lambda0" = mu_lambda, 
                 "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
                 "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
                 "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
                 "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
                 "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
                 "partition" = seq(1,sum(n_j))#real_partition
            )
    
    GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
    
    
    
    # RI
    
    part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix
    RI = apply( part_matrix, 1, 
                 FUN = function(part_it){
                          arandi(part_it,real_partition)
                      }
                )
    
    # COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 
    part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix
    
    # Compute similarity matrix
    sim_matrix <- psm(part_matrix)
    
    
    binder_sara = minbinder(sim_matrix)
    VI_sara = minVI(sim_matrix)
    
    
    Bin = arandi(binder_sara$cl,real_partition, )
    VI  = arandi(VI_sara$cl,real_partition)
    
    
    CoClust = Compute_coclust_error(real_partition, sim_matrix)
    
    
    xrange = c(-6,6)
    l_grid = 200
    grid = seq(xrange[1],xrange[2],length.out = l_grid)
    # Predictive in all groups
    Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)
    
    
    Pred_median = vector("list", length = d)
    for(j in 1:d){
      Pred_median[[j]] = Pred_all[[j]][2,]
    }
    
    L1err = Compute_L1_dist(Pred = Pred_median, 
                             p_mix = prob, mu = mu, sigma = sd,
                             grid = grid)
    
    
    res = list("P0_hyparam"= c("mu0"=mu0,"k0"=k0,"nu0"=nu0,"sigma0"=sigma0),
               "Prior_marginal"=c("mean_marginal"=mean_marginal,"var_marginal"=var_marginal),
               "Post_marg_in_cluster" = c("mean"=mean_marginal_per_cluster, "var" = var_marginal_per_cluster),
               "RI" = c("MCmean" = mean(RI), "MCvar" = var(RI), "Binder"=Bin, "VI"=VI ),
               "CoClust"=c("err" = CoClust$coclust_err, "star" = CoClust$coclust_err_star), 
               "L1err" = L1err$L1err_average, 
               "Kstats" = c("MCmedian" = median(GDFMM$K), "MCvar" = var(GDFMM$K), 
                            "Binder"=length(table(binder_sara$cl)), "VI"=length(table(VI_sara$cl)) ),
               "Lambda" = c("mean" = mean(GDFMM$lambda), "median" = median(GDFMM$lambda), "var" = var(GDFMM$lambda)),
               "gamma" = colMeans(t(GDFMM$gamma))
               )
    
    FinalResults[[jj]] = res
    
    #Update simulation number
    nsim = nsim + 1
  }
```



```{r}
# Save
#save(FinalResults, file = "G:\\.shortcut-targets-by-id\\1Ck2MctcmCBWOueSMeW4Zr8IOvOaOzdqz\\BicoccaDrive\\TutoratoPoli2121_Group-dependent\\Test&Results\\SimCasarin_varsig2fix_exp1.Rdat")
```


### Leggo


```{r}
c("mu0"=mu0,"k0"=k0,"nu0"=nu0,"sigma0"=sigma0)
```



```{r}
var_marginal
```

```{r}
Lambda_mean = unlist(lapply(FinalResults, FUN = function(x){
  x$Lambda[1]
}))

Lambda_var = unlist(lapply(FinalResults, FUN = function(x){
  x$Lambda[2]
}))

names(Lambda_mean) <- names(Lambda_mean) <- nomi_meanL_grid

gamma_means = matrix(0,nrow = N_Lambda, ncol = d)
for(it in 1:N_Lambda){
  gamma_means[it,] = FinalResults[[it]]$gamma
}

rownames(gamma_means) <- nomi_meanL_grid
colnames(gamma_means) <- paste0("Lev=", 1:d)

Lambda_mean
Lambda_var
gamma_means
```

```{r}

RI_Bin = unlist(lapply(FinalResults, FUN = function(x){
  x$RI[3]
}))

RI_VI = unlist(lapply(FinalResults, FUN = function(x){
  x$RI[4]
}))

CoClustering_error = unlist(lapply(FinalResults, FUN = function(x){
  x$CoClust[1]
}))

L1 = unlist(lapply(FinalResults, FUN = function(x){
  x$L1err
}))

Kest = unlist(lapply(FinalResults, FUN = function(x){
  x$Kstats[4]
}))

MC_Kest = unlist(lapply(FinalResults, FUN = function(x){
  x$Kstats[1]
}))

Kest_var = unlist(lapply(FinalResults, FUN = function(x){
  x$Kstats[2]
}))


names(RI_Bin) <- names(RI_VI) <- names(L1) <- names(Kest) <- names(Kest_var) <- names(MC_Kest) <- names(CoClustering_error) <- nomi_meanL_grid

RI_Bin
RI_VI
CoClustering_error
L1
Kest
MC_Kest
Kest_var
```

## 3.5 - Experiment Setup 

Faccio variare la varianza di $\Lambda$ (secondo me questo è un po' imbrogliare)

* $\bar{V}_\mu = 10$ 
* $\bar{V}_{\sigma^2} = \text{Var}(data)^2$
* $\text{c} = 3$
* $b_\gamma = 1$
* $\text{E}[\Lambda] = 3$ 
* $\text{Var}(\Lambda) $ variabile

```{r}
# Data generation
d = 2               # number of groups
K = 3               # number of global clusters
mu = c(-3,0,1)      # vectors of means
sd = c(sqrt(0.1), sqrt(0.5), sqrt(1.5) )     # vector of sd
prob <- matrix(c(0.2,0.8,  0,
                   0,0.1,0.9),nrow = d, ncol = K, byrow = T)
  
n_j = rep(100, d)  # set cardinality of the groups
seed = 20051131

#genD = generate_data_prob(d=d,p=prob, K=K, mu = mu, sd = sd, n_j = n_j, seed = seed)
genD = simulate_data(d=d,prob=prob, K=K, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition


#table(real_partition)
#table(real_partition[1:n_j[1]])
#table(real_partition[(n_j[1]+1):(n_j[1] + n_j[2])])


mycol = hcl.colors(n=3,palette = "Zissou1")
mycol_cluster = brewer.pal(n=K, name = "Dark2")


## Common parameters
vardata = var(as.vector(data), na.rm = T)
niter  <-  10000
burnin <-  6000
thin   <-  1

a_gamma <-   1; b_gamma  <- 1
mu_lambda = 3
var_lambda_grid = c(0.01,0.1,1,10) 
nomi_varL_grid = c("V(L)=0.01","V(L)=0.1","V(L)=1","V(L)=10")

N_Lambda = length(var_lambda_grid)

FinalResults = vector("list", length = N_Lambda)



```





### Run simulations

```{r message=TRUE, warning=TRUE, include=FALSE}
nsim = 1
for(jj in 1:N_Lambda){
    
    cat("\n Sim Number ",nsim,"\n")
    # Define hyperparameters
    P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 10, 
                                                varsig2 = var(as.vector(data), na.rm = T)^2, 
                                                correction = 3
                                                )
    mu0 = P0_hyparam$mu0
    k0= P0_hyparam$k0
    nu0=P0_hyparam$nu0
    sigma0=P0_hyparam$sigma0
    
    # Compute mean and variance marginals
    scale = sqrt( (k0 + 1)/(k0) * sigma0 )
    mean_marginal = mu0
    var_marginal  = nu0/(nu0-2) * scale^2
    
    
    counter = 1
    data_per_cluster = vector("list", length = K)
    log_marginal_prior = vector("list", length = d)
    for(j in 1:d){
      log_marginal_prior[[j]] = vector( length = n_j[j])
      for(i in 1:n_j[j]){
        c_ji = real_partition[counter]
        data_per_cluster[[c_ji]] = c(data_per_cluster[[c_ji]], data[j,i])
        log_marginal_prior[[j]][i] = log(ACutils::dnct(data[j,i], n0 = 10, mu0 = 1, gamma0 = sqrt(2)) )
        counter = counter + 1
      }
      
    }
    mean_data_per_cluster = unlist(lapply(data_per_cluster, mean))
    var_data_per_cluster  = unlist(lapply(data_per_cluster, var))
    ndata_per_cluster = unlist(lapply(data_per_cluster, length))
    
    
    dof_post = nu0 + ndata_per_cluster
    k0_post  = k0 + ndata_per_cluster
    location_post = (k0*mu0 + mean_data_per_cluster*ndata_per_cluster)/(k0 + ndata_per_cluster)
    sigma0_post   = 1/dof_post*( (ndata_per_cluster - 1)*var_data_per_cluster + 
                                  nu0*sigma0 + 
                                  (k0*ndata_per_cluster)/(k0 + ndata_per_cluster) * (mu0 - mean_data_per_cluster)^2  
                               )
    
    scale_post = sqrt( (sigma0_post*(k0_post + 1))/(k0_post)  )
    
    mean_marginal_per_cluster = location_post
    var_marginal_per_cluster = scale_post^2 * (dof_post)/(dof_post - 2)
    
    
    # Lambda hyperparameters
    var_lambda = var_lambda_grid[jj]
    a_lambda = (mu_lambda*mu_lambda)/var_lambda
    b_lambda = mu_lambda/(var_lambda)

    
    # Run 
    
    option = set_options_marginal(
                 "mu0" = mu0,"sigma0"= sigma0, "k0"= k0, "nu0"=nu0,
                 "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
                 "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
                 "gamma0" = 3,"Lambda0" = mu_lambda, 
                 "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
                 "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
                 "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
                 "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
                 "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
                 "partition" = seq(1,sum(n_j))#real_partition
            )
    
    GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
    
    
    
    # RI
    
    part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix
    RI = apply( part_matrix, 1, 
                 FUN = function(part_it){
                          arandi(part_it,real_partition)
                      }
                )
    
    # COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 
    part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix
    
    # Compute similarity matrix
    sim_matrix <- psm(part_matrix)
    
    
    binder_sara = minbinder(sim_matrix)
    VI_sara = minVI(sim_matrix)
    
    
    Bin = arandi(binder_sara$cl,real_partition, )
    VI  = arandi(VI_sara$cl,real_partition)
    
    
    CoClust = Compute_coclust_error(real_partition, sim_matrix)
    
    
    xrange = c(-6,6)
    l_grid = 200
    grid = seq(xrange[1],xrange[2],length.out = l_grid)
    # Predictive in all groups
    Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)
    
    
    Pred_median = vector("list", length = d)
    for(j in 1:d){
      Pred_median[[j]] = Pred_all[[j]][2,]
    }
    
    L1err = Compute_L1_dist(Pred = Pred_median, 
                             p_mix = prob, mu = mu, sigma = sd,
                             grid = grid)
    
    
    res = list("P0_hyparam"= c("mu0"=mu0,"k0"=k0,"nu0"=nu0,"sigma0"=sigma0),
               "Prior_marginal"=c("mean_marginal"=mean_marginal,"var_marginal"=var_marginal),
               "Post_marg_in_cluster" = c("mean"=mean_marginal_per_cluster, "var" = var_marginal_per_cluster),
               "RI" = c("MCmean" = mean(RI), "MCvar" = var(RI), "Binder"=Bin, "VI"=VI ),
               "CoClust"=c("err" = CoClust$coclust_err, "star" = CoClust$coclust_err_star), 
               "L1err" = L1err$L1err_average, 
               "Kstats" = c("MCmedian" = median(GDFMM$K), "MCvar" = var(GDFMM$K), 
                            "Binder"=length(table(binder_sara$cl)), "VI"=length(table(VI_sara$cl)) ),
               "Lambda" = c("mean" = mean(GDFMM$lambda), "median" = median(GDFMM$lambda), "var" = var(GDFMM$lambda)),
               "gamma" = colMeans(t(GDFMM$gamma))
               )
    
    FinalResults[[jj]] = res
    
    #Update simulation number
    nsim = nsim + 1
  }
```



```{r}
# Save
#save(FinalResults, file = "G:\\.shortcut-targets-by-id\\1Ck2MctcmCBWOueSMeW4Zr8IOvOaOzdqz\\BicoccaDrive\\TutoratoPoli2121_Group-dependent\\Test&Results\\SimCasarin_varsig2fix_exp1.Rdat")
```


### Leggo


```{r}
c("mu0"=mu0,"k0"=k0,"nu0"=nu0,"sigma0"=sigma0)
```



```{r}
var_marginal
```

```{r}
Lambda_mean = unlist(lapply(FinalResults, FUN = function(x){
  x$Lambda[1]
}))

Lambda_var = unlist(lapply(FinalResults, FUN = function(x){
  x$Lambda[2]
}))

names(Lambda_mean) <- names(Lambda_mean) <- nomi_varL_grid

gamma_means = matrix(0,nrow = N_Lambda, ncol = d)
for(it in 1:N_Lambda){
  gamma_means[it,] = FinalResults[[it]]$gamma
}

rownames(gamma_means) <- nomi_varL_grid
colnames(gamma_means) <- paste0("Lev=", 1:d)

Lambda_mean
Lambda_var
gamma_means
```

```{r}

RI_Bin = unlist(lapply(FinalResults, FUN = function(x){
  x$RI[3]
}))

RI_VI = unlist(lapply(FinalResults, FUN = function(x){
  x$RI[4]
}))

CoClustering_error = unlist(lapply(FinalResults, FUN = function(x){
  x$CoClust[1]
}))

L1 = unlist(lapply(FinalResults, FUN = function(x){
  x$L1err
}))

Kest = unlist(lapply(FinalResults, FUN = function(x){
  x$Kstats[4]
}))

MC_Kest = unlist(lapply(FinalResults, FUN = function(x){
  x$Kstats[1]
}))

Kest_var = unlist(lapply(FinalResults, FUN = function(x){
  x$Kstats[2]
}))


names(RI_Bin) <- names(RI_VI) <- names(L1) <- names(Kest) <- names(Kest_var) <- names(MC_Kest) <- names(CoClustering_error) <- nomi_varL_grid

RI_Bin
RI_VI
CoClustering_error
L1
Kest
MC_Kest
Kest_var
```



## 3.6 - Experiment Setup 

Ora invece faccio esperimenti al variare dei parametri di $\gamma$. 

* $\bar{V}_\mu = 10$ 
* $\bar{V}_{\sigma^2} = \text{Var}(data)^2$
* $\text{c} = 3$
* $b_\gamma $ variabile
* $\text{E}[\Lambda] = 3$ 
* $\text{Var}(\Lambda) = 1$ 

```{r}
# Data generation
d = 2               # number of groups
K = 3               # number of global clusters
mu = c(-3,0,1)      # vectors of means
sd = c(sqrt(0.1), sqrt(0.5), sqrt(1.5) )     # vector of sd
prob <- matrix(c(0.2,0.8,  0,
                   0,0.1,0.9),nrow = d, ncol = K, byrow = T)
  
n_j = rep(100, d)  # set cardinality of the groups
seed = 20051131

#genD = generate_data_prob(d=d,p=prob, K=K, mu = mu, sd = sd, n_j = n_j, seed = seed)
genD = simulate_data(d=d,prob=prob, K=K, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition


#table(real_partition)
#table(real_partition[1:n_j[1]])
#table(real_partition[(n_j[1]+1):(n_j[1] + n_j[2])])


mycol = hcl.colors(n=3,palette = "Zissou1")
mycol_cluster = brewer.pal(n=K, name = "Dark2")


## Common parameters
vardata = var(as.vector(data), na.rm = T)
niter  <-  10#000
burnin <-  6#000
thin   <-  1


# Lambda hyperparameters
mu_lambda  = 3
var_lambda = 1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)

b_gamma_grid = c(1,5,10,100)
nomi_bgamma_grid = c("b=1","b=5","b=10","b=100")

N_gamma = length(b_gamma_grid)

FinalResults = vector("list", length = N_gamma)



```





### Run simulations

```{r message=TRUE, warning=TRUE, include=FALSE}
nsim = 1
for(jj in 1:N_gamma){
    
    cat("\n Sim Number ",nsim,"\n")
    # Define hyperparameters
    P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 10, 
                                                varsig2 = var(as.vector(data), na.rm = T)^2, 
                                                correction = 3
                                                )
    mu0 = P0_hyparam$mu0
    k0= P0_hyparam$k0
    nu0=P0_hyparam$nu0
    sigma0=P0_hyparam$sigma0
    
    # Compute mean and variance marginals
    scale = sqrt( (k0 + 1)/(k0) * sigma0 )
    mean_marginal = mu0
    var_marginal  = nu0/(nu0-2) * scale^2
    
    
    counter = 1
    data_per_cluster = vector("list", length = K)
    log_marginal_prior = vector("list", length = d)
    for(j in 1:d){
      log_marginal_prior[[j]] = vector( length = n_j[j])
      for(i in 1:n_j[j]){
        c_ji = real_partition[counter]
        data_per_cluster[[c_ji]] = c(data_per_cluster[[c_ji]], data[j,i])
        log_marginal_prior[[j]][i] = log(ACutils::dnct(data[j,i], n0 = 10, mu0 = 1, gamma0 = sqrt(2)) )
        counter = counter + 1
      }
      
    }
    mean_data_per_cluster = unlist(lapply(data_per_cluster, mean))
    var_data_per_cluster  = unlist(lapply(data_per_cluster, var))
    ndata_per_cluster = unlist(lapply(data_per_cluster, length))
    
    
    dof_post = nu0 + ndata_per_cluster
    k0_post  = k0 + ndata_per_cluster
    location_post = (k0*mu0 + mean_data_per_cluster*ndata_per_cluster)/(k0 + ndata_per_cluster)
    sigma0_post   = 1/dof_post*( (ndata_per_cluster - 1)*var_data_per_cluster + 
                                  nu0*sigma0 + 
                                  (k0*ndata_per_cluster)/(k0 + ndata_per_cluster) * (mu0 - mean_data_per_cluster)^2  
                               )
    
    scale_post = sqrt( (sigma0_post*(k0_post + 1))/(k0_post)  )
    
    mean_marginal_per_cluster = location_post
    var_marginal_per_cluster = scale_post^2 * (dof_post)/(dof_post - 2)
    
  
    # gamma
    a_gamma <-   1; b_gamma  <- b_gamma_grid[jj]

    
    # Run 
    
    option = set_options_marginal(
                 "mu0" = mu0,"sigma0"= sigma0, "k0"= k0, "nu0"=nu0,
                 "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
                 "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
                 "gamma0" = 3,"Lambda0" = mu_lambda, 
                 "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
                 "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
                 "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
                 "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
                 "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
                 "partition" = seq(1,sum(n_j))#real_partition
            )
    
    GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
    
    
    
    # RI
    
    part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix
    RI = apply( part_matrix, 1, 
                 FUN = function(part_it){
                          arandi(part_it,real_partition)
                      }
                )
    
    # COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 
    part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix
    
    # Compute similarity matrix
    sim_matrix <- psm(part_matrix)
    
    
    binder_sara = minbinder(sim_matrix)
    VI_sara = minVI(sim_matrix)
    
    
    Bin = arandi(binder_sara$cl,real_partition, )
    VI  = arandi(VI_sara$cl,real_partition)
    
    
    CoClust = Compute_coclust_error(real_partition, sim_matrix)
    
    
    xrange = c(-6,6)
    l_grid = 200
    grid = seq(xrange[1],xrange[2],length.out = l_grid)
    # Predictive in all groups
    Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)
    
    
    Pred_median = vector("list", length = d)
    for(j in 1:d){
      Pred_median[[j]] = Pred_all[[j]][2,]
    }
    
    L1err = Compute_L1_dist(Pred = Pred_median, 
                             p_mix = prob, mu = mu, sigma = sd,
                             grid = grid)
    
    
    res = list("P0_hyparam"= c("mu0"=mu0,"k0"=k0,"nu0"=nu0,"sigma0"=sigma0),
               "Prior_marginal"=c("mean_marginal"=mean_marginal,"var_marginal"=var_marginal),
               "Post_marg_in_cluster" = c("mean"=mean_marginal_per_cluster, "var" = var_marginal_per_cluster),
               "RI" = c("MCmean" = mean(RI), "MCvar" = var(RI), "Binder"=Bin, "VI"=VI ),
               "CoClust"=c("err" = CoClust$coclust_err, "star" = CoClust$coclust_err_star), 
               "L1err" = L1err$L1err_average, 
               "Kstats" = c("MCmedian" = median(GDFMM$K), "MCvar" = var(GDFMM$K), 
                            "Binder"=length(table(binder_sara$cl)), "VI"=length(table(VI_sara$cl)) ),
               "Lambda" = c("mean" = mean(GDFMM$lambda), "median" = median(GDFMM$lambda), "var" = var(GDFMM$lambda)),
               "gamma" = colMeans(t(GDFMM$gamma))
               )
    
    FinalResults[[jj]] = res
    
    #Update simulation number
    nsim = nsim + 1
  }
```



```{r}
# Save
#save(FinalResults, file = "G:\\.shortcut-targets-by-id\\1Ck2MctcmCBWOueSMeW4Zr8IOvOaOzdqz\\BicoccaDrive\\TutoratoPoli2121_Group-dependent\\Test&Results\\SimCasarin_varsig2fix_exp1.Rdat")
```


### Leggo


```{r}
c("mu0"=mu0,"k0"=k0,"nu0"=nu0,"sigma0"=sigma0)
```


```{r}
var_marginal
```

```{r}
Lambda_mean = unlist(lapply(FinalResults, FUN = function(x){
  x$Lambda[1]
}))

Lambda_var = unlist(lapply(FinalResults, FUN = function(x){
  x$Lambda[2]
}))

names(Lambda_mean) <- names(Lambda_mean) <- nomi_bgamma_grid

gamma_means = matrix(0,nrow = N_gamma, ncol = d)
for(it in 1:N_gamma){
  gamma_means[it,] = FinalResults[[it]]$gamma
}

rownames(gamma_means) <-nomi_bgamma_grid
colnames(gamma_means) <- paste0("Lev=", 1:d)

Lambda_mean
Lambda_var
gamma_means
```


```{r}

RI_Bin = unlist(lapply(FinalResults, FUN = function(x){
  x$RI[3]
}))

RI_VI = unlist(lapply(FinalResults, FUN = function(x){
  x$RI[4]
}))

CoClustering_error = unlist(lapply(FinalResults, FUN = function(x){
  x$CoClust[1]
}))

L1 = unlist(lapply(FinalResults, FUN = function(x){
  x$L1err
}))

Kest = unlist(lapply(FinalResults, FUN = function(x){
  x$Kstats[4]
}))

MC_Kest = unlist(lapply(FinalResults, FUN = function(x){
  x$Kstats[1]
}))

Kest_var = unlist(lapply(FinalResults, FUN = function(x){
  x$Kstats[2]
}))


names(RI_Bin) <- names(RI_VI) <- names(L1) <- names(Kest) <- names(Kest_var) <- names(MC_Kest) <- names(CoClustering_error) <- nomi_bgamma_grid

RI_VI
CoClustering_error
L1
Kest
MC_Kest
Kest_var
```



## 3.7 - Experiment Setup 

Inizio lo studio congiunto facendo variare $\Lambda$ e $b_\gamma$

* $\bar{V}_\mu = 10$ 
* $\bar{V}_{\sigma^2} = \text{Var}(data)^2$
* $\text{c} = 3$
* $b_\gamma $ variabile
* $\text{E}[\Lambda] $ variabile 
* $\text{Var}(\Lambda) = 1$ 

```{r}
# Data generation
d = 2               # number of groups
K = 3               # number of global clusters
mu = c(-3,0,1)      # vectors of means
sd = c(sqrt(0.1), sqrt(0.5), sqrt(1.5) )     # vector of sd
prob <- matrix(c(0.2,0.8,  0,
                   0,0.1,0.9),nrow = d, ncol = K, byrow = T)
  
n_j = rep(100, d)  # set cardinality of the groups
seed = 20051131

#genD = generate_data_prob(d=d,p=prob, K=K, mu = mu, sd = sd, n_j = n_j, seed = seed)
genD = simulate_data(d=d,prob=prob, K=K, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition


#table(real_partition)
#table(real_partition[1:n_j[1]])
#table(real_partition[(n_j[1]+1):(n_j[1] + n_j[2])])


mycol = hcl.colors(n=3,palette = "Zissou1")
mycol_cluster = brewer.pal(n=K, name = "Dark2")


## Common parameters
vardata = var(as.vector(data), na.rm = T)
niter  <-  10#000
burnin <-  6#000
thin   <-  1


# Lambda hyperparameters
mean_Lambda_grid = c(1,3,5,10)
var_lambda = 1
nomi_meanL_grid = c("E[L]=1","E[L]=3","E[L]=5","E[L]=10")
N_Lambda = length(mean_Lambda_grid)

# gamma
a_gamma = 1
b_gamma_grid = c(1,5,10,100)
nomi_bgamma_grid = c("b=1","b=5","b=10","b=100")

N_gamma = length(b_gamma_grid)



lists = vector("list", length = N_Lambda*N_gamma)

FinalResults = matrix(lists, nrow = N_Lambda, ncol = N_gamma, byrow = T)
```





### Run simulations

```{r message=TRUE, warning=TRUE, include=FALSE}
nsim = 1
for(ii in 1:N_Lambda){
  for(jj in 1:N_gamma){
    
    cat("\n Sim Number ",nsim,"\n")
    # Define hyperparameters
    P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 10, 
                                                varsig2 = vardata^2, 
                                                correction = 3
                                                )
    mu0 = P0_hyparam$mu0
    k0= P0_hyparam$k0
    nu0=P0_hyparam$nu0
    sigma0=P0_hyparam$sigma0
    
    
    
    
    # Compute mean and variance marginals
    scale = sqrt( (k0 + 1)/(k0) * sigma0 )
    mean_marginal = mu0
    var_marginal  = nu0/(nu0-2) * scale^2
    
    
    counter = 1
    data_per_cluster = vector("list", length = K)
    log_marginal_prior = vector("list", length = d)
    for(j in 1:d){
      log_marginal_prior[[j]] = vector( length = n_j[j])
      for(i in 1:n_j[j]){
        c_ji = real_partition[counter]
        data_per_cluster[[c_ji]] = c(data_per_cluster[[c_ji]], data[j,i])
        log_marginal_prior[[j]][i] = log(ACutils::dnct(data[j,i], n0 = 10, mu0 = 1, gamma0 = sqrt(2)) )
        counter = counter + 1
      }
      
    }
    mean_data_per_cluster = unlist(lapply(data_per_cluster, mean))
    var_data_per_cluster  = unlist(lapply(data_per_cluster, var))
    ndata_per_cluster = unlist(lapply(data_per_cluster, length))
    
    
    dof_post = nu0 + ndata_per_cluster
    k0_post  = k0 + ndata_per_cluster
    location_post = (k0*mu0 + mean_data_per_cluster*ndata_per_cluster)/(k0 + ndata_per_cluster)
    sigma0_post   = 1/dof_post*( (ndata_per_cluster - 1)*var_data_per_cluster + 
                                  nu0*sigma0 + 
                                  (k0*ndata_per_cluster)/(k0 + ndata_per_cluster) * (mu0 - mean_data_per_cluster)^2  
                               )
    
    scale_post = sqrt( (sigma0_post*(k0_post + 1))/(k0_post)  )
    
    mean_marginal_per_cluster = location_post
    var_marginal_per_cluster = scale_post^2 * (dof_post)/(dof_post - 2)
    
    
    
    
    # Run 
    mu_lambda = mean_Lambda_grid[ii]
    a_lambda = (mu_lambda*mu_lambda)/var_lambda
    b_lambda = mu_lambda/(var_lambda)
    
    b_gamma = b_gamma_grid[jj]

    option = set_options_marginal(
                 "mu0" = mu0,"sigma0"= sigma0, "k0"= k0, "nu0"=nu0,
                 "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
                 "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
                 "gamma0" = 3,"Lambda0" = 10, 
                 "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
                 "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
                 "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
                 "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
                 "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
                 "partition" = seq(1,sum(n_j))#real_partition
            )
    
    GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
    
    
    
    # RI
    
    part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix
    RI = apply( part_matrix, 1, 
                 FUN = function(part_it){
                          arandi(part_it,real_partition)
                      }
                )
    
    # COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 
    part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix
    
    # Compute similarity matrix
    sim_matrix <- psm(part_matrix)
    
    
    binder_sara = minbinder(sim_matrix)
    VI_sara = minVI(sim_matrix)
    
    
    Bin = arandi(binder_sara$cl,real_partition, )
    VI  = arandi(VI_sara$cl,real_partition)
    
    
    CoClust = Compute_coclust_error(real_partition, sim_matrix)
    
    
    xrange = c(-6,6)
    l_grid = 200
    grid = seq(xrange[1],xrange[2],length.out = l_grid)
    # Predictive in all groups
    Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)
    
    
    Pred_median = vector("list", length = d)
    for(j in 1:d){
      Pred_median[[j]] = Pred_all[[j]][2,]
    }
    
    L1err = Compute_L1_dist(Pred = Pred_median, 
                             p_mix = prob, mu = mu, sigma = sd,
                             grid = grid)
    
    
    res = list("P0_hyparam"= c("mu0"=mu0,"k0"=k0,"nu0"=nu0,"sigma0"=sigma0),
               "Prior_marginal"=c("mean_marginal"=mean_marginal,"var_marginal"=var_marginal),
               "Post_marg_in_cluster" = c("mean"=mean_marginal_per_cluster, "var" = var_marginal_per_cluster),
               "RI" = c("MCmean" = mean(RI), "MCvar" = var(RI), "Binder"=Bin, "VI"=VI ),
               "CoClust"=c("err" = CoClust$coclust_err, "star" = CoClust$coclust_err_star), 
               "L1err" = L1err$L1err_average, 
               "Kstats" = c("MCmedian" = median(GDFMM$K), "MCvar" = var(GDFMM$K), 
                            "Binder"=length(table(binder_sara$cl)), "VI"=length(table(VI_sara$cl)) ),
               "Lambda" = c("mean" = mean(GDFMM$lambda), "median" = median(GDFMM$lambda), "var" = var(GDFMM$lambda)),
               "gamma" = colMeans(t(GDFMM$gamma))
               )
    
    FinalResults[ii,jj][[1]] = res
    
    #Update simulation number
    nsim = nsim + 1
  }
}
```



```{r}
# Save
#save(FinalResults, file = "G:\\.shortcut-targets-by-id\\1Ck2MctcmCBWOueSMeW4Zr8IOvOaOzdqz\\BicoccaDrive\\TutoratoPoli2121_Group-dependent\\Test&Results\\SimCasarin_varsig2fix_exp1.Rdat")
```


### Leggo


```{r}
nu0s = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$P0_hyparam[3]
})

k0s = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$P0_hyparam[2]
})

sigma0s = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$P0_hyparam[4]
})


rownames(nu0s) <- rownames(k0s) <- rownames(sigma0s) <- nomi_meanL_grid
colnames(nu0s) <- colnames(k0s) <- colnames(sigma0s) <- nomi_bgamma_grid

round(nu0s, digits = 4)
round(k0s, digits = 4)
round(sigma0s, digits = 4)

```





```{r}
PriorVar = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$Prior_marginal[2]
})


rownames(PriorVar) <- nomi_meanL_grid
colnames(PriorVar) <- nomi_bgamma_grid

round(PriorVar, digits = 4)
```
```{r}
Lambda_mean_mat = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$Lambda[1]
})

Lambda_var_mat = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$Lambda[2]
})

rownames(Lambda_mean_mat) <- rownames(Lambda_var_mat) <- nomi_meanL_grid
colnames(Lambda_mean_mat) <- colnames(Lambda_var_mat) <- nomi_bgamma_grid

Lambda_mean_mat
Lambda_var_mat
```



```{r}

RI_mat_Bin = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$RI[3]
})

RI_mat_VI = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$RI[4]
})

L1_mat = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$L1err
})

Kest_mat = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$Kstats[4]
})

MC_Kest_mat = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$Kstats[1]
})

Kvar_mat = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$Kstats[2]
})


rownames(RI_mat_Bin) <- rownames(RI_mat_VI) <- rownames(L1_mat) <- rownames(Kest_mat) <- rownames(Kvar_mat) <- rownames(MC_Kest_mat) <- nomi_meanL_grid
colnames(RI_mat_Bin) <- colnames(RI_mat_VI) <- colnames(L1_mat) <- colnames(Kest_mat) <- colnames(Kvar_mat) <- colnames(MC_Kest_mat) <- nomi_bgamma_grid

RI_mat_Bin
RI_mat_VI
L1_mat
Kest_mat
MC_Kest_mat
Kvar_mat
```




## 3.8 - Experiment Setup 


* $\bar{V}_\mu = 10$ 
* $\bar{V}_{\sigma^2}$ variabile
* $\text{c} $ variabile
* $b_\gamma = 10$
* $\text{E}[\Lambda] = 3$
* $\text{Var}(\Lambda) = 1$

```{r}
# Data generation
d = 2               # number of groups
K = 3               # number of global clusters
mu = c(-3,0,1)      # vectors of means
sd = c(sqrt(0.1), sqrt(0.5), sqrt(1.5) )     # vector of sd
prob <- matrix(c(0.2,0.8,  0,
                   0,0.1,0.9),nrow = d, ncol = K, byrow = T)
  
n_j = rep(100, d)  # set cardinality of the groups
seed = 20051131

#genD = generate_data_prob(d=d,p=prob, K=K, mu = mu, sd = sd, n_j = n_j, seed = seed)
genD = simulate_data(d=d,prob=prob, K=K, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition


#table(real_partition)
#table(real_partition[1:n_j[1]])
#table(real_partition[(n_j[1]+1):(n_j[1] + n_j[2])])


mycol = hcl.colors(n=3,palette = "Zissou1")
mycol_cluster = brewer.pal(n=K, name = "Dark2")


## Common parameters
vardata = var(as.vector(data), na.rm = T)
niter  <-  10#000
burnin <-  6#000
thin   <-  1


a_gamma <-   1; b_gamma  <- 10

mu_lambda  = 3
var_lambda = 1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)



correction_grid = c(3,5,10,50,100,sum(n_j))
N_corr = length(correction_grid)
nomi_correction_grid = c("c=3","c=5","c=10","c=50","c=100","c=200")

varsig2_grid = c(1,vardata,vardata^2,10*vardata,100)
N_varsd = length(varsig2_grid)
nomi_varsig2_grid = c("Vsig=1","Vsig=Vdata","Vsig=Vdata^2","Vsig=10Vdata","Vsig=100")


lists = vector("list", length = N_corr*N_varsd)

FinalResults = matrix(lists, nrow = N_corr, ncol = N_varsd, byrow = T)
```





### Run simulations

```{r message=TRUE, warning=TRUE, include=FALSE}
nsim = 1
for(ii in 1:N_corr){
  for(jj in 1:N_varsd){
    
    cat("\n Sim Number ",nsim,"\n")
    # Define hyperparameters
    varsig2 = 
    P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 10, 
                                                varsig2 = varsig2_grid[jj], 
                                                correction = correction_grid[ii]
                                                )
    mu0 = P0_hyparam$mu0
    k0= P0_hyparam$k0
    nu0=P0_hyparam$nu0
    sigma0=P0_hyparam$sigma0
    
    
    
    
    # Compute mean and variance marginals
    scale = sqrt( (k0 + 1)/(k0) * sigma0 )
    mean_marginal = mu0
    var_marginal  = nu0/(nu0-2) * scale^2
    
    
    counter = 1
    data_per_cluster = vector("list", length = K)
    log_marginal_prior = vector("list", length = d)
    for(j in 1:d){
      log_marginal_prior[[j]] = vector( length = n_j[j])
      for(i in 1:n_j[j]){
        c_ji = real_partition[counter]
        data_per_cluster[[c_ji]] = c(data_per_cluster[[c_ji]], data[j,i])
        log_marginal_prior[[j]][i] = log(ACutils::dnct(data[j,i], n0 = 10, mu0 = 1, gamma0 = sqrt(2)) )
        counter = counter + 1
      }
      
    }
    mean_data_per_cluster = unlist(lapply(data_per_cluster, mean))
    var_data_per_cluster  = unlist(lapply(data_per_cluster, var))
    ndata_per_cluster = unlist(lapply(data_per_cluster, length))
    
    
    dof_post = nu0 + ndata_per_cluster
    k0_post  = k0 + ndata_per_cluster
    location_post = (k0*mu0 + mean_data_per_cluster*ndata_per_cluster)/(k0 + ndata_per_cluster)
    sigma0_post   = 1/dof_post*( (ndata_per_cluster - 1)*var_data_per_cluster + 
                                  nu0*sigma0 + 
                                  (k0*ndata_per_cluster)/(k0 + ndata_per_cluster) * (mu0 - mean_data_per_cluster)^2  
                               )
    
    scale_post = sqrt( (sigma0_post*(k0_post + 1))/(k0_post)  )
    
    mean_marginal_per_cluster = location_post
    var_marginal_per_cluster = scale_post^2 * (dof_post)/(dof_post - 2)
    
    
    
    
    # Run 
    
    option = set_options_marginal(
                 "mu0" = mu0,"sigma0"= sigma0, "k0"= k0, "nu0"=nu0,
                 "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
                 "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
                 "gamma0" = 3,"Lambda0" = mu_lambda, 
                 "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
                 "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
                 "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
                 "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
                 "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
                 "partition" = seq(1,sum(n_j))#real_partition
            )
    
    GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
    
    
    
    # RI
    
    part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix
    RI = apply( part_matrix, 1, 
                 FUN = function(part_it){
                          arandi(part_it,real_partition)
                      }
                )
    
    # COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 
    part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix
    
    # Compute similarity matrix
    sim_matrix <- psm(part_matrix)
    
    
    binder_sara = minbinder(sim_matrix)
    VI_sara = minVI(sim_matrix)
    
    
    Bin = arandi(binder_sara$cl,real_partition, )
    VI  = arandi(VI_sara$cl,real_partition)
    
    
    CoClust = Compute_coclust_error(real_partition, sim_matrix)
    
    
    xrange = c(-6,6)
    l_grid = 200
    grid = seq(xrange[1],xrange[2],length.out = l_grid)
    # Predictive in all groups
    Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)
    
    
    Pred_median = vector("list", length = d)
    for(j in 1:d){
      Pred_median[[j]] = Pred_all[[j]][2,]
    }
    
    L1err = Compute_L1_dist(Pred = Pred_median, 
                             p_mix = prob, mu = mu, sigma = sd,
                             grid = grid)
    
    
    res = list("P0_hyparam"= c("mu0"=mu0,"k0"=k0,"nu0"=nu0,"sigma0"=sigma0),
               "Prior_marginal"=c("mean_marginal"=mean_marginal,"var_marginal"=var_marginal),
               "Post_marg_in_cluster" = c("mean"=mean_marginal_per_cluster, "var" = var_marginal_per_cluster),
               "RI" = c("MCmean" = mean(RI), "MCvar" = var(RI), "Binder"=Bin, "VI"=VI ),
               "CoClust"=c("err" = CoClust$coclust_err, "star" = CoClust$coclust_err_star), 
               "L1err" = L1err$L1err_average, 
               "Kstats" = c("MCmedian" = median(GDFMM$K), "MCvar" = var(GDFMM$K), 
                            "Binder"=length(table(binder_sara$cl)), "VI"=length(table(VI_sara$cl)) ),
               "Lambda" = c("mean" = mean(GDFMM$lambda), "median" = median(GDFMM$lambda), "var" = var(GDFMM$lambda)),
               "gamma" = colMeans(t(GDFMM$gamma))
               )
    
    FinalResults[ii,jj][[1]] = res
    
    #Update simulation number
    nsim = nsim + 1
  }
}
```



```{r}
# Save
# save(FinalResults, file = "G:\\.shortcut-targets-by-id\\1Ck2MctcmCBWOueSMeW4Zr8IOvOaOzdqz\\BicoccaDrive\\TutoratoPoli2121_Group-dependent\\Test&Results\\SimCasarin_muvar10_exp2.Rdat")
```


### Leggo

```{r}
nu0s = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$P0_hyparam[3]
})

k02 = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$P0_hyparam[2]
})

sigma02 = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$P0_hyparam[4]
})


rownames(nu0s) <- rownames(k02) <- rownames(sigma02) <- nomi_correction_grid
colnames(nu0s) <- colnames(k02) <- colnames(sigma02) <- nomi_varsig2_grid

round(nu0s, digits = 4)
round(k02, digits = 4)
round(sigma02, digits = 4)

```

```{r}
PriorVar = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$Prior_marginal[2]
})


rownames(PriorVar) <- nomi_correction_grid
colnames(PriorVar) <- nomi_varsig2_grid

round(PriorVar, digits = 4)
```

```{r}
Lambda_mean_mat = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$Lambda[1]
})

Lambda_var_mat = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$Lambda[2]
})

rownames(Lambda_mean_mat) <- rownames(Lambda_var_mat) <- nomi_correction_grid
colnames(Lambda_mean_mat) <- colnames(Lambda_var_mat) <- nomi_varsig2_grid

Lambda_mean_mat
Lambda_var_mat
```


```{r}

RI_mat_Bin = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$RI[3]
})

RI_mat_VI = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$RI[4]
})

L1_mat = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$L1err
})

Kest_mat = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$Kstats[4]
})

MC_Kest_mat = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$Kstats[1]
})


Kvar_mat = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$Kstats[2]
})


rownames(RI_mat_Bin) <- rownames(RI_mat_VI) <- rownames(L1_mat) <- rownames(Kest_mat) <- rownames(MC_Kest_mat) <- rownames(Kvar_mat) <- nomi_correction_grid
colnames(RI_mat_Bin) <- colnames(RI_mat_VI) <- colnames(L1_mat) <- colnames(Kest_mat) <- colnames(MC_Kest_mat) <- colnames(Kvar_mat) <- nomi_varsig2_grid


RI_mat_Bin
RI_mat_VI
L1_mat
Kest_mat
MC_Kest_mat
Kvar_mat
```



## 3.9 - Experiment Setup 


* $\bar{V}_\mu $ variabilie 
* $\bar{V}_{\sigma^2} = \text{Var}(\text{data})^2$ 
* $\text{c} $ variabile

```{r}
# Data generation
d = 2               # number of groups
K = 3               # number of global clusters
mu = c(-3,0,1)      # vectors of means
sd = c(sqrt(0.1), sqrt(0.5), sqrt(1.5) )     # vector of sd
prob <- matrix(c(0.2,0.8,  0,
                   0,0.1,0.9),nrow = d, ncol = K, byrow = T)
  
n_j = rep(100, d)  # set cardinality of the groups
seed = 20051131

#genD = generate_data_prob(d=d,p=prob, K=K, mu = mu, sd = sd, n_j = n_j, seed = seed)
genD = simulate_data(d=d,prob=prob, K=K, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition


#table(real_partition)
#table(real_partition[1:n_j[1]])
#table(real_partition[(n_j[1]+1):(n_j[1] + n_j[2])])


mycol = hcl.colors(n=3,palette = "Zissou1")
mycol_cluster = brewer.pal(n=K, name = "Dark2")


## Common parameters
vardata = var(as.vector(data), na.rm = T)
niter  <-  10000
burnin <-  6000
thin   <-  1

a_gamma <-   1; b_gamma  <- 1
a_lambda <- 10; b_lambda <- 2

correction_grid = c(1,5,10,50,100,sum(n_j))
varmu_grid = c(5,10,vardata,50,100,vardata^2)

N_varmu = length(varmu_grid)
N_corr = length(correction_grid)


nomi_correction_grid = c("c=1","c=5","c=10","c=50","c=100","c=200")
nomi_varmu_grid = c("Vmu=5","Vmu=10","Vmu=Vdata","Vmu=50","Vmu=100", "Vmu=Vdata^2")


lists = vector("list", length = N_corr*N_varmu)

FinalResults = matrix(lists, nrow = N_corr, ncol = N_varmu, byrow = T)
```





### Run simulations

```{r message=TRUE, warning=TRUE, include=FALSE}
nsim = 1
for(ii in 1:N_corr){
  for(jj in 1:N_varmu){
    
    cat("\n Sim Number ",nsim,"\n")
    # Define hyperparameters
    P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = varmu_grid[jj], 
                                                varsig2 = vardata^2, 
                                                correction = correction_grid[ii]
                                                )
    mu0 = P0_hyparam$mu0
    k0= P0_hyparam$k0
    nu0=P0_hyparam$nu0
    sigma0=P0_hyparam$sigma0
    
    
    
    
    # Compute mean and variance marginals
    scale = sqrt( (k0 + 1)/(k0) * sigma0 )
    mean_marginal = mu0
    var_marginal  = nu0/(nu0-2) * scale^2
    
    
    counter = 1
    data_per_cluster = vector("list", length = K)
    log_marginal_prior = vector("list", length = d)
    for(j in 1:d){
      log_marginal_prior[[j]] = vector( length = n_j[j])
      for(i in 1:n_j[j]){
        c_ji = real_partition[counter]
        data_per_cluster[[c_ji]] = c(data_per_cluster[[c_ji]], data[j,i])
        log_marginal_prior[[j]][i] = log(ACutils::dnct(data[j,i], n0 = 10, mu0 = 1, gamma0 = sqrt(2)) )
        counter = counter + 1
      }
      
    }
    mean_data_per_cluster = unlist(lapply(data_per_cluster, mean))
    var_data_per_cluster  = unlist(lapply(data_per_cluster, var))
    ndata_per_cluster = unlist(lapply(data_per_cluster, length))
    
    
    dof_post = nu0 + ndata_per_cluster
    k0_post  = k0 + ndata_per_cluster
    location_post = (k0*mu0 + mean_data_per_cluster*ndata_per_cluster)/(k0 + ndata_per_cluster)
    sigma0_post   = 1/dof_post*( (ndata_per_cluster - 1)*var_data_per_cluster + 
                                  nu0*sigma0 + 
                                  (k0*ndata_per_cluster)/(k0 + ndata_per_cluster) * (mu0 - mean_data_per_cluster)^2  
                               )
    
    scale_post = sqrt( (sigma0_post*(k0_post + 1))/(k0_post)  )
    
    mean_marginal_per_cluster = location_post
    var_marginal_per_cluster = scale_post^2 * (dof_post)/(dof_post - 2)
    
    
    
    
    # Run 
    
    option = set_options_marginal(
                 "mu0" = mu0,"sigma0"= sigma0, "k0"= k0, "nu0"=nu0,
                 "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
                 "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
                 "gamma0" = 3,"Lambda0" = 10, 
                 "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
                 "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
                 "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
                 "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
                 "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
                 "partition" = seq(1,sum(n_j))#real_partition
            )
    
    GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
    
    
    
    # RI
    
    part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix
    RI = apply( part_matrix, 1, 
                 FUN = function(part_it){
                          arandi(part_it,real_partition)
                      }
                )
    
    # COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 
    part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix
    
    # Compute similarity matrix
    sim_matrix <- psm(part_matrix)
    
    
    binder_sara = minbinder(sim_matrix)
    VI_sara = minVI(sim_matrix)
    
    
    Bin = arandi(binder_sara$cl,real_partition, )
    VI  = arandi(VI_sara$cl,real_partition)
    
    
    CoClust = Compute_coclust_error(real_partition, sim_matrix)
    
    
    xrange = c(-10,10)
    l_grid = 200
    grid = seq(xrange[1],xrange[2],length.out = l_grid)
    # Predictive in all groups
    Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)
    
    
    Pred_median = vector("list", length = d)
    for(j in 1:d){
      Pred_median[[j]] = Pred_all[[j]][2,]
    }
    
    L1err = Compute_L1_dist(Pred = Pred_median, 
                             p_mix = mix_probs, mu = mu, sigma = sd,
                             grid = grid)
    
    
    res = list("P0_hyparam"= c("mu0"=mu0,"k0"=k0,"nu0"=nu0,"sigma0"=sigma0),
               "Prior_marginal"=c("mean_marginal"=mean_marginal,"var_marginal"=var_marginal),
               "Post_marg_in_cluster" = c("mean"=mean_marginal_per_cluster, "var" = var_marginal_per_cluster),
               "RI" = c("MCmean" = mean(RI), "MCvar" = var(RI), "Binder"=Bin, "VI"=VI ),
               "CoClust"=c("err" = CoClust$coclust_err, "star" = CoClust$coclust_err_star), 
               "L1err" = L1err$L1err_average, 
               "Kstats" = c("MCmedian" = median(GDFMM$K), "MCvar" = var(GDFMM$K), 
                            "Binder"=length(table(binder_sara$cl)), "VI"=length(table(VI_sara$cl)) ),
               "Lambda" = c("mean" = mean(GDFMM$lambda), "median" = median(GDFMM$lambda), "var" = var(GDFMM$lambda)),
               "gamma" = colMeans(t(GDFMM$gamma))
               )
    
    FinalResults[ii,jj][[1]] = res
    
    #Update simulation number
    nsim = nsim + 1
  }
}
```



```{r}
# Save
#save(FinalResults, file = "G:\\.shortcut-targets-by-id\\1Ck2MctcmCBWOueSMeW4Zr8IOvOaOzdqz\\BicoccaDrive\\TutoratoPoli2121_Group-dependent\\Test&Results\\SimCasarin_varsig2fix_exp1.Rdat")
```


### Leggo


```{r}
nu0s = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$P0_hyparam[3]
})

k0s = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$P0_hyparam[2]
})

sigma0s = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$P0_hyparam[4]
})


rownames(nu0s) <- rownames(k0s) <- rownames(sigma0s) <- nomi_correction_grid
colnames(nu0s) <- colnames(k0s) <- colnames(sigma0s) <- nomi_varmu_grid

round(nu0s, digits = 4)
round(k0s, digits = 4)
round(sigma0s, digits = 4)

```





```{r}
PriorVar = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$Prior_marginal[2]
})


rownames(PriorVar) <- nomi_correction_grid
colnames(PriorVar) <- nomi_varmu_grid

round(PriorVar, digits = 4)
```

```{r}
Lambda_mean_mat = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$Lambda[1]
})

Lambda_var_mat = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$Lambda[2]
})

rownames(Lambda_mean_mat) <- rownames(Lambda_var_mat) <- nomi_correction_grid
colnames(Lambda_mean_mat) <- colnames(Lambda_var_mat) <- nomi_varmu_grid

Lambda_mean_mat
Lambda_var_mat
```


```{r}

RI_mat_Bin = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$RI[3]
})

RI_mat_VI = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$RI[4]
})

L1_mat = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$L1err
})

Kest_mat = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$Kstats[4]
})

MC_Kest_mat = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$Kstats[1]
})

Kvar_mat = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$Kstats[2]
})


rownames(RI_mat_Bin) <- rownames(RI_mat_VI) <- rownames(L1_mat) <- rownames(Kest_mat) <- rownames(Kvar_mat) <- rownames(MC_Kest_mat) <-  nomi_correction_grid
colnames(RI_mat_Bin) <- colnames(RI_mat_VI) <- colnames(L1_mat) <- colnames(Kest_mat) <- colnames(Kvar_mat) <- colnames(MC_Kest_mat) <- nomi_varmu_grid

RI_mat_VI
L1_mat
Kest_mat
MC_Kest_mat
Kvar_mat
```

## 3.10 - Experiment Setup 


* $\bar{V}_\mu $ variabile 
* $\bar{V}_{\sigma^2} $ variabile 
* $\text{c} = 50$

```{r}
# Data generation
d = 2               # number of groups
K = 3               # number of global clusters
mu = c(-3,0,1)      # vectors of means
sd = c(sqrt(0.1), sqrt(0.5), sqrt(1.5) )     # vector of sd
prob <- matrix(c(0.2,0.8,  0,
                   0,0.1,0.9),nrow = d, ncol = K, byrow = T)
  
n_j = rep(100, d)  # set cardinality of the groups
seed = 20051131

#genD = generate_data_prob(d=d,p=prob, K=K, mu = mu, sd = sd, n_j = n_j, seed = seed)
genD = simulate_data(d=d,prob=prob, K=K, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition


#table(real_partition)
#table(real_partition[1:n_j[1]])
#table(real_partition[(n_j[1]+1):(n_j[1] + n_j[2])])


mycol = hcl.colors(n=3,palette = "Zissou1")
mycol_cluster = brewer.pal(n=K, name = "Dark2")


## Common parameters
vardata = var(as.vector(data), na.rm = T)
niter  <-  10000
burnin <-  6000
thin   <-  1

a_gamma <-   1; b_gamma  <- 1
a_lambda <- 10; b_lambda <- 2


varmu_grid = c(5,10,vardata,50,100,vardata^2)
varsig2_grid = c(0.1*vardata,vardata,10*vardata,vardata^2,10*vardata^2)

N_varmu = length(varmu_grid)
N_varsd = length(varsig2_grid)


nomi_varmu_grid = c("Vmu=5","Vmu=10","Vmu=Vdata","Vmu=50","Vmu=100","Vmu=Vdata^2")
nomi_varsig2_grid = c("Vsig=0.1Vdata","Vsig=Vdata","Vsig=10Vdata","Vsig=Vdata^2","Vsig=10Vdata^2")


lists = vector("list", length = N_varsd*N_varmu)

FinalResults = matrix(lists, nrow = N_varsd, ncol = N_varmu, byrow = T)
```





### Run simulations

```{r message=TRUE, warning=TRUE, include=FALSE}
nsim = 1
for(ii in 1:N_varsd){
  for(jj in 1:N_varmu){
    
    cat("\n Sim Number ",nsim,"\n")
    # Define hyperparameters
    P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = varmu_grid[jj], 
                                                varsig2 = varsig2_grid[ii], 
                                                correction = 50
                                                )
    mu0 = P0_hyparam$mu0
    k0= P0_hyparam$k0
    nu0=P0_hyparam$nu0
    sigma0=P0_hyparam$sigma0
    
    
    
    
    # Compute mean and variance marginals
    scale = sqrt( (k0 + 1)/(k0) * sigma0 )
    mean_marginal = mu0
    var_marginal  = nu0/(nu0-2) * scale^2
    
    
    counter = 1
    data_per_cluster = vector("list", length = K)
    log_marginal_prior = vector("list", length = d)
    for(j in 1:d){
      log_marginal_prior[[j]] = vector( length = n_j[j])
      for(i in 1:n_j[j]){
        c_ji = real_partition[counter]
        data_per_cluster[[c_ji]] = c(data_per_cluster[[c_ji]], data[j,i])
        log_marginal_prior[[j]][i] = log(ACutils::dnct(data[j,i], n0 = 10, mu0 = 1, gamma0 = sqrt(2)) )
        counter = counter + 1
      }
      
    }
    mean_data_per_cluster = unlist(lapply(data_per_cluster, mean))
    var_data_per_cluster  = unlist(lapply(data_per_cluster, var))
    ndata_per_cluster = unlist(lapply(data_per_cluster, length))
    
    
    dof_post = nu0 + ndata_per_cluster
    k0_post  = k0 + ndata_per_cluster
    location_post = (k0*mu0 + mean_data_per_cluster*ndata_per_cluster)/(k0 + ndata_per_cluster)
    sigma0_post   = 1/dof_post*( (ndata_per_cluster - 1)*var_data_per_cluster + 
                                  nu0*sigma0 + 
                                  (k0*ndata_per_cluster)/(k0 + ndata_per_cluster) * (mu0 - mean_data_per_cluster)^2  
                               )
    
    scale_post = sqrt( (sigma0_post*(k0_post + 1))/(k0_post)  )
    
    mean_marginal_per_cluster = location_post
    var_marginal_per_cluster = scale_post^2 * (dof_post)/(dof_post - 2)
    
    
    
    
    # Run 
    
    option = set_options_marginal(
                 "mu0" = mu0,"sigma0"= sigma0, "k0"= k0, "nu0"=nu0,
                 "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
                 "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
                 "gamma0" = 3,"Lambda0" = 10, 
                 "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
                 "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
                 "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
                 "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
                 "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
                 "partition" = seq(1,sum(n_j))#real_partition
            )
    
    GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
    
    
    
    # RI
    
    part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix
    RI = apply( part_matrix, 1, 
                 FUN = function(part_it){
                          arandi(part_it,real_partition)
                      }
                )
    
    # COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 
    part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix
    
    # Compute similarity matrix
    sim_matrix <- psm(part_matrix)
    
    
    binder_sara = minbinder(sim_matrix)
    VI_sara = minVI(sim_matrix)
    
    
    Bin = arandi(binder_sara$cl,real_partition, )
    VI  = arandi(VI_sara$cl,real_partition)
    
    
    CoClust = Compute_coclust_error(real_partition, sim_matrix)
    
    
    xrange = c(-10,10)
    l_grid = 200
    grid = seq(xrange[1],xrange[2],length.out = l_grid)
    # Predictive in all groups
    Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)
    
    
    Pred_median = vector("list", length = d)
    for(j in 1:d){
      Pred_median[[j]] = Pred_all[[j]][2,]
    }
    
    L1err = Compute_L1_dist(Pred = Pred_median, 
                             p_mix = mix_probs, mu = mu, sigma = sd,
                             grid = grid)
    
    
    res = list("P0_hyparam"= c("mu0"=mu0,"k0"=k0,"nu0"=nu0,"sigma0"=sigma0),
               "Prior_marginal"=c("mean_marginal"=mean_marginal,"var_marginal"=var_marginal),
               "Post_marg_in_cluster" = c("mean"=mean_marginal_per_cluster, "var" = var_marginal_per_cluster),
               "RI" = c("MCmean" = mean(RI), "MCvar" = var(RI), "Binder"=Bin, "VI"=VI ),
               "CoClust"=c("err" = CoClust$coclust_err, "star" = CoClust$coclust_err_star), 
               "L1err" = L1err$L1err_average, 
               "Kstats" = c("MCmedian" = median(GDFMM$K), "MCvar" = var(GDFMM$K), 
                            "Binder"=length(table(binder_sara$cl)), "VI"=length(table(VI_sara$cl)) ),
               "Lambda" = c("mean" = mean(GDFMM$lambda), "median" = median(GDFMM$lambda), "var" = var(GDFMM$lambda)),
               "gamma" = colMeans(t(GDFMM$gamma))
               )
    
    FinalResults[ii,jj][[1]] = res
    
    #Update simulation number
    nsim = nsim + 1
  }
}
```



```{r}
# Save
#save(FinalResults, file = "G:\\.shortcut-targets-by-id\\1Ck2MctcmCBWOueSMeW4Zr8IOvOaOzdqz\\BicoccaDrive\\TutoratoPoli2121_Group-dependent\\Test&Results\\SimCasarin_varsig2fix_exp1.Rdat")
```


### Leggo


```{r}
nu0s = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$P0_hyparam[3]
})

k0s = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$P0_hyparam[2]
})

sigma0s = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$P0_hyparam[4]
})


rownames(nu0s) <- rownames(k0s) <- rownames(sigma0s) <- nomi_varsig2_grid
colnames(nu0s) <- colnames(k0s) <- colnames(sigma0s) <- nomi_varmu_grid

round(nu0s, digits = 4)
round(k0s, digits = 4)
round(sigma0s, digits = 4)

```





```{r}
PriorVar = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$Prior_marginal[2]
})


rownames(PriorVar) <- nomi_varsig2_grid
colnames(PriorVar) <- nomi_varmu_grid

round(PriorVar, digits = 4)
```
```{r}
Lambda_mean_mat = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$Lambda[1]
})

Lambda_var_mat = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$Lambda[2]
})

rownames(Lambda_mean_mat) <- rownames(Lambda_var_mat) <- nomi_varsig2_grid
colnames(Lambda_mean_mat) <- colnames(Lambda_var_mat) <- nomi_varmu_grid

Lambda_mean_mat
Lambda_var_mat
```



```{r}

RI_mat_Bin = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$RI[3]
})

RI_mat_VI = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$RI[4]
})

L1_mat = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$L1err
})

Kest_mat = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$Kstats[4]
})

MC_Kest_mat = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$Kstats[1]
})

Kvar_mat = apply(FinalResults, c(1,2), FUN = function(x){
  x[[1]]$Kstats[2]
})


rownames(RI_mat_Bin) <- rownames(RI_mat_VI) <- rownames(L1_mat) <- rownames(Kest_mat) <- rownames(Kvar_mat) <- rownames(MC_Kest_mat) <- nomi_varsig2_grid
colnames(RI_mat_Bin) <- colnames(RI_mat_VI) <- colnames(L1_mat) <- colnames(Kest_mat) <- colnames(Kvar_mat) <- colnames(MC_Kest_mat) <- nomi_varmu_grid

RI_mat_VI
L1_mat
Kest_mat
MC_Kest_mat
Kvar_mat
```



