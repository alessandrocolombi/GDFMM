---
title: "RMD_DUBIGGING"
author: "Alessandro Colombi"
date: "30/8/2022"
output: html_document
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  out.width = "50%"
)
```


# Testing Marginal sampler and comparing with estimated density


## Primo caso, un solo livello
Dati esperimento:

* **K = 4**
* d = 1
* $n_1 = 200$.
* parto da partizione vera.
* **Tengo la partizione fissata**

```{r}
suppressWarnings(suppressPackageStartupMessages(library(GDFMM)))
suppressWarnings(suppressPackageStartupMessages(library(ACutils)))
suppressWarnings(suppressPackageStartupMessages(library(tidyverse)))
suppressWarnings(suppressPackageStartupMessages(library(RColorBrewer)))
suppressWarnings(suppressPackageStartupMessages(library(salso)))
suppressWarnings(suppressPackageStartupMessages(library(wesanderson)))
suppressWarnings(suppressPackageStartupMessages(library(mcclust.ext)))



# color palette -----------------------------------------------------------
mycol = hcl.colors(n=3,palette = "Zissou1")

# data generation ---------------------------------------------------------

d = 1                   # number of groups
K = 4                   # number of global clusters
mu = c(-20,-10,0, 10)   # vectors of means
sd = c(1,1,1,1)         # vector of sd
n_j = rep(200, d)        # set cardinality of the groups
seed = 20051131

genD = generate_data(d = d, K = K, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition

mycol_cluster = brewer.pal(n=K, name = "Dark2")

```

```{r}
counter = 1
data_per_cluster = vector("list", length = K)
log_marginal_prior = vector("list", length = d)
for(j in 1:d){
  log_marginal_prior[[j]] = vector( length = n_j[j])
  for(i in 1:n_j[j]){
    c_ji = real_partition[counter]
    data_per_cluster[[c_ji]] = c(data_per_cluster[[c_ji]], data[j,i])
    log_marginal_prior[[j]][i] = log(ACutils::dnct(data[j,i], n0 = 10, mu0 = 1, gamma0 = sqrt(2)) )
    counter = counter + 1
  }
  
}
#data_per_cluster 
mean_data_per_cluster = lapply(data_per_cluster, mean)
var_data_per_cluster  = lapply(data_per_cluster, var)
#log_marginal_prior
```

### Iperparametri con empirical Bayes

In questo primo esperimento, setto i parametri relativi a P0 con la funzione di empirical Bayes che abbiamo fatto.
Plotto i dati e la marginale del modello, che è 

$student-t(dof = \nu_0, loc = \mu_0, scale = \sqrt{\frac{k_0}{k_0+1}\sigma^2_0})$
```{r}
# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]

# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 1000, varsig2 = 1000, correction = 10)
mu0 = P0_hyparam$mu0;k0= P0_hyparam$k0;nu0=P0_hyparam$nu0;sigma0=P0_hyparam$sigma0

l_grid = 200
grid = seq(-25,15,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)

for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(  data = tibble(grid = grid, value = Prior_grid),
                color = 'black', aes(x=grid,y=value), size = 1.2,
                inherit.aes = F)  +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}
```
Sembra essere proprio la situazione che voglio, la prior abbraccia tutti e 4 i cluster


```{r}
# Run  --------------------------------------------------------------------
niter  <-  10
burnin <-  1
thin   <- 1

P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 1000, varsig2 = 1000, correction = 10)

option = set_options_marginal(
             "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "gamma0" = 1,"Lambda0" = 3, 
             "alpha_gamma"=5, "beta_gamma"=0.5, 
             "alpha_lambda"=1, "beta_lambda"=5,
             "init_mean_cluster" = NULL, "init_var_cluster" = NULL,
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = real_partition
        )
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = T, option = option)

```




Analisi parte parametrica con partizione fissa
```{r}
mu_est = rep(0,K)
s2_est = rep(0,K)

for(it in 1:niter){
  mu_est = mu_est + GDFMM$mu[[it]]
  s2_est = s2_est + GDFMM$sigma[[it]]
}
mu_est = mu_est/niter
s2_est = s2_est/niter

mu_est
s2_est
```
Le stime fanno schifo, se le medie ancora ancora possono ricordare quelle vere, le varianze sono gigantesche

```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
#Pred_all = predictive_all_groups(grid = grid, fit = GDFMM, burnin = 1)

# Predictive in one group
Pred_one_group = predictive_marginal(idx_group = 1, grid = grid, fit = GDFMM, option = option)

j = 1
idx_start = 1
idx_end = n_j[1]
tibble(value = data[j,],
       true_clus = as.factor(real_partition[idx_start:idx_end])
       ) %>%
  ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
  geom_point(y = rep(0.005,length(data[j,])), size = 2) +
  geom_histogram(data = tibble(value = data[j,]),
                 aes(x=value, y = ..density..), 
                 color = 'black', alpha = 0.3,
                 binwidth = 0.5, inherit.aes = F) +
  scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
  scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] )  +
  geom_path( data = as_tibble( t(Pred_one_group) ),
             color = 'black', aes(x=grid,y=`50%`), size = 1.2,
             inherit.aes = F)  +
  labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
  geom_ribbon(data = as_tibble( t(Pred_one_group) ), fill = mycol[1],
              aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
              alpha = 0.5, inherit.aes = F)

```
La predittiva non c'entra niente con i dati. 


### Iperparametri a caso

Faccio come prima ma gli iperparametri gli scelgo "a caso", con dei valori selezionati senza guardare i dati
```{r}
# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]

# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra

mu0 = 0;k0= 1/10;nu0=10;sigma0=1

l_grid = 200
grid = seq(-25,15,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)

for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(  data = tibble(grid = grid, value = Prior_grid),
                color = 'black', aes(x=grid,y=value), size = 1.2,
                inherit.aes = F)  +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}
```
Non sembra essere per niente una buona P0, è tutta schiacciata su un cluster 


```{r}
# Run  --------------------------------------------------------------------
niter  <- 500
burnin <- 500
thin   <- 1

P0_hyparam = empirical_bayes_normalinvgamma(data = data)

option = set_options_marginal(
             "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "gamma0" = 1,"Lambda0" = 3, 
             "alpha_gamma"=5, "beta_gamma"=0.5, 
             "alpha_lambda"=1, "beta_lambda"=5,
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = real_partition
        )
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = T, option = option)

```




Analisi parte parametrica con partizione fissa
```{r}
mu_est = rep(0,K)
s2_est = rep(1,K)

for(it in 1:niter){
  mu_est = mu_est + GDFMM$mu[[it]]
  s2_est = s2_est + GDFMM$sigma[[it]]
}
mu_est = mu_est/niter
s2_est = s2_est/niter

mu_est
s2_est
```
Le stime sono quasi perfette
```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
#Pred_all = predictive_all_groups(grid = grid, fit = GDFMM, burnin = 1)

# Predictive in one group
Pred_one_group = predictive_marginal(idx_group = 1, grid = grid, fit = GDFMM, 
                                     option = option, burnin = niter/2)

j = 1
idx_start = 1
idx_end = n_j[1]
tibble(value = data[j,],
       true_clus = as.factor(real_partition[idx_start:idx_end])
       ) %>%
  ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
  geom_point(y = rep(0.005,length(data[j,])), size = 2) +
  geom_histogram(data = tibble(value = data[j,]),
                 aes(x=value, y = ..density..), 
                 color = 'black', alpha = 0.3,
                 binwidth = 0.5, inherit.aes = F) +
  scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
  scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] )  +
  geom_path( data = as_tibble( t(Pred_one_group) ),
             color = 'black', aes(x=grid,y=`50%`), size = 1.2,
             inherit.aes = F)  +
  labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
  geom_ribbon(data = as_tibble( t(Pred_one_group) ), fill = mycol[1],
              aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
              alpha = 0.5, inherit.aes = F)

```
La predittiva è proprio dove dovrebbe essere 

Provo a spiegare l'evidenza osservata:



# Tutto aleatorio

```{r}
suppressWarnings(suppressPackageStartupMessages(library(GDFMM)))
suppressWarnings(suppressPackageStartupMessages(library(ACutils)))
suppressWarnings(suppressPackageStartupMessages(library(tidyverse)))
suppressWarnings(suppressPackageStartupMessages(library(RColorBrewer)))
suppressWarnings(suppressPackageStartupMessages(library(salso)))
suppressWarnings(suppressPackageStartupMessages(library(wesanderson)))
suppressWarnings(suppressPackageStartupMessages(library(mcclust.ext)))



# color palette -----------------------------------------------------------
mycol = hcl.colors(n=3,palette = "Zissou1")

# data generation ---------------------------------------------------------

d = 1                   # number of groups
K = 4                   # number of global clusters
mu = c(-20,-10,0, 10)   # vectors of means
sd = c(1,1,1,1)         # vector of sd
n_j = rep(200, d)        # set cardinality of the groups
seed = 20051131

genD = generate_data(d = d, K = K, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition

mycol_cluster = brewer.pal(n=K, name = "Dark2")

```

```{r}
counter = 1
data_per_cluster = vector("list", length = K)
log_marginal_prior = vector("list", length = d)
for(j in 1:d){
  log_marginal_prior[[j]] = vector( length = n_j[j])
  for(i in 1:n_j[j]){
    c_ji = real_partition[counter]
    data_per_cluster[[c_ji]] = c(data_per_cluster[[c_ji]], data[j,i])
    log_marginal_prior[[j]][i] = log(ACutils::dnct(data[j,i], n0 = 10, mu0 = 1, gamma0 = sqrt(2)) )
    counter = counter + 1
  }
  
}
#data_per_cluster 
mean_data_per_cluster = lapply(data_per_cluster, mean)
var_data_per_cluster  = lapply(data_per_cluster, var)
#log_marginal_prior
```

### Iperparametri con empirical Bayes

In questo primo esperimento, setto i parametri relativi a P0 con la funzione di empirical Bayes che abbiamo fatto.
Plotto i dati e la marginale del modello, che è 

$student-t(dof = \nu_0, loc = \mu_0, scale = \sqrt{\frac{k_0}{k_0+1}\sigma^2_0})$
```{r}
# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]

# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 1, varsig2 = 1)
mu0 = P0_hyparam$mu0;k0= P0_hyparam$k0;nu0=P0_hyparam$nu0;sigma0=P0_hyparam$sigma0

l_grid = 200
grid = seq(-25,15,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)

for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(  data = tibble(grid = grid, value = Prior_grid),
                color = 'black', aes(x=grid,y=value), size = 1.2,
                inherit.aes = F)  +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}
```
Sembra essere proprio la situazione che voglio, la prior abbraccia tutti e 4 i cluster



Provo a fissare $\tau$ e $\Lambda$ in modo intelligente.
Voglio che la probabilità a priori di avere $K=4$ cluster sia più alta possibile.

* f è la funzione da ottimizzare
```{r}
f <- function(x){
  -p_distinct_prior(k = K, n_j = c(n_j), 
                   gamma = c(x[1]), 
                   prior = "Poisson", lambda = x[2] )
}

opt = optim(par = c(0.1,0.1), #initial points
            method = "L-BFGS-B", # that's the only method that takes bounds
            fn  = f, #function to be minimized
            lower = c(0.001,0.001) #lower bound for variables
            )
opt

```

Parametrizzo la gamma secondo media e varianza. 

Se $X \sim gamma(a,b)$, allora $E[X] = a/b$ e $V(X) = a/b^2$.

Invece, se $X \sim gamma(\mu_x,V_x)$, allora $a = \frac{\mu_x^2}{V_x}$ e $b = \frac{\mu_x}{V_x}$

```{r}
# gamma
mu_gamma  = opt$par[1]
var_gamma = 0.01
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma
x_range_gamma = c(  max(0, mu_gamma-2.5*sqrt(var_gamma) ),
                    mu_gamma+2.5*sqrt(var_gamma) )
#lambda
mu_lambda  = opt$par[2]
var_lambda = 0.01
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)
x_range_lambda = c(  max(0, mu_lambda-2.5*sqrt(var_lambda) ),
                     mu_lambda+2.5*sqrt(var_lambda) )


par(mfrow = c(1,2))
plot_density(shape = a_gamma, rate = b_gamma,
             dist = "gamma", main = "gamma prior", use_x11_device = F,
             xlim = x_range_gamma)
plot_density(shape = a_lambda, rate = b_lambda,
             dist = "gamma", main = "lambda prior", use_x11_device = F,
             xlim = x_range_lambda)
```


```{r}
# Run  --------------------------------------------------------------------
niter  <-  1000
burnin <-  1000
thin   <- 1

#P0_hyparam = empirical_bayes_normalinvgamma(data = data)
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 100, varsig2 = 100, correction = 10)
# gamma
mu_gamma  = opt$par[1]
var_gamma = 0.01
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma

#lambda
mu_lambda  = opt$par[2]
var_lambda = 0.01
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)


option = set_options_marginal(
             "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "gamma0" = opt$par[1],"Lambda0" = opt$par[2], 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = NULL#real_partition
        )

# P0 Hyperparameters:
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,

# Process parameters
# old set:  "gamma0" = 1,"Lambda0" = 3, 
#           "alpha_gamma"=5, "beta_gamma"=0.5, "alpha_lambda"=1, "beta_lambda"=5,


#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)

```





```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```

```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)
```
Clustering osceno, è sempre su 4 cluster ma non va per niente bene

Le stime dei parametri $\mu$ e $\sigma$ sono praticamente uguali al caso in cui la partizione viene tenuta fissa (infatti il clustering è perfetto, non è strano, anzi è quello che c'era da aspettarsi). Ovviamente la predittiva fa schifo tanto quanto l'altro caso
```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
#Pred_all = predictive_all_groups(grid = grid, fit = GDFMM, burnin = 1)

# Predictive in one group
Pred_one_group = predictive_marginal(idx_group = 1, grid = grid, fit = GDFMM, 
                                     option = option, burnin = 0)

j = 1
idx_start = 1
idx_end = n_j[1]
tibble(value = data[j,],
       true_clus = as.factor(real_partition[idx_start:idx_end])
       ) %>%
  ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
  geom_point(y = rep(0.005,length(data[j,])), size = 2) +
  geom_histogram(data = tibble(value = data[j,]),
                 aes(x=value, y = ..density..), 
                 color = 'black', alpha = 0.3,
                 binwidth = 0.5, inherit.aes = F) +
  scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
  scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] )  +
  geom_path( data = as_tibble( t(Pred_one_group) ),
             color = 'black', aes(x=grid,y=`50%`), size = 1.2,
             inherit.aes = F)  +
  labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
  geom_ribbon(data = as_tibble( t(Pred_one_group) ), fill = mycol[1],
              aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
              alpha = 0.5, inherit.aes = F)

```




### Iperparametri a caso

Faccio come prima ma gli iperparametri gli scelgo "a caso", con dei valori selezionati senza guardare i dati
```{r}
# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]

# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra

mu0 = 0;k0= 1/10;nu0=10;sigma0=1

l_grid = 200
grid = seq(-25,15,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)

for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(  data = tibble(grid = grid, value = Prior_grid),
                color = 'black', aes(x=grid,y=value), size = 1.2,
                inherit.aes = F)  +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}
```



Provo a fissare $\tau$ e $\Lambda$ in modo intelligente.
Voglio che la probabilità a priori di avere $K=4$ cluster sia più alta possibile.

* f è la funzione da ottimizzare
```{r}
f <- function(x){
  -p_distinct_prior(k = K, n_j = c(n_j), 
                   gamma = c(x[1]), 
                   prior = "Poisson", lambda = x[2] )
}

opt = optim(par = c(0.1,0.1), #initial points
            method = "L-BFGS-B", # that's the only method that takes bounds
            fn  = f, #function to be minimized
            lower = c(0.001,0.001) #lower bound for variables
            )
opt

```

Parametrizzo la gamma secondo media e varianza. 

Se $X \sim gamma(a,b)$, allora $E[X] = a/b$ e $V(X) = a/b^2$.

Invece, se $X \sim gamma(\mu_x,V_x)$, allora $a = \frac{\mu_x^2}{V_x}$ e $b = \frac{\mu_x}{V_x}$

```{r}
# gamma
mu_gamma  = opt$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma
x_range_gamma = c(  max(0, mu_gamma-2*sqrt(var_gamma) ),
                    mu_gamma+2*sqrt(var_gamma) )
#lambda
mu_lambda  = opt$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)
x_range_lambda = c(  max(0, mu_lambda-2*sqrt(var_lambda) ),
                     mu_lambda+2*sqrt(var_lambda) )


par(mfrow = c(1,2))
plot_density(shape = a_gamma, rate = b_gamma,
             dist = "gamma", main = "gamma prior", use_x11_device = F,
             xlim = x_range_gamma)
plot_density(shape = a_lambda, rate = b_lambda,
             dist = "gamma", main = "lambda prior", use_x11_device = F,
             xlim = x_range_lambda)
```

In questo momento, se parto dalla partizione vera OK (anche se U può crescere tanto) ma se parto da quella nulla malissimo perché U potrebbe anche esplodere completamente (1e+25). 
```{r}
# Run  --------------------------------------------------------------------
niter  <-  1000
burnin <-    1
thin   <- 1

P0_hyparam = empirical_bayes_normalinvgamma(data = data)
# gamma
mu_gamma  = opt$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma

#lambda
mu_lambda  = opt$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)


option = set_options_marginal(
             "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234,
             "sp_mala_U" = 0.01, "sp_mala_gamma"=10,
             "gamma0" = opt$par[1],"Lambda0" = opt$par[2], 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = real_partition#seq(1:length(real_partition))
        )

# P0 Hyperparameters:
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,

# Process parameters
# old set:  "gamma0" = 1,"Lambda0" = 3, 
#           "alpha_gamma"=5, "beta_gamma"=0.5, "alpha_lambda"=1, "beta_lambda"=5,


#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)

```

```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j) )
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```

```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)
```
male, sia il clustering che la predittiva.


```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
#Pred_all = predictive_all_groups(grid = grid, fit = GDFMM, burnin = 1)

# Predictive in one group
Pred_one_group = predictive_marginal(idx_group = 1, grid = grid, fit = GDFMM, 
                                     option = option, burnin = 0)

j = 1
idx_start = 1
idx_end = n_j[1]
tibble(value = data[j,],
       true_clus = as.factor(real_partition[idx_start:idx_end])
       ) %>%
  ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
  geom_point(y = rep(0.005,length(data[j,])), size = 2) +
  geom_histogram(data = tibble(value = data[j,]),
                 aes(x=value, y = ..density..), 
                 color = 'black', alpha = 0.3,
                 binwidth = 0.5, inherit.aes = F) +
  scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
  scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] )  +
  geom_path( data = as_tibble( t(Pred_one_group) ),
             color = 'black', aes(x=grid,y=`50%`), size = 1.2,
             inherit.aes = F)  +
  labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
  geom_ribbon(data = as_tibble( t(Pred_one_group) ), fill = mycol[1],
              aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
              alpha = 0.5, inherit.aes = F)

```

# Parametri fissati e partizione random

Questo esperimento è l'opposto di quello di prima. Ora la partizione è random mentre $\mu$ e $\sigma$ fissati.

Domanda: ma cosa succede se tengo fissi i parametri e poi K viene aggiornato?

```{r}
suppressWarnings(suppressPackageStartupMessages(library(GDFMM)))
suppressWarnings(suppressPackageStartupMessages(library(ACutils)))
suppressWarnings(suppressPackageStartupMessages(library(tidyverse)))
suppressWarnings(suppressPackageStartupMessages(library(RColorBrewer)))
suppressWarnings(suppressPackageStartupMessages(library(salso)))
suppressWarnings(suppressPackageStartupMessages(library(wesanderson)))
suppressWarnings(suppressPackageStartupMessages(library(mcclust.ext)))



# color palette -----------------------------------------------------------
mycol = hcl.colors(n=3,palette = "Zissou1")

# data generation ---------------------------------------------------------

d = 1                   # number of groups
K = 4                   # number of global clusters
mu = c(-20,-10,0, 10)   # vectors of means
sd = c(1,1,1,1)         # vector of sd
n_j = rep(200, d)        # set cardinality of the groups
seed = 20051131

genD = generate_data(d = d, K = K, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition

mycol_cluster = brewer.pal(n=K, name = "Dark2")

```

```{r}
counter = 1
data_per_cluster = vector("list", length = K)
log_marginal_prior = vector("list", length = d)
for(j in 1:d){
  log_marginal_prior[[j]] = vector( length = n_j[j])
  for(i in 1:n_j[j]){
    c_ji = real_partition[counter]
    data_per_cluster[[c_ji]] = c(data_per_cluster[[c_ji]], data[j,i])
    log_marginal_prior[[j]][i] = log(ACutils::dnct(data[j,i], n0 = 10, mu0 = 1, gamma0 = sqrt(2)) )
    counter = counter + 1
  }
  
}
#data_per_cluster 
mean_data_per_cluster = lapply(data_per_cluster, mean)
var_data_per_cluster  = lapply(data_per_cluster, var)
#log_marginal_prior
```

### Iperparametri con empirical Bayes

In questo primo esperimento, setto i parametri relativi a P0 con la funzione di empirical Bayes che abbiamo fatto.
Plotto i dati e la marginale del modello, che è 

$student-t(dof = \nu_0, loc = \mu_0, scale = \sqrt{\frac{k_0}{k_0+1}\sigma^2_0})$
```{r}
# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]

# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 1, varsig2 = 1)
mu0 = P0_hyparam$mu0;k0= P0_hyparam$k0;nu0=P0_hyparam$nu0;sigma0=P0_hyparam$sigma0

l_grid = 200
grid = seq(-25,15,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)

for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(  data = tibble(grid = grid, value = Prior_grid),
                color = 'black', aes(x=grid,y=value), size = 1.2,
                inherit.aes = F)  +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}
```
Sembra essere proprio la situazione che voglio, la prior abbraccia tutti e 4 i cluster


```{r}
# Run  --------------------------------------------------------------------
niter  <- 500
burnin <- 500
thin   <- 1

P0_hyparam = empirical_bayes_normalinvgamma(data = data)

option = set_options_marginal(
             "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "gamma0" = 1,"Lambda0" = 3, 
             "alpha_gamma"=5, "beta_gamma"=0.5, 
             "alpha_lambda"=1, "beta_lambda"=5,
             "init_mean_cluster" = unlist(mean_data_per_cluster), "init_var_cluster" = unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = F, "UpdateLambda" = T, 
             "partition" = real_partition
        )
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)

```
```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```

```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)
```
Clustering perfetto, però la catena non ha mixato la partizione

Le stime dei parametri $\mu$ e $\sigma$ sono praticamente uguali al caso in cui la partizione viene tenuta fissa (infatti il clustering è perfetto, non è strano, anzi è quello che c'era da aspettarsi). Ovviamente la predittiva fa schifo tanto quanto l'altro caso
```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
#Pred_all = predictive_all_groups(grid = grid, fit = GDFMM, burnin = 1)

# Predictive in one group
Pred_one_group = predictive_marginal(idx_group = 1, grid = grid, fit = GDFMM, 
                                     option = option, burnin = 0)

j = 1
idx_start = 1
idx_end = n_j[1]
tibble(value = data[j,],
       true_clus = as.factor(real_partition[idx_start:idx_end])
       ) %>%
  ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
  geom_point(y = rep(0.005,length(data[j,])), size = 2) +
  geom_histogram(data = tibble(value = data[j,]),
                 aes(x=value, y = ..density..), 
                 color = 'black', alpha = 0.3,
                 binwidth = 0.5, inherit.aes = F) +
  scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
  scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] )  +
  geom_path( data = as_tibble( t(Pred_one_group) ),
             color = 'black', aes(x=grid,y=`50%`), size = 1.2,
             inherit.aes = F)  +
  labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
  geom_ribbon(data = as_tibble( t(Pred_one_group) ), fill = mycol[1],
              aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
              alpha = 0.5, inherit.aes = F)

```




### Iperparametri a caso

Faccio come prima ma gli iperparametri gli scelgo "a caso", con dei valori selezionati senza guardare i dati
```{r}
# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]

# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra

mu0 = 0;k0= 1/10;nu0=10;sigma0=1

l_grid = 200
grid = seq(-25,15,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)

for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(  data = tibble(grid = grid, value = Prior_grid),
                color = 'black', aes(x=grid,y=value), size = 1.2,
                inherit.aes = F)  +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}
```

```{r}
# Run  --------------------------------------------------------------------
niter  <- 500
burnin <- 500
thin   <- 1

P0_hyparam = empirical_bayes_normalinvgamma(data = data)

option = set_options_marginal(
             "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "gamma0" = 1,"Lambda0" = 3, 
             "alpha_gamma"=5, "beta_gamma"=0.5, 
             "alpha_lambda"=1, "beta_lambda"=5,
             "init_mean_cluster" = unlist(mean_data_per_cluster), "init_var_cluster" = unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = real_partition
        )
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = T, option = option)

```


```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```

```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)
```
Clustering perfetto, però la catena non ha mixato la partizione

Le stime dei parametri $\mu$ e $\sigma$ sono praticamente uguali al caso in cui la partizione viene tenuta fissa (infatti il clustering è perfetto, non è strano, anzi è quello che c'era da aspettarsi). Ovviamente la predittiva fa schifo tanto quanto l'altro caso
```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
#Pred_all = predictive_all_groups(grid = grid, fit = GDFMM, burnin = 1)

# Predictive in one group
Pred_one_group = predictive_marginal(idx_group = 1, grid = grid, fit = GDFMM, 
                                     option = option, burnin = 0)

j = 1
idx_start = 1
idx_end = n_j[1]
tibble(value = data[j,],
       true_clus = as.factor(real_partition[idx_start:idx_end])
       ) %>%
  ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
  geom_point(y = rep(0.005,length(data[j,])), size = 2) +
  geom_histogram(data = tibble(value = data[j,]),
                 aes(x=value, y = ..density..), 
                 color = 'black', alpha = 0.3,
                 binwidth = 0.5, inherit.aes = F) +
  scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
  scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] )  +
  geom_path( data = as_tibble( t(Pred_one_group) ),
             color = 'black', aes(x=grid,y=`50%`), size = 1.2,
             inherit.aes = F)  +
  labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
  geom_ribbon(data = as_tibble( t(Pred_one_group) ), fill = mycol[1],
              aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
              alpha = 0.5, inherit.aes = F)

```



