---
title: "Marginal Sampler with 1 level only"
author: "Alessandro Colombi"
date: "30/8/2022"
output: html_document
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  out.width = "50%"
)
```


Testing Marginal sampler, when $d=1$. Density estimation is now implemented.


*Dati esperimento:*

* **K = 4**
* d = 1
* $n_1 = 200$
* $\mu_1 = -20$, $\mu_1 = -10$, $\mu_3 = -20$, $\mu_4 = -20$
* $\sigma_1 = \sigma_2 = \sigma_3 = \sigma_4 = 1$
* $\pi_1 = \pi_2 = \pi_3 = \pi_4 = 1/4$

# Partizione Fissata

```{r}
suppressWarnings(suppressPackageStartupMessages(library(GDFMM)))
suppressWarnings(suppressPackageStartupMessages(library(ACutils)))
suppressWarnings(suppressPackageStartupMessages(library(tidyverse)))
suppressWarnings(suppressPackageStartupMessages(library(RColorBrewer)))
suppressWarnings(suppressPackageStartupMessages(library(salso)))
suppressWarnings(suppressPackageStartupMessages(library(wesanderson)))
suppressWarnings(suppressPackageStartupMessages(library(mcclust.ext)))



# color palette -----------------------------------------------------------
mycol = hcl.colors(n=3,palette = "Zissou1")

# data generation ---------------------------------------------------------

d = 1                   # number of groups
K = 4                   # number of global clusters
mu = c(-20,-10,0, 10)   # vectors of means
sd = c(1,1,1,1)         # vector of sd
n_j = rep(200, d)        # set cardinality of the groups
seed = 20051131

#genD = generate_data(d = d, K = K, mu = mu, sd = sd, n_j = n_j, seed = seed)
genD = simulate_data(d = d, K = K, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition

mycol_cluster = brewer.pal(n=K, name = "Dark2")

```

```{r}
counter = 1
data_per_cluster = vector("list", length = K)
log_marginal_prior = vector("list", length = d)
for(j in 1:d){
  log_marginal_prior[[j]] = vector( length = n_j[j])
  for(i in 1:n_j[j]){
    c_ji = real_partition[counter]
    data_per_cluster[[c_ji]] = c(data_per_cluster[[c_ji]], data[j,i])
    log_marginal_prior[[j]][i] = log(ACutils::dnct(data[j,i], n0 = 10, mu0 = 1, gamma0 = sqrt(2)) )
    counter = counter + 1
  }
  
}
#data_per_cluster 
mean_data_per_cluster = lapply(data_per_cluster, mean)
var_data_per_cluster  = lapply(data_per_cluster, var)
#log_marginal_prior
```

## Iperparametri con empirical Bayes

In questo primo esperimento, setto i parametri relativi a $P_0$ con la funzione di empirical Bayes che abbiamo fatto.
Plotto i dati e la marginale del modello, che è 

$$student-t(dof = \nu_0, loc = \mu_0, scale = \sqrt{\frac{k_0}{k_0+1}\sigma^2_0})$$
```{r}
# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]

# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 1, varsig2 = 1)
mu0 = P0_hyparam$mu0;k0= P0_hyparam$k0;nu0=P0_hyparam$nu0;sigma0=P0_hyparam$sigma0

l_grid = 200
grid = seq(-25,15,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)

for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(  data = tibble(grid = grid, value = Prior_grid),
                color = 'black', aes(x=grid,y=value), size = 1.2,
                inherit.aes = F)  +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}
```
Sembra essere proprio la situazione che voglio, la prior abbraccia tutti e 4 i cluster


```{r}
# Run  --------------------------------------------------------------------
niter  <- 1000
burnin <- 1000
thin   <- 1

P0_hyparam = empirical_bayes_normalinvgamma(data = data)

option = set_options_marginal(
             "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = 1,"Lambda0" = 3, 
             "alpha_gamma"=5, "beta_gamma"=0.5, 
             "alpha_lambda"=1, "beta_lambda"=5,
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = real_partition
        )
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = T, option = option)

```




Analisi parte parametrica con partizione fissa
```{r}
mu_est = rep(0,K)
s2_est = rep(1,K)

for(it in 1:niter){
  mu_est = mu_est + GDFMM$mu[[it]]
  s2_est = s2_est + GDFMM$sigma[[it]]
}
mu_est = mu_est/niter
s2_est = s2_est/niter

mu_est
s2_est
```
Le stime fanno schifo, se le medie ancora ancora possono ricordare quelle vere, le varianze sono gigantesche

```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
#Pred_all = predictive_all_groups(grid = grid, fit = GDFMM, burnin = 1)

# Predictive in one group
Pred_one_group = predictive_marginal(idx_group = 1, grid = grid, fit = GDFMM, option = option)

j = 1
idx_start = 1
idx_end = n_j[1]
tibble(value = data[j,],
       true_clus = as.factor(real_partition[idx_start:idx_end])
       ) %>%
  ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
  geom_point(y = rep(0.005,length(data[j,])), size = 2) +
  geom_histogram(data = tibble(value = data[j,]),
                 aes(x=value, y = ..density..), 
                 color = 'black', alpha = 0.3,
                 binwidth = 0.5, inherit.aes = F) +
  scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
  scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] )  +
  geom_path( data = as_tibble( t(Pred_one_group) ),
             color = 'black', aes(x=grid,y=`50%`), size = 1.2,
             inherit.aes = F)  +
  labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
  geom_ribbon(data = as_tibble( t(Pred_one_group) ), fill = mycol[1],
              aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
              alpha = 0.5, inherit.aes = F)

```
La predittiva non c'entra niente con i dati. 


## Iperparametri di default

La partizione la tengo sempre fissa, questa volta però gli iperparametri non li scelgo con la funzione di empirical Bayes ma con le scelte di default che erano maturate con un po' di esperienza sul codice. In particolare:

$\mu_0 = 0;k_0= 1/10;\nu_0=10;\sigma^2_0=1$
```{r}
# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]

# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra

mu0 = 0;k0= 1/10;nu0=10;sigma0=1

l_grid = 200
grid = seq(-25,15,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)

for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(  data = tibble(grid = grid, value = Prior_grid),
                color = 'black', aes(x=grid,y=value), size = 1.2,
                inherit.aes = F)  +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}
```
Non sembra essere per niente una buona marginale (a priori) dei dati, è tutta schiacciata su un solo cluster 


```{r}
# Run  --------------------------------------------------------------------
niter  <- 1000
burnin <- 1000
thin   <- 1

P0_hyparam = empirical_bayes_normalinvgamma(data = data)

option = set_options_marginal(
             "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = 1,"Lambda0" = 3, 
             "alpha_gamma"=5, "beta_gamma"=0.5, 
             "alpha_lambda"=1, "beta_lambda"=5,
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = real_partition
        )
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = T, option = option)

```




Analisi parte parametrica con partizione fissa
```{r}
mu_est = rep(0,K)
s2_est = rep(1,K)

for(it in 1:niter){
  mu_est = mu_est + GDFMM$mu[[it]]
  s2_est = s2_est + GDFMM$sigma[[it]]
}
mu_est = mu_est/niter
s2_est = s2_est/niter

mu_est
s2_est
```
Le stime sono quasi perfette
```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
#Pred_all = predictive_all_groups(grid = grid, fit = GDFMM, burnin = 1)

# Predictive in one group
Pred_one_group = predictive_marginal(idx_group = 1, grid = grid, fit = GDFMM, 
                                     option = option, burnin = 0)

j = 1
idx_start = 1
idx_end = n_j[1]
tibble(value = data[j,],
       true_clus = as.factor(real_partition[idx_start:idx_end])
       ) %>%
  ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
  geom_point(y = rep(0.005,length(data[j,])), size = 2) +
  geom_histogram(data = tibble(value = data[j,]),
                 aes(x=value, y = ..density..), 
                 color = 'black', alpha = 0.3,
                 binwidth = 0.5, inherit.aes = F) +
  scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
  scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] )  +
  geom_path( data = as_tibble( t(Pred_one_group) ),
             color = 'black', aes(x=grid,y=`50%`), size = 1.2,
             inherit.aes = F)  +
  labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
  geom_ribbon(data = as_tibble( t(Pred_one_group) ), fill = mycol[1],
              aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
              alpha = 0.5, inherit.aes = F)

```
La predittiva è proprio dove dovrebbe essere 


Provo a spiegare l'evidenza osservata:

* Dal punto di vista della marginale $m(X)$, la $P_0$ scelta con iperparametri scelti con empirical Bayes sembra molto bella perché abbraccia bene i dati. Se scelgo invece "a caso" ho una $m(X)$ schiacciata solo sui dati vicini allo zero (perché metto $\mu_0 = 0$), quindi sembra piuttosto brutta. 

* Dal punto di vista delle medie a posteriori, abbiamo

$\mu_k | X_1,\dots,X_n \sim N(\frac{n_k}{n_k + k_0}\bar{X_k} + \frac{k_0}{n_k + k_0}\mu_0 )$

$\sigma^2_k | X_1,\dots,X_n \sim InvGamma(\frac{\nu_0 + n_k}{2}, \frac{1}{2}(\nu_0\sigma_0^2 + (n_k-1)V_k + \frac{n_k k_0}{n_k + k_0}(\bar{X}_k - \mu_0)^2))$

dove $n_k$ è la numerosità, $\bar{X}_k$ è la media campionaria e $V_k$ è la varianza campionaria del cluster $k$.
In particolare, con empircal bayes vengono dei valori molto alti di $k_0$ e quindi per esempio il peso legato alla prior nella media a posteriori è molto simile a quello dato all'evidenza dei dati $\frac{n_k}{n_k + k_0} \approx  \frac{k_0}{n_k + k_0}$. Per la varianza invece viene $\nu_0\sigma_0^2$ molto più alto degli altri termini.

* Quindi, dalla media a posteriori mi viene da dire che sarebbe meglio avere $k_0$ basso in modo da essere meno informativi sulla media
  - In Empirical Bayes mettiamo $k_0 = \frac{\bar{\sigma^2}}{\bar{V_\mu}}$, quindi è meglio aumentare $\bar{V_\mu}$. Per i      ragionamenti che ho fatto sulla varianza, vuol dire diminuire un po' la varianza di $X$. 
    Bello che $k_0$ sia l'unico valore a dipendere da $\bar{V_{\mu}}$, quindi posso regolarlo direttamente.
    

* Per $\nu_0$ viene scelto un valore enorme, tipo $2700$. Quindi in $\sigma_k$ conta solo la parte dalla prior
  - Per contenere $\nu$ posso aumentare $\bar{V_\sigma}$, ha anche un po' di impatto su $\sigma_0^2$. 
  - Per ridurre tutto, si può lavorare anche sul parametro di correzione: per fare matching dei momenti, metto
    $E[\sigma^2] = \bar{\sigma^2}/correction$. Seguendo il lavoro di Raf avevo sempre considerato $correction = 3$ 
    ma penso sia meglio alzarlo.
  
* La contraddizione però resta, tutti questi interventi hanno reso la $m(X)$ molto più concentrata invece che diffusa

## Iperparametri con empirical Bayes - Modificati

Faccio un terzo esperimento settando i parametri con empirical bayes seguendo le intuizioni elencate al punto precedente
```{r}
# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]

# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 100, varsig2 = 100, correction = 10)
mu0 = P0_hyparam$mu0;k0= P0_hyparam$k0;nu0=P0_hyparam$nu0;sigma0=P0_hyparam$sigma0

l_grid = 200
grid = seq(-25,15,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)

for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(  data = tibble(grid = grid, value = Prior_grid),
                color = 'black', aes(x=grid,y=value), size = 1.2,
                inherit.aes = F)  +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}
```
Un po' più larga che con i valori a caso ma molto più stretta rispetto a prima.


```{r}
# Run  --------------------------------------------------------------------
niter  <-  1000
burnin <-  1000
thin   <- 1

P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 1000, varsig2 = 1000, correction = 10)

option = set_options_marginal(
             "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = 1,"Lambda0" = 3, 
             "alpha_gamma"=5, "beta_gamma"=0.5, 
             "alpha_lambda"=1, "beta_lambda"=5,
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = real_partition
        )
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = T, option = option)

```




Analisi parte parametrica con partizione fissa
```{r}
mu_est = rep(0,K)
s2_est = rep(0,K)

for(it in 1:niter){
  mu_est = mu_est + GDFMM$mu[[it]]
  s2_est = s2_est + GDFMM$sigma[[it]]
}
mu_est = mu_est/niter
s2_est = s2_est/niter

mu_est
s2_est
```
Ora le stime tornano ad essere piuttosto buone

```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
#Pred_all = predictive_all_groups(grid = grid, fit = GDFMM, burnin = 1)

# Predictive in one group
Pred_one_group = predictive_marginal(idx_group = 1, grid = grid, fit = GDFMM, option = option)

j = 1
idx_start = 1
idx_end = n_j[1]
tibble(value = data[j,],
       true_clus = as.factor(real_partition[idx_start:idx_end])
       ) %>%
  ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
  geom_point(y = rep(0.005,length(data[j,])), size = 2) +
  geom_histogram(data = tibble(value = data[j,]),
                 aes(x=value, y = ..density..), 
                 color = 'black', alpha = 0.3,
                 binwidth = 0.5, inherit.aes = F) +
  scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
  scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] )  +
  geom_path( data = as_tibble( t(Pred_one_group) ),
             color = 'black', aes(x=grid,y=`50%`), size = 1.2,
             inherit.aes = F)  +
  labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
  geom_ribbon(data = as_tibble( t(Pred_one_group) ), fill = mycol[1],
              aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
              alpha = 0.5, inherit.aes = F)

```
Anche la predittiva è dove dovrebbe essere



# Partizione aleatoria

Rispetto alla prime sezione, ora lascio variare anche la partizione. Quindi tutti i parametri entrano in gioco e devono essere aggiornati. Come prima, analizzerò 3 modi per settare gli iperparametri di $P_0$: con empirical Bayes, con le scelte di default e con empirical Bayes modificato. Vedremo che con la prima scelta, ancora una volta non abbiamo risultati accettabili. Con gli altri due si, dunque per questi due casi analizzo 3 situazioni diverse sulla partizione iniziale: quella vera, quella dove tutti sono in un unico cluster e quella in cui ognuno fa un cluster a sé.

Inolte, discuto anche una procedura su come scegliere i valori iniziali e gli iperparametri per $\gamma$ e $\Lambda$.

```{r}
suppressWarnings(suppressPackageStartupMessages(library(GDFMM)))
suppressWarnings(suppressPackageStartupMessages(library(ACutils)))
suppressWarnings(suppressPackageStartupMessages(library(tidyverse)))
suppressWarnings(suppressPackageStartupMessages(library(RColorBrewer)))
suppressWarnings(suppressPackageStartupMessages(library(salso)))
suppressWarnings(suppressPackageStartupMessages(library(wesanderson)))
suppressWarnings(suppressPackageStartupMessages(library(mcclust.ext)))



# color palette -----------------------------------------------------------
mycol = hcl.colors(n=3,palette = "Zissou1")

# data generation ---------------------------------------------------------

d = 1                   # number of groups
K = 4                   # number of global clusters
mu = c(-20,-10,0, 10)   # vectors of means
sd = c(1,1,1,1)         # vector of sd
n_j = rep(200, d)        # set cardinality of the groups
seed = 2352571

#genD = generate_data(d = d, K = K, mu = mu, sd = sd, n_j = n_j, seed = seed)
genD = simulate_data(d = d, K = K, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition

mycol_cluster = brewer.pal(n=K, name = "Dark2")

table(real_partition)
```


```{r}
counter = 1
data_per_cluster = vector("list", length = K)
log_marginal_prior = vector("list", length = d)
for(j in 1:d){
  log_marginal_prior[[j]] = vector( length = n_j[j])
  for(i in 1:n_j[j]){
    c_ji = real_partition[counter]
    data_per_cluster[[c_ji]] = c(data_per_cluster[[c_ji]], data[j,i])
    log_marginal_prior[[j]][i] = log(ACutils::dnct(data[j,i], n0 = 10, mu0 = 1, gamma0 = sqrt(2)) )
    counter = counter + 1
  }
  
}
#data_per_cluster 
mean_data_per_cluster = lapply(data_per_cluster, mean)
var_data_per_cluster  = lapply(data_per_cluster, var)

mean_data_per_cluster

var_data_per_cluster
#log_marginal_prior
```


Provo a fissare $\gamma$ e $\Lambda$ in modo intelligente.
Voglio che la probabilità a priori di avere $K=4$ cluster sia più alta possibile.


```{r}
# f è la funzione da ottimizzare
f <- function(x){
  -p_distinct_prior(k = K, n_j = c(n_j), 
                   gamma = c(x[1]), 
                   prior = "Poisson", lambda = x[2] )
}

opt = optim(par = c(0.1,0.1), #initial points
            method = "L-BFGS-B", # that's the only method that takes bounds
            fn  = f, #function to be minimized
            lower = c(0.001,0.001) #lower bound for variables
            )
opt

```

Parametrizzo la gamma secondo media e varianza. Questo perché ho trovato dei valori di $\gamma$ e $\Lambda$ che a priori mi piacciono, quindi voglio settare delle prior centrate su quei valori e regolare la varianza attorno ad essi.

Se $X \sim gamma(a,b)$, allora $E[X] = a/b$ e $V(X) = a/b^2$.

Invece, se $X \sim gamma(\mu_x,V_x)$, allora $a = \frac{\mu_x^2}{V_x}$ e $b = \frac{\mu_x}{V_x}$

```{r}
# gamma
mu_gamma  = opt$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma
x_range_gamma = c(  max(0, mu_gamma-2*sqrt(var_gamma) ),
                    mu_gamma+2*sqrt(var_gamma) )
#lambda
mu_lambda  = opt$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)
x_range_lambda = c(  max(0, mu_lambda-2*sqrt(var_lambda) ),
                     mu_lambda+2*sqrt(var_lambda) )


par(mfrow = c(1,2))
plot_density(shape = a_gamma, rate = b_gamma,
             dist = "gamma", main = "gamma prior", use_x11_device = F,
             xlim = x_range_gamma)
plot_density(shape = a_lambda, rate = b_lambda,
             dist = "gamma", main = "lambda prior", use_x11_device = F,
             xlim = x_range_lambda)
```


## Iperparametri con empirical Bayes

In questo primo esperimento, setto i parametri relativi a $P_0$ con la funzione di empirical Bayes che abbiamo fatto.
Plotto i dati e la marginale del modello, che è 

$student-t(dof = \nu_0, loc = \mu_0, scale = \sqrt{\frac{k_0}{k_0+1}\sigma^2_0})$
```{r}
# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]

# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 1, varsig2 = 1)
mu0 = P0_hyparam$mu0;k0= P0_hyparam$k0;nu0=P0_hyparam$nu0;sigma0=P0_hyparam$sigma0

l_grid = 200
grid = seq(-25,15,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)

for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(  data = tibble(grid = grid, value = Prior_grid),
                color = 'black', aes(x=grid,y=value), size = 1.2,
                inherit.aes = F)  +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}
```
Sembra essere proprio la situazione che voglio, la prior abbraccia tutti e 4 i cluster

*Parto dalla partizione vera*
```{r}
# Run  --------------------------------------------------------------------
niter  <-  1000
burnin <-  1000
thin   <- 1

# tau 
P0_hyparam = empirical_bayes_normalinvgamma(data = data)

# gamma
mu_gamma  = opt$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma

#lambda
mu_lambda  = opt$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)


option = set_options_marginal(
             "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = opt$par[1],"Lambda0" = opt$par[2], 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = real_partition
        )

# P0 Hyperparameters:
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,

# Process parameters
# old set:  "gamma0" = 1,"Lambda0" = 3, 
#           "alpha_gamma"=5, "beta_gamma"=0.5, "alpha_lambda"=1, "beta_lambda"=5,


#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```


```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```

```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)
```
Clustering osceno, è sempre su 2 cluster ma non va per niente bene. Inoltre la $U$ e la $\gamma$ hanno traceplot bruttissimi.

Le stime dei parametri $\mu$ e $\sigma$ sono praticamente uguali al caso in cui la partizione viene tenuta fissa (in particolare le varianze sono grandissime). Ovviamente la predittiva fa schifo tanto quanto l'altro caso
```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
#Pred_all = predictive_all_groups(grid = grid, fit = GDFMM, burnin = 1)

# Predictive in one group
Pred_one_group = predictive_marginal(idx_group = 1, grid = grid, fit = GDFMM, 
                                     option = option, burnin = 0)

j = 1
idx_start = 1
idx_end = n_j[1]
tibble(value = data[j,],
       true_clus = as.factor(real_partition[idx_start:idx_end])
       ) %>%
  ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
  geom_point(y = rep(0.005,length(data[j,])), size = 2) +
  geom_histogram(data = tibble(value = data[j,]),
                 aes(x=value, y = ..density..), 
                 color = 'black', alpha = 0.3,
                 binwidth = 0.5, inherit.aes = F) +
  scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
  scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] )  +
  geom_path( data = as_tibble( t(Pred_one_group) ),
             color = 'black', aes(x=grid,y=`50%`), size = 1.2,
             inherit.aes = F)  +
  labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
  geom_ribbon(data = as_tibble( t(Pred_one_group) ), fill = mycol[1],
              aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
              alpha = 0.5, inherit.aes = F)

```




## Iperparametri di default

Faccio come prima ma gli iperparametri gli scelgo "a caso", con dei valori selezionati senza guardare i dati
```{r}
# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]

# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra

mu0 = 0;k0= 1/10;nu0=10;sigma0=1

l_grid = 200
grid = seq(-25,15,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)

for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(  data = tibble(grid = grid, value = Prior_grid),
                color = 'black', aes(x=grid,y=value), size = 1.2,
                inherit.aes = F)  +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}
```

```{r}
# gamma
mu_gamma  = opt$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma
x_range_gamma = c(  max(0, mu_gamma-2*sqrt(var_gamma) ),
                    mu_gamma+2*sqrt(var_gamma) )
#lambda
mu_lambda  = opt$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)
x_range_lambda = c(  max(0, mu_lambda-2*sqrt(var_lambda) ),
                     mu_lambda+2*sqrt(var_lambda) )


par(mfrow = c(1,2))
plot_density(shape = a_gamma, rate = b_gamma,
             dist = "gamma", main = "gamma prior", use_x11_device = F,
             xlim = x_range_gamma)
plot_density(shape = a_lambda, rate = b_lambda,
             dist = "gamma", main = "lambda prior", use_x11_device = F,
             xlim = x_range_lambda)
```


### Parto da partizione vera
```{r}
# Run  --------------------------------------------------------------------
niter  <-  1000
burnin <-  1000
thin   <- 1

# tau 
P0_hyparam = empirical_bayes_normalinvgamma(data = data)

# gamma
mu_gamma  = opt$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma

#lambda
mu_lambda  = opt$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)


option = set_options_marginal(
             "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = opt$par[1],"Lambda0" = opt$par[2], 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = real_partition
        )

# P0 Hyperparameters:
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,

# Process parameters
# old set:  "gamma0" = 1,"Lambda0" = 3, 
#           "alpha_gamma"=5, "beta_gamma"=0.5, "alpha_lambda"=1, "beta_lambda"=5,


#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```

```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```

```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)
```



```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
#Pred_all = predictive_all_groups(grid = grid, fit = GDFMM, burnin = 1)

# Predictive in one group
Pred_one_group = predictive_marginal(idx_group = 1, grid = grid, fit = GDFMM, 
                                     option = option, burnin = 0)

j = 1
idx_start = 1
idx_end = n_j[1]
tibble(value = data[j,],
       true_clus = as.factor(real_partition[idx_start:idx_end])
       ) %>%
  ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
  geom_point(y = rep(0.005,length(data[j,])), size = 2) +
  geom_histogram(data = tibble(value = data[j,]),
                 aes(x=value, y = ..density..), 
                 color = 'black', alpha = 0.3,
                 binwidth = 0.5, inherit.aes = F) +
  scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
  scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] )  +
  geom_path( data = as_tibble( t(Pred_one_group) ),
             color = 'black', aes(x=grid,y=`50%`), size = 1.2,
             inherit.aes = F)  +
  labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
  geom_ribbon(data = as_tibble( t(Pred_one_group) ), fill = mycol[1],
              aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
              alpha = 0.5, inherit.aes = F)

```


Riassunto:

* $\Lambda$ è molto stabile sui valori scelti dalla prior
* $\gamma$ e $U$ hanno delle brutte catene, sembrano molto autocorrelate.
* Sensibilità rispetto al parametro MALA $s_p$: se aumenta ($0.1$), la catena di $U$ esplora anche valori molto più alti mentre per quanto riguarda $\gamma$ non ci sono cambiamenti significativi. Se diminuisce ($0.001$) sembra ci voglia di più per la convergenza, $\gamma$ diminuisce e $U$ non esplode su valori grandi. Il clustering si assesta su partizioni più parsimoniose ma a nei risultati finali non ci sono differenze.  
* $K$ numero di cluster *NON* è quello corretto, è più alto ma si muove tanto, non rimane fisso. Sembra mixxare molto.
* A posteriori però ritrovo il clustering esatto 
* La stima della densità è come vorrei

### Parto da ognuno fa cluster da se
```{r}
# Run  --------------------------------------------------------------------
niter  <-  1000
burnin <-  2000
thin   <- 1

# tau 
P0_hyparam = empirical_bayes_normalinvgamma(data = data)

# gamma
mu_gamma  = opt$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma

#lambda
mu_lambda  = opt$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)


option = set_options_marginal(
             "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = opt$par[1],"Lambda0" = opt$par[2], 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = seq(1:length(real_partition))#real_partition
        )

# P0 Hyperparameters:
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,

# Process parameters
# old set:  "gamma0" = 1,"Lambda0" = 3, 
#           "alpha_gamma"=5, "beta_gamma"=0.5, "alpha_lambda"=1, "beta_lambda"=5,


#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```

```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```

```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)
```



```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
#Pred_all = predictive_all_groups(grid = grid, fit = GDFMM, burnin = 1)

# Predictive in one group
Pred_one_group = predictive_marginal(idx_group = 1, grid = grid, fit = GDFMM, 
                                     option = option, burnin = 0)

j = 1
idx_start = 1
idx_end = n_j[1]
tibble(value = data[j,],
       true_clus = as.factor(real_partition[idx_start:idx_end])
       ) %>%
  ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
  geom_point(y = rep(0.005,length(data[j,])), size = 2) +
  geom_histogram(data = tibble(value = data[j,]),
                 aes(x=value, y = ..density..), 
                 color = 'black', alpha = 0.3,
                 binwidth = 0.5, inherit.aes = F) +
  scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
  scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] )  +
  geom_path( data = as_tibble( t(Pred_one_group) ),
             color = 'black', aes(x=grid,y=`50%`), size = 1.2,
             inherit.aes = F)  +
  labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
  geom_ribbon(data = as_tibble( t(Pred_one_group) ), fill = mycol[1],
              aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
              alpha = 0.5, inherit.aes = F)

```

Riassunto:

* Ho aumentato il numero di iterazioni perché $\gamma$ non sembrava per niente a convergenza.
* Per il resto, le conclusioni sono tali e quali al caso in cui partivo dalla partizione vera.

### Parto da tutti in un unico cluster
```{r}
# Run  --------------------------------------------------------------------
niter  <-  2000
burnin <-  3000
thin   <- 1

# tau 
P0_hyparam = empirical_bayes_normalinvgamma(data = data)

# gamma
mu_gamma  = opt$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma

#lambda
mu_lambda  = opt$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)


option = set_options_marginal(
             "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = opt$par[1],"Lambda0" = opt$par[2], 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = NULL#real_partition
        )

# P0 Hyperparameters:
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,

# Process parameters
# old set:  "gamma0" = 1,"Lambda0" = 3, 
#           "alpha_gamma"=5, "beta_gamma"=0.5, "alpha_lambda"=1, "beta_lambda"=5,


#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```

```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```

```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)
```



```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
#Pred_all = predictive_all_groups(grid = grid, fit = GDFMM, burnin = 1)

# Predictive in one group
Pred_one_group = predictive_marginal(idx_group = 1, grid = grid, fit = GDFMM, 
                                     option = option, burnin = 0)

j = 1
idx_start = 1
idx_end = n_j[1]
tibble(value = data[j,],
       true_clus = as.factor(real_partition[idx_start:idx_end])
       ) %>%
  ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
  geom_point(y = rep(0.005,length(data[j,])), size = 2) +
  geom_histogram(data = tibble(value = data[j,]),
                 aes(x=value, y = ..density..), 
                 color = 'black', alpha = 0.3,
                 binwidth = 0.5, inherit.aes = F) +
  scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
  scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] )  +
  geom_path( data = as_tibble( t(Pred_one_group) ),
             color = 'black', aes(x=grid,y=`50%`), size = 1.2,
             inherit.aes = F)  +
  labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
  geom_ribbon(data = as_tibble( t(Pred_one_group) ), fill = mycol[1],
              aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
              alpha = 0.5, inherit.aes = F)

```

Riassunto:

* Questo è il caso più problematico, ho notato che cambiando il seed con cui genero i dati, a volte va bene e a volte no



## Iperparametri con empirical Bayes - Modificati

Seguo le indicazioni della sezione precedente
```{r}
# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]

# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 100, varsig2 = 100, correction = 10)
mu0 = P0_hyparam$mu0;k0= P0_hyparam$k0;nu0=P0_hyparam$nu0;sigma0=P0_hyparam$sigma0

l_grid = 200
grid = seq(-25,15,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)

for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(  data = tibble(grid = grid, value = Prior_grid),
                color = 'black', aes(x=grid,y=value), size = 1.2,
                inherit.aes = F)  +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}
```

```{r}
# gamma
mu_gamma  = opt$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma
x_range_gamma = c(  max(0, mu_gamma-2*sqrt(var_gamma) ),
                    mu_gamma+2*sqrt(var_gamma) )
#lambda
mu_lambda  = opt$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)
x_range_lambda = c(  max(0, mu_lambda-2*sqrt(var_lambda) ),
                     mu_lambda+2*sqrt(var_lambda) )


par(mfrow = c(1,2))
plot_density(shape = a_gamma, rate = b_gamma,
             dist = "gamma", main = "gamma prior", use_x11_device = F,
             xlim = x_range_gamma)
plot_density(shape = a_lambda, rate = b_lambda,
             dist = "gamma", main = "lambda prior", use_x11_device = F,
             xlim = x_range_lambda)
```


### Parto da partizione vera
```{r}
# Run  --------------------------------------------------------------------
niter  <-  1000
burnin <-  1000
thin   <- 1


# tau 
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 100, varsig2 = 100, correction = 10)

# gamma
mu_gamma  = opt$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma

#lambda
mu_lambda  = opt$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)


option = set_options_marginal(
             "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = opt$par[1],"Lambda0" = opt$par[2], 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = real_partition
        )

# P0 Hyperparameters:
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,

# Process parameters
# old set:  "gamma0" = 1,"Lambda0" = 3, 
#           "alpha_gamma"=5, "beta_gamma"=0.5, "alpha_lambda"=1, "beta_lambda"=5,


#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```

```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```

```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)
```



```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
#Pred_all = predictive_all_groups(grid = grid, fit = GDFMM, burnin = 1)

# Predictive in one group
Pred_one_group = predictive_marginal(idx_group = 1, grid = grid, fit = GDFMM, 
                                     option = option, burnin = 0)

j = 1
idx_start = 1
idx_end = n_j[1]
tibble(value = data[j,],
       true_clus = as.factor(real_partition[idx_start:idx_end])
       ) %>%
  ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
  geom_point(y = rep(0.005,length(data[j,])), size = 2) +
  geom_histogram(data = tibble(value = data[j,]),
                 aes(x=value, y = ..density..), 
                 color = 'black', alpha = 0.3,
                 binwidth = 0.5, inherit.aes = F) +
  scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
  scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] )  +
  geom_path( data = as_tibble( t(Pred_one_group) ),
             color = 'black', aes(x=grid,y=`50%`), size = 1.2,
             inherit.aes = F)  +
  labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
  geom_ribbon(data = as_tibble( t(Pred_one_group) ), fill = mycol[1],
              aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
              alpha = 0.5, inherit.aes = F)

```


Riassunto:

* $K$ numero di cluster mi piace molto, perché come valore più basso ha 4 che è il numero di cluster veri, e poi esplora un po' cosa succede per $K$ maggiori
* Questa volta anche la matrice di similarity è praticamente perfetta
* A posteriori ritrovo il clustering esatto
* La stima della densità è come vorrei

### Parto da ognuno fa cluster da se
```{r}
# Run  --------------------------------------------------------------------
niter  <-  2000
burnin <-  2000
thin   <- 1

# tau 
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 100, varsig2 = 100, correction = 10)


# gamma
mu_gamma  = opt$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma

#lambda
mu_lambda  = opt$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)


option = set_options_marginal(
             "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = opt$par[1],"Lambda0" = opt$par[2], 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = seq(1:length(real_partition))#real_partition
        )

# P0 Hyperparameters:
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,

# Process parameters
# old set:  "gamma0" = 1,"Lambda0" = 3, 
#           "alpha_gamma"=5, "beta_gamma"=0.5, "alpha_lambda"=1, "beta_lambda"=5,


#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```

```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```

```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)
```



```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
#Pred_all = predictive_all_groups(grid = grid, fit = GDFMM, burnin = 1)

# Predictive in one group
Pred_one_group = predictive_marginal(idx_group = 1, grid = grid, fit = GDFMM, 
                                     option = option, burnin = 0)

j = 1
idx_start = 1
idx_end = n_j[1]
tibble(value = data[j,],
       true_clus = as.factor(real_partition[idx_start:idx_end])
       ) %>%
  ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
  geom_point(y = rep(0.005,length(data[j,])), size = 2) +
  geom_histogram(data = tibble(value = data[j,]),
                 aes(x=value, y = ..density..), 
                 color = 'black', alpha = 0.3,
                 binwidth = 0.5, inherit.aes = F) +
  scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
  scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] )  +
  geom_path( data = as_tibble( t(Pred_one_group) ),
             color = 'black', aes(x=grid,y=`50%`), size = 1.2,
             inherit.aes = F)  +
  labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
  geom_ribbon(data = as_tibble( t(Pred_one_group) ), fill = mycol[1],
              aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
              alpha = 0.5, inherit.aes = F)

```

Riassunto:

* Mi piace, il comportamento è molto simile al caso degli iperparametri settati di default ma c'è da dire che la matrice di similarity ora è quasi perfetta. Infatti si arriva alla partizione corretta anche usando la Binder loss, cosa che per il caso di default non avveniva, bisognava guardare solo la VI. 
* Anche la convergenza mi piace, ho aumentato il numero di iterazioni ma non era cosi necessario come nel caso di prima


### Parto da tutti in un unico cluster
```{r}
# Run  --------------------------------------------------------------------
niter  <-  2000
burnin <-  2000
thin   <- 1

# tau 
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 100, varsig2 = 100, correction = 10)

# gamma
mu_gamma  = opt$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma

#lambda
mu_lambda  = opt$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)


option = set_options_marginal(
             "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = opt$par[1],"Lambda0" = opt$par[2], 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = NULL#real_partition
        )

# P0 Hyperparameters:
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,

# Process parameters
# old set:  "gamma0" = 1,"Lambda0" = 3, 
#           "alpha_gamma"=5, "beta_gamma"=0.5, "alpha_lambda"=1, "beta_lambda"=5,


#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```

```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```

```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)
```



```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
#Pred_all = predictive_all_groups(grid = grid, fit = GDFMM, burnin = 1)

# Predictive in one group
Pred_one_group = predictive_marginal(idx_group = 1, grid = grid, fit = GDFMM, 
                                     option = option, burnin = 0)

j = 1
idx_start = 1
idx_end = n_j[1]
tibble(value = data[j,],
       true_clus = as.factor(real_partition[idx_start:idx_end])
       ) %>%
  ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
  geom_point(y = rep(0.005,length(data[j,])), size = 2) +
  geom_histogram(data = tibble(value = data[j,]),
                 aes(x=value, y = ..density..), 
                 color = 'black', alpha = 0.3,
                 binwidth = 0.5, inherit.aes = F) +
  scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
  scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] )  +
  geom_path( data = as_tibble( t(Pred_one_group) ),
             color = 'black', aes(x=grid,y=`50%`), size = 1.2,
             inherit.aes = F)  +
  labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
  geom_ribbon(data = as_tibble( t(Pred_one_group) ), fill = mycol[1],
              aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
              alpha = 0.5, inherit.aes = F)

```

Riassunto:

Ancora tutto bene

# CONCLUSIONE
* Settare una $P_0$ più stretta rispetto ai dati ma comunque scelta usando il metodo empirical Bayes sembra essere la scelta più conveniente in termini di mixing, rapidità di convergenza (anche se le catene di $\gamma$ e $\U$ continuano ad essere molto autocorrelate) e come similarity matrix. 
* Le catene di $U$ e $\gamma$ dipendono chiaramente dalla scelta di parametro MALA $s_p$. In questo caso semplice in cui $d=1$ non sembra esserci niente di problematico, tuttavia bisogna vedere se questo crea problemi quando $d>1$
* *NON* riesco proprio a capire come mai la scelta di iperparametri fatta con empirical Bayes che ci porta a delle marginali a priori che abbracciano molto bene i dati non funzioni. Se guardo come vengono calcolate le stime a posteriori è chiaro che non stiamo facendo una buona scelta di iperparametri. Però graficamente mi sembrerebbe davvero un'ottima scelta. Non me lo riesco a spiegare.


# Cluster più vicini


*Dati esperimento:*

* **K = 3**
* d = 1
* $n_1 = 200$
* $\mu_1 = -2$, $\mu_2 = 0$, $\mu_3 = 1$
* $\sigma_1 = \sigma_2 = \sigma_3 = 1$
* $\pi_{11} = 1/4, \pi_{12} = 1/4, \pi_{13} = 1/4$

## Data generation

```{r}
# color palette -----------------------------------------------------------
mycol = hcl.colors(n=3,palette = "Zissou1")

# data generation ---------------------------------------------------------

d = 1                   # number of groups
K = 3                   # number of global clusters
mu = c(-5,0,2)   # vectors of means
sd = c(2,1,0.5)         # vector of sd
n_j = c(200)#rep(200, d)        # set cardinality of the groups
mix_probs = matrix( c(1/3,1/3,1/3), nrow = d, ncol = K, byrow = T)
seed = 20051131

genD = simulate_data(d = d, K = K, p = mix_probs, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition

mycol_cluster = brewer.pal(n=K, name = "Dark2")

```

```{r}
counter = 1
data_per_cluster = vector("list", length = K)
log_marginal_prior = vector("list", length = d)
for(j in 1:d){
  log_marginal_prior[[j]] = vector( length = n_j[j])
  for(i in 1:n_j[j]){
    c_ji = real_partition[counter]
    data_per_cluster[[c_ji]] = c(data_per_cluster[[c_ji]], data[j,i])
    log_marginal_prior[[j]][i] = log(ACutils::dnct(data[j,i], n0 = 10, mu0 = 1, gamma0 = sqrt(2)) )
    counter = counter + 1
  }
  
}
#data_per_cluster 
mean_data_per_cluster = lapply(data_per_cluster, mean)
mean_data_per_cluster
var_data_per_cluster  = lapply(data_per_cluster, var)
lapply(var_data_per_cluster, sqrt)
#log_marginal_prior
```


```{r}
idx_start = 1
idx_end = n_j[1]

par(mfrow = c(1,d))
for(j in 1:d){
  mycol_cluster = brewer.pal(n=K, name = "Dark2")
  mycol_points  = as.factor(real_partition[idx_start:idx_end])
  levels(mycol_points) = unique(mycol_cluster)
  
  dj = density(data[j,1:n_j[j]])
  
  hist(data[j,1:n_j[j]], freq = F, breaks = n_j[j]/5,
       main = paste0("Level = ",j))
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points)
  lines(dj$x,dj$y, lwd = 2)
  
  idx_start = idx_end + 1
  if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}
```
## Partizione Fissata

Inizio fissando la partizione, voglio capire se stimo bene la densità o no

### Iperparam EmpBayes - 1

```{r}
# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 1, varsig2 = 1)
mu0 = P0_hyparam$mu0;k0= P0_hyparam$k0;nu0=P0_hyparam$nu0;sigma0=P0_hyparam$sigma0

l_grid = 200
grid = seq(-15,5,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)


# Start plot
idx_start = 1
idx_end = n_j[1]

par(mfrow = c(1,d))
for(j in 1:d){
  mycol_cluster = brewer.pal(n=K, name = "Dark2")
  mycol_points  = as.factor(real_partition[idx_start:idx_end])
  levels(mycol_points) = unique(mycol_cluster)
  
  # plot histogram and data points
  par(mar = c(2,2,1,1))
  hist(data[j,1:n_j[j]], freq = F, breaks = n_j[j]/5,
       main = paste0("Level = ",j), ylim = c(0,0.3))
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points)
  # plot marginal 
  lines(x = grid, y = Prior_grid, type = "l", lwd = 2)
  
  idx_start = idx_end + 1
  if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}

```


```{r}
# Run  --------------------------------------------------------------------
niter  <- 1000
burnin <- 1000
thin   <- 1

P0_hyparam = empirical_bayes_normalinvgamma(data = data)

option = set_options_marginal(
             "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = 1,"Lambda0" = 3, 
             "alpha_gamma"=5, "beta_gamma"=0.5, 
             "alpha_lambda"=1, "beta_lambda"=5,
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = real_partition
        )
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = T, option = option)

```

Analisi parte parametrica con partizione fissa
```{r}
mu_est = rep(0,K)
s2_est = rep(1,K)

for(it in 1:niter){
  mu_est = mu_est + GDFMM$mu[[it]]
  s2_est = s2_est + GDFMM$sigma[[it]]
}
mu_est = mu_est/niter
s2_est = s2_est/niter

mu_est
s2_est
```

```{r}
l_grid = 200
grid = seq(-12,5,length.out = l_grid)

# Predictive in all groups
Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)

# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]
par(mfrow = c(1,d))
for(j in 1:d){
  
  # set colors
  mycol_cluster = brewer.pal(n=K, name = "Dark2")
  mycol_points  = as.factor(real_partition[idx_start:idx_end])
  levels(mycol_points) = unique(mycol_cluster)
  
  # frequentist density
  dj = density(data[j,1:n_j[j]])
  
  # set axis limits
  yrange = range(Pred_all[[j]][3,]) + 0.1*max(Pred_all[[j]][3,])
  xrange = range( c(data[j,1:n_j[j]], grid) )
  
  # Open new empty plot so that I can add the grid below
  # the displayed figures
  par( mar = c(2,2,1,1), bty="l" )
  plot(0,0, type = "n",
       ylim = c(0,0.3), xlim = xrange,
       main = paste0("Level = ",j), cex.main = 0.9, 
       ylab = "y-axis", xlab = "x-axis")
  # add grid
  grid(lty = 1,lwd = 2,col = "gray90" )
  # add histogram
  hist(data[j,1:n_j[j]], 
       freq = F, 
       breaks = n_j[j]/6, 
       add = T )
  # add observed points
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points)
  # add frequentist density
  lines(dj$x,dj$y, lwd = 3)
  # add estimate density with credible band for estimated density
  polygon( c(grid, rev(grid)), 
           c(Pred_all[[j]][1,], rev(Pred_all[[j]][3,])),
           col = ACutils::t_col(mycol[1], percent = 45),
           border = ACutils::t_col(mycol[1], percent = 45) ) 
  lines(x = grid, y = Pred_all[[j]][2,], col = mycol[1], lwd = 3) #0.5
  legend("topleft", lty = 1, lwd = 2,
         legend = c("empricial", "bayesian"),
         col = c("black", mycol[1]))

  idx_start = idx_end + 1
    if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}

```

### Iperparam EmpBayes - 2

```{r}
# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 10, varsig2 = 10, correction = 5)
mu0 = P0_hyparam$mu0;k0= P0_hyparam$k0;nu0=P0_hyparam$nu0;sigma0=P0_hyparam$sigma0


l_grid = 200
grid = seq(-15,5,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)


# Start plot
idx_start = 1
idx_end = n_j[1]

par(mfrow = c(1,d))
for(j in 1:d){
  mycol_cluster = brewer.pal(n=K, name = "Dark2")
  mycol_points  = as.factor(real_partition[idx_start:idx_end])
  levels(mycol_points) = unique(mycol_cluster)
  
  # plot histogram and data points
  par(mar = c(2,2,1,1))
  hist(data[j,1:n_j[j]], freq = F, breaks = n_j[j]/5,
       main = paste0("Level = ",j), ylim = c(0,0.53))
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points)
  # plot marginal 
  lines(x = grid, y = Prior_grid, type = "l", lwd = 2)
  
  idx_start = idx_end + 1
  if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}

```


```{r}
# Run  --------------------------------------------------------------------
niter  <- 1000
burnin <- 1000
thin   <- 1

P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 10, varsig2 = 10, correction = 5)
mu0 = P0_hyparam$mu0;k0= P0_hyparam$k0;nu0=P0_hyparam$nu0;sigma0=P0_hyparam$sigma0

option = set_options_marginal(
             "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = 1,"Lambda0" = 3, 
             "alpha_gamma"=5, "beta_gamma"=0.5, 
             "alpha_lambda"=1, "beta_lambda"=5,
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = real_partition
        )
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = T, option = option)

```

Analisi parte parametrica con partizione fissa
```{r}
mu_est = rep(0,K)
s2_est = rep(1,K)

for(it in 1:niter){
  mu_est = mu_est + GDFMM$mu[[it]]
  s2_est = s2_est + GDFMM$sigma[[it]]
}
mu_est = mu_est/niter
s2_est = s2_est/niter

mu_est
sqrt(s2_est)
```

```{r}
l_grid = 200
grid = seq(-12,5,length.out = l_grid)

# Predictive in all groups
Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)

# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]
par(mfrow = c(1,d))
for(j in 1:d){
  
  # set colors
  mycol_cluster = brewer.pal(n=K, name = "Dark2")
  mycol_points  = as.factor(real_partition[idx_start:idx_end])
  levels(mycol_points) = unique(mycol_cluster)
  
  # frequentist density
  dj = density(data[j,1:n_j[j]])
  
  # set axis limits
  yrange = range(Pred_all[[j]][3,]) + 0.1*max(Pred_all[[j]][3,])
  xrange = range( c(data[j,1:n_j[j]], grid) )
  
  # Open new empty plot so that I can add the grid below
  # the displayed figures
  par( mar = c(2,2,1,1), bty="l" )
  plot(0,0, type = "n",
       ylim = c(0,0.3), xlim = xrange,
       main = paste0("Level = ",j), cex.main = 0.9, 
       ylab = "y-axis", xlab = "x-axis")
  # add grid
  grid(lty = 1,lwd = 2,col = "gray90" )
  # add histogram
  hist(data[j,1:n_j[j]], 
       freq = F, 
       breaks = n_j[j]/6, 
       add = T )
  # add observed points
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points)
  # add frequentist density
  lines(dj$x,dj$y, lwd = 3)
  # add estimate density with credible band for estimated density
  polygon( c(grid, rev(grid)), 
           c(Pred_all[[j]][1,], rev(Pred_all[[j]][3,])),
           col = ACutils::t_col(mycol[1], percent = 45),
           border = ACutils::t_col(mycol[1], percent = 45) ) 
  lines(x = grid, y = Pred_all[[j]][2,], col = mycol[1], lwd = 3) #0.5
  
  legend("topleft", lty = 1, lwd = 2,
         legend = c("empricial", "bayesian"),
         col = c("black", mycol[1]))

  idx_start = idx_end + 1
    if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}

```





### Iperparam Default

```{r}
# Scelta di default
mu0 = 0;k0= 1/10;nu0=10;sigma0=1

l_grid = 200
grid = seq(-15,5,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)


# Start plot
idx_start = 1
idx_end = n_j[1]

par(mfrow = c(1,d))
for(j in 1:d){
  mycol_cluster = brewer.pal(n=K, name = "Dark2")
  mycol_points  = as.factor(real_partition[idx_start:idx_end])
  levels(mycol_points) = unique(mycol_cluster)
  
  # plot histogram and data points
  par(mar = c(2,2,1,1))
  hist(data[j,1:n_j[j]], freq = F, breaks = n_j[j]/5,
       main = paste0("Level = ",j), ylim = c(0,0.7))
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points)
  # plot marginal 
  lines(x = grid, y = Prior_grid, type = "l", lwd = 2)
  
  idx_start = idx_end + 1
  if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}

```


```{r}
# Run  --------------------------------------------------------------------
niter  <- 1000
burnin <- 1000
thin   <- 1

option = set_options_marginal(
             "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = 1,"Lambda0" = 3, 
             "alpha_gamma"=5, "beta_gamma"=0.5, 
             "alpha_lambda"=1, "beta_lambda"=5,
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = real_partition
        )
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = T, option = option)

```

Analisi parte parametrica con partizione fissa
```{r}
mu_est = rep(0,K)
s2_est = rep(1,K)

for(it in 1:niter){
  mu_est = mu_est + GDFMM$mu[[it]]
  s2_est = s2_est + GDFMM$sigma[[it]]
}
mu_est = mu_est/niter
s2_est = s2_est/niter

mu_est
sqrt(s2_est)
```

```{r}
l_grid = 200
grid = seq(-12,5,length.out = l_grid)

# Predictive in all groups
Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)

# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]
par(mfrow = c(1,d))
for(j in 1:d){
  
  # set colors
  mycol_cluster = brewer.pal(n=K, name = "Dark2")
  mycol_points  = as.factor(real_partition[idx_start:idx_end])
  levels(mycol_points) = unique(mycol_cluster)
  
  # frequentist density
  dj = density(data[j,1:n_j[j]])
  
  # set axis limits
  yrange = range(Pred_all[[j]][3,]) + 0.1*max(Pred_all[[j]][3,])
  xrange = range( c(data[j,1:n_j[j]], grid) )
  
  # Open new empty plot so that I can add the grid below
  # the displayed figures
  par( mar = c(2,2,1,1), bty="l" )
  plot(0,0, type = "n",
       ylim = c(0,0.3), xlim = xrange,
       main = paste0("Level = ",j), cex.main = 0.9, 
       ylab = "y-axis", xlab = "x-axis")
  # add grid
  grid(lty = 1,lwd = 2,col = "gray90" )
  # add histogram
  hist(data[j,1:n_j[j]], 
       freq = F, 
       breaks = n_j[j]/6, 
       add = T )
  # add observed points
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points)
  # add frequentist density
  lines(dj$x,dj$y, lwd = 3)
  # add estimate density with credible band for estimated density
  polygon( c(grid, rev(grid)), 
           c(Pred_all[[j]][1,], rev(Pred_all[[j]][3,])),
           col = ACutils::t_col(mycol[1], percent = 45),
           border = ACutils::t_col(mycol[1], percent = 45) ) 
  lines(x = grid, y = Pred_all[[j]][2,], col = mycol[1], lwd = 3) #0.5
  
  legend("topleft", lty = 1, lwd = 2,
         legend = c("empricial", "bayesian"),
         col = c("black", mycol[1]))

  idx_start = idx_end + 1
    if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}

```







## Partizione Aleatoria

Ora invece la partizione torno a metterla aleatoria. Elicito i parametri per $\gamma$ e $\Lambda$

```{r}
# fK è la funzione da ottimizzare: now it takes 3 parameters, gamma_1, gamma_2 and Lambda
fK <- function(x){
  -p_distinct_prior(k = K, n_j = n_j, 
                    gamma = c(x[1]), 
                    prior = "Poisson", lambda = x[2] )
}



optK = optim(par = rep(0.1,2), #initial points
             method = "L-BFGS-B", # that's the only method that takes bounds
             fn  = fK, #function to be minimized
             lower = rep(0.001,2) #lower bound for variables
)
optK
```

```{r}
# gamma
mu_gamma  = optK$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma
x_range_gamma = c(  max(0, mu_gamma-2*sqrt(var_gamma) ),
                    mu_gamma+2*sqrt(var_gamma) )
#lambda
mu_lambda  = optK$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)
x_range_lambda = c(  max(0, mu_lambda-2*sqrt(var_lambda) ),
                     mu_lambda+2*sqrt(var_lambda) )


par(mfrow = c(1,2))
plot_density(shape = a_gamma, rate = b_gamma,
             dist = "gamma", main = "gamma prior", use_x11_device = F,
             xlim = x_range_gamma)
plot_density(shape = a_lambda, rate = b_lambda,
             dist = "gamma", main = "lambda prior", use_x11_device = F,
             xlim = x_range_lambda)
```



Con i parametri scelti, plotto un'approssimazione al $99.5\%$ della massa totale della distribuzione del numero di cluster
```{r}
prob_k = rep(0,sum(n_j))
threshold = 0.995
for(kk in 1:sum(n_j) ){
  prob_k[kk] = p_distinct_prior(k = kk, n_j = n_j, 
                                gamma = rep(optK$par[1],d),
                                prior = "Poisson", lambda = optK$par[2])
  if(sum(prob_k)>=threshold)
    break
}

plot(which(prob_k>0), prob_k[prob_k>0], 
     type = 'b', pch = 16, lty = 1, lwd = 3,
     main = "Prior number of clusters", xlab = "K", ylab = "prob")
```

### Iperparam EmpBayes - 1

```{r}
# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 1, varsig2 = 1)
mu0 = P0_hyparam$mu0;k0= P0_hyparam$k0;nu0=P0_hyparam$nu0;sigma0=P0_hyparam$sigma0

l_grid = 200
grid = seq(-15,5,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)


# Start plot
idx_start = 1
idx_end = n_j[1]

par(mfrow = c(1,d))
for(j in 1:d){
  mycol_cluster = brewer.pal(n=K, name = "Dark2")
  mycol_points  = as.factor(real_partition[idx_start:idx_end])
  levels(mycol_points) = unique(mycol_cluster)
  
  # plot histogram and data points
  par(mar = c(2,2,1,1))
  hist(data[j,1:n_j[j]], freq = F, breaks = n_j[j]/5,
       main = paste0("Level = ",j), ylim = c(0,0.3))
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points)
  # plot marginal 
  lines(x = grid, y = Prior_grid, type = "l", lwd = 2)
  
  idx_start = idx_end + 1
  if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}

```

#### Run

```{r}
# Run  --------------------------------------------------------------------
niter  <- 1000
burnin <- 1000
thin   <- 1

#P0_hyparam = empirical_bayes_normalinvgamma(data = data)

a_gamma <- 1; b_gamma <- 1
a_lambda <- 10; b_lambda <- 2

option = set_options_marginal(
             "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = optK$par[1],"Lambda0" = optK$par[2], 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = seq(1,sum(n_j))
        )
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)

```

```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```


```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)

arandi(binder_sara$cl,real_partition, adjust = F)
arandi(VI_sara$cl,real_partition, adjust = F)


#mean( apply(part_matrix, 1, FUN = function(part_it){arandi(part_it,real_partition, adjust = T)}) )
```


```{r}
l_grid = 200
grid = seq(-12,5,length.out = l_grid)

# Predictive in all groups
Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)

# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]
par(mfrow = c(1,d))
for(j in 1:d){
  
  # set colors
  mycol_cluster = brewer.pal(n=K, name = "Dark2")
  mycol_points  = as.factor(real_partition[idx_start:idx_end])
  levels(mycol_points) = unique(mycol_cluster)
  
  # frequentist density
  dj = density(data[j,1:n_j[j]])
  
  # set axis limits
  yrange = range(Pred_all[[j]][3,]) + 0.1*max(Pred_all[[j]][3,])
  xrange = range( c(data[j,1:n_j[j]], grid) )
  
  # Open new empty plot so that I can add the grid below
  # the displayed figures
  par( mar = c(2,2,1,1), bty="l" )
  plot(0,0, type = "n",
       ylim = c(0,0.3), xlim = xrange,
       main = paste0("Level = ",j), cex.main = 0.9, 
       ylab = "y-axis", xlab = "x-axis")
  # add grid
  grid(lty = 1,lwd = 2,col = "gray90" )
  # add histogram
  hist(data[j,1:n_j[j]], 
       freq = F, 
       breaks = n_j[j]/6, 
       add = T )
  # add observed points
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points)
  # add frequentist density
  lines(dj$x,dj$y, lwd = 3)
  # add estimate density with credible band for estimated density
  polygon( c(grid, rev(grid)), 
           c(Pred_all[[j]][1,], rev(Pred_all[[j]][3,])),
           col = ACutils::t_col(mycol[1], percent = 45),
           border = ACutils::t_col(mycol[1], percent = 45) ) 
  lines(x = grid, y = Pred_all[[j]][2,], col = mycol[1], lwd = 3) #0.5
  legend("topleft", lty = 1, lwd = 2,
         legend = c("empricial", "bayesian"),
         col = c("black", mycol[1]))

  idx_start = idx_end + 1
    if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}

```

### Iperparam EmpBayes - 2

```{r}
# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 10, varsig2 = 10, correction = 5)
mu0 = P0_hyparam$mu0;k0= P0_hyparam$k0;nu0=P0_hyparam$nu0;sigma0=P0_hyparam$sigma0


l_grid = 200
grid = seq(-15,5,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)


# Start plot
idx_start = 1
idx_end = n_j[1]

par(mfrow = c(1,d))
for(j in 1:d){
  mycol_cluster = brewer.pal(n=K, name = "Dark2")
  mycol_points  = as.factor(real_partition[idx_start:idx_end])
  levels(mycol_points) = unique(mycol_cluster)
  
  # plot histogram and data points
  par(mar = c(2,2,1,1))
  hist(data[j,1:n_j[j]], freq = F, breaks = n_j[j]/5,
       main = paste0("Level = ",j), ylim = c(0,0.53))
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points)
  # plot marginal 
  lines(x = grid, y = Prior_grid, type = "l", lwd = 2)
  
  idx_start = idx_end + 1
  if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}

```

#### Run

```{r}
# Run  --------------------------------------------------------------------
niter  <- 1000
burnin <- 1000
thin   <- 1

#P0_hyparam = empirical_bayes_normalinvgamma(data = data)

a_gamma <- 1; b_gamma <- 1
a_lambda <- 10; b_lambda <- 2

option = set_options_marginal(
             "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = optK$par[1],"Lambda0" = optK$par[2], 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = seq(1,sum(n_j))
        )
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)

```

```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```


```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)

arandi(binder_sara$cl,real_partition, adjust = F)
arandi(VI_sara$cl,real_partition, adjust = F)


#mean( apply(part_matrix, 1, FUN = function(part_it){arandi(part_it,real_partition, adjust = T)}) )
```


```{r}
l_grid = 200
grid = seq(-12,5,length.out = l_grid)

# Predictive in all groups
Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)

# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]
par(mfrow = c(1,d))
for(j in 1:d){
  
  # set colors
  mycol_cluster = brewer.pal(n=K, name = "Dark2")
  mycol_points  = as.factor(real_partition[idx_start:idx_end])
  levels(mycol_points) = unique(mycol_cluster)
  
  # frequentist density
  dj = density(data[j,1:n_j[j]])
  
  # set axis limits
  yrange = range(Pred_all[[j]][3,]) + 0.1*max(Pred_all[[j]][3,])
  xrange = range( c(data[j,1:n_j[j]], grid) )
  
  # Open new empty plot so that I can add the grid below
  # the displayed figures
  par( mar = c(2,2,1,1), bty="l" )
  plot(0,0, type = "n",
       ylim = c(0,0.3), xlim = xrange,
       main = paste0("Level = ",j), cex.main = 0.9, 
       ylab = "y-axis", xlab = "x-axis")
  # add grid
  grid(lty = 1,lwd = 2,col = "gray90" )
  # add histogram
  hist(data[j,1:n_j[j]], 
       freq = F, 
       breaks = n_j[j]/6, 
       add = T )
  # add observed points
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points)
  # add frequentist density
  lines(dj$x,dj$y, lwd = 3)
  # add estimate density with credible band for estimated density
  polygon( c(grid, rev(grid)), 
           c(Pred_all[[j]][1,], rev(Pred_all[[j]][3,])),
           col = ACutils::t_col(mycol[1], percent = 45),
           border = ACutils::t_col(mycol[1], percent = 45) ) 
  lines(x = grid, y = Pred_all[[j]][2,], col = mycol[1], lwd = 3) #0.5
  
  legend("topleft", lty = 1, lwd = 2,
         legend = c("empricial", "bayesian"),
         col = c("black", mycol[1]))

  idx_start = idx_end + 1
    if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}

```





### Iperparam Default

```{r}
# Scelta di default
mu0 = 0;k0= 1/10;nu0=10;sigma0=1

l_grid = 200
grid = seq(-15,5,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)


# Start plot
idx_start = 1
idx_end = n_j[1]

par(mfrow = c(1,d))
for(j in 1:d){
  mycol_cluster = brewer.pal(n=K, name = "Dark2")
  mycol_points  = as.factor(real_partition[idx_start:idx_end])
  levels(mycol_points) = unique(mycol_cluster)
  
  # plot histogram and data points
  par(mar = c(2,2,1,1))
  hist(data[j,1:n_j[j]], freq = F, breaks = n_j[j]/5,
       main = paste0("Level = ",j), ylim = c(0,0.7))
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points)
  # plot marginal 
  lines(x = grid, y = Prior_grid, type = "l", lwd = 2)
  
  idx_start = idx_end + 1
  if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}

```

#### Run

```{r}
# Run  --------------------------------------------------------------------
niter  <- 1000
burnin <- 1000
thin   <- 1

#P0_hyparam = empirical_bayes_normalinvgamma(data = data)

a_gamma <- 1; b_gamma <- 1
a_lambda <- 10; b_lambda <- 2

option = set_options_marginal(
             "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = optK$par[1],"Lambda0" = optK$par[2], 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = seq(1,sum(n_j))
        )
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)

```

```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```


```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)

arandi(binder_sara$cl,real_partition, adjust = F)
arandi(VI_sara$cl,real_partition, adjust = F)


#mean( apply(part_matrix, 1, FUN = function(part_it){arandi(part_it,real_partition, adjust = T)}) )
```


```{r}
l_grid = 200
grid = seq(-12,5,length.out = l_grid)

# Predictive in all groups
Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)

# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]
par(mfrow = c(1,d))
for(j in 1:d){
  
  # set colors
  mycol_cluster = brewer.pal(n=K, name = "Dark2")
  mycol_points  = as.factor(real_partition[idx_start:idx_end])
  levels(mycol_points) = unique(mycol_cluster)
  
  # frequentist density
  dj = density(data[j,1:n_j[j]])
  
  # set axis limits
  yrange = range(Pred_all[[j]][3,]) + 0.1*max(Pred_all[[j]][3,])
  xrange = range( c(data[j,1:n_j[j]], grid) )
  
  # Open new empty plot so that I can add the grid below
  # the displayed figures
  par( mar = c(2,2,1,1), bty="l" )
  plot(0,0, type = "n",
       ylim = c(0,0.3), xlim = xrange,
       main = paste0("Level = ",j), cex.main = 0.9, 
       ylab = "y-axis", xlab = "x-axis")
  # add grid
  grid(lty = 1,lwd = 2,col = "gray90" )
  # add histogram
  hist(data[j,1:n_j[j]], 
       freq = F, 
       breaks = n_j[j]/6, 
       add = T )
  # add observed points
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points)
  # add frequentist density
  lines(dj$x,dj$y, lwd = 3)
  # add estimate density with credible band for estimated density
  polygon( c(grid, rev(grid)), 
           c(Pred_all[[j]][1,], rev(Pred_all[[j]][3,])),
           col = ACutils::t_col(mycol[1], percent = 45),
           border = ACutils::t_col(mycol[1], percent = 45) ) 
  lines(x = grid, y = Pred_all[[j]][2,], col = mycol[1], lwd = 3) #0.5
  
  legend("topleft", lty = 1, lwd = 2,
         legend = c("empricial", "bayesian"),
         col = c("black", mycol[1]))

  idx_start = idx_end + 1
    if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}

```



# Galaxy data

```{r}
mycol = hcl.colors(n=3,palette = "Zissou1")
load("C:\\Users\\colom\\Downloads\\galaxy.rda")
n = length(galaxy)
data = t(as.matrix(galaxy))
n_j = c(n)
d = 1

```

Visualization
```{r}
idx_start = 1
idx_end = n_j[1]

par(mfrow = c(1,d))
for(j in 1:d){
  mycol_points  = "black"
  
  dj = density(data[j,1:n_j[j]])
  
  par(mar = c(2,2,1,1), bty = "l")
  hist(data[j,1:n_j[j]], freq = F, breaks = n_j[j]/4,
       main = paste0("Level = ",j), ylim = c(0,0.22), nclass = "fd")
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points)
  lines(dj$x,dj$y, lwd = 2)
  
  idx_start = idx_end + 1
  if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}

```

## Hyperparameter setup:

### P0

Nel paper vengono riportati questi valori
```{r}
# Nel paper
mu0    = mean(data)
k0     = 0.01
nu0    = 4
sigma0 = 0.5
```

Però nel codice vedo che $\kappa_0$ e $\sigma_0^2$ sono diversi
```{r}
# Nel codice
# k0=1/3
# nu0=4.222222
# sigma0=3.661027
```

Da notare come entrambi sono molto diversi dalla scelta che verrebbe a noi
```{r}
P0_hyparam = empirical_bayes_normalinvgamma(data = data)
P0_hyparam
```


```{r}
l_grid = 500
grid = seq(8,35,length.out = l_grid)
scale = sqrt( (k0 + 1)/(k0) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)


# Start plot
idx_start = 1
idx_end = n_j[1]

par(mfrow = c(1,d))
for(j in 1:d){
  mycol_points = "black"
  
  # plot histogram and data points
  par(mar = c(2,2,1,1))
  hist(data[j,1:n_j[j]], freq = F, breaks = n_j[j]/4,
       main = paste0("Level = ",j), ylim = c(0,0.5))
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points)
  # plot marginal 
  lines(x = grid, y = Prior_grid, type = "l", lwd = 2, col = "darkred")
  
  # plot empirical density
  lines(dj$x,dj$y, lwd = 2)
  
  legend("topright", c("marginal","empirical density"), col = c("darkred","black"),
         lwd = 2, lty = 1)
  
  idx_start = idx_end + 1
  if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}

```

### gamma, Lambda

Elicito i parametri per $\gamma$ e $\Lambda$. Nel paper dicono che vengono tenuti fissi in modo da fissare $\text{E}[K]$ a un valore prestabilito. In particolare considerano tre casi, $\text{E}[K] = 1$, $\text{E}[K] = 5$, $\text{E}[K] = 10$

```{r}
# I want to minimaze |f(gamma,lambda) - Kstar|
Kstar = 5
fExpVal <- function(x){
  abs( Expected_prior(n_j = n_j, 
                      gamma = x[1], 
                      type = "distinct", prior = "Poisson", 
                      lambda = x[2])$Mean 
       - Kstar
  )
}

optK = optim(par = rep(1,2), #initial points
             method = "L-BFGS-B", # that's the only method that takes bounds
             fn  = fExpVal, #function to be minimized
             lower = rep(0.001,2) #lower bound for variables
)
optK


Expected_prior(n_j = n_j, gamma = optK$par[1], 
                      type = "distinct", prior = "Poisson", 
                      lambda = optK$par[2])
```


```{r}
# gamma
mu_gamma  = optK$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma
x_range_gamma = c(  max(0, mu_gamma-2*sqrt(var_gamma) ),
                    mu_gamma+2*sqrt(var_gamma) )
#lambda
mu_lambda  = optK$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)
x_range_lambda = c(  max(0, mu_lambda-2*sqrt(var_lambda) ),
                     mu_lambda+2*sqrt(var_lambda) )


par(mfrow = c(1,2))
plot_density(shape = a_gamma, rate = b_gamma,
             dist = "gamma", main = "gamma prior", use_x11_device = F,
             xlim = x_range_gamma)
plot_density(shape = a_lambda, rate = b_lambda,
             dist = "gamma", main = "lambda prior", use_x11_device = F,
             xlim = x_range_lambda)
```



Con i parametri scelti, plotto un'approssimazione al $99.5\%$ della massa totale della distribuzione del numero di cluster
```{r}
prob_k = rep(0,sum(n_j))
threshold = 0.995
for(kk in 1:sum(n_j) ){
  prob_k[kk] = p_distinct_prior(k = kk, n_j = n_j, 
                                gamma = rep(optK$par[1],d),
                                prior = "Poisson", lambda = optK$par[2])
  if(sum(prob_k)>=threshold)
    break
}

plot(which(prob_k>0), prob_k[prob_k>0], 
     type = 'b', pch = 16, lty = 1, lwd = 3,
     main = "Prior number of clusters", xlab = "K", ylab = "prob")
```

## Run

```{r}
# Run  --------------------------------------------------------------------
niter  <- 5000
burnin <- 5000
thin   <- 10

#P0_hyparam = empirical_bayes_normalinvgamma(data = data)

# a_gamma <- 1; b_gamma <- 1
# a_lambda <- 10; b_lambda <- 2

option = set_options_marginal(
             "mu0" = mu0, "k0"= k0, "nu0"=nu0, "sigma0"= sigma0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = optK$par[1],"Lambda0" = optK$par[2], 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = seq(1,sum(n_j))
        )
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)

```

```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```


```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)

#mean( apply(part_matrix, 1, FUN = function(part_it){arandi(part_it,real_partition, adjust = T)}) )
```


```{r}
l_grid = 200
grid = seq(8,35,length.out = l_grid)

# Predictive in all groups
Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)

# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]
par(mfrow = c(1,d))
for(j in 1:d){
  
  # set colors
  mycol_points  = "black"
  
  # frequentist density
  dj = density(data[j,1:n_j[j]])
  
  # set axis limits
  yrange = range(Pred_all[[j]][3,]) + 0.1*max(Pred_all[[j]][3,])
  xrange = range( c(data[j,1:n_j[j]], grid) )
  
  # Open new empty plot so that I can add the grid below
  # the displayed figures
  par( mar = c(2,2,1,1), bty="l" )
  plot(0,0, type = "n",
       ylim = c(0,0.3), xlim = xrange,
       main = paste0("Level = ",j), cex.main = 0.9, 
       ylab = "y-axis", xlab = "x-axis")
  # add grid
  grid(lty = 1,lwd = 2,col = "gray90" )
  # add histogram
  hist(data[j,1:n_j[j]], 
       freq = F, 
       breaks = n_j[j]/4, 
       add = T )
  # add observed points
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points)
  # add frequentist density
  lines(dj$x,dj$y, lwd = 3)
  # add estimate density with credible band for estimated density
  polygon( c(grid, rev(grid)), 
           c(Pred_all[[j]][1,], rev(Pred_all[[j]][3,])),
           col = ACutils::t_col(mycol[1], percent = 45),
           border = ACutils::t_col(mycol[1], percent = 45) ) 
  lines(x = grid, y = Pred_all[[j]][2,], col = mycol[1], lwd = 3) #0.5
  legend("topleft", lty = 1, lwd = 2,
         legend = c("empricial", "bayesian"),
         col = c("black", mycol[1]))

  idx_start = idx_end + 1
    if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}

```

## Risultati

Usando gli iperparametri del paper:

* Con $\text{E}[K] = 5$ e tutto fisso viene male, nel centro non riesco a riconoscere due cluster centrali
* Con $\text{E}[K] = 5$ e solo $\Lambda$ aleatorio viene male, nel centro non riesco a riconoscere due cluster
* Con $\text{E}[K] = 5$ e sia $\gamma$ che $\Lambda$ aleatorii viene male, nel centro non riesco a riconoscere due cluster
* Con $\text{E}[K] = 10$ e tutto fisso la densità viene già meglio, ho una gobbetta.Il clustering però mi sembra brutto
* Con $\text{E}[K] = 10$ e $\Lambda$ aleatorio mi sembra molto carino, sia densità che clustering
* Con $\text{E}[K] = 10$ e sia $\gamma$ che $\Lambda$ aleatori viene ancora carino, la catena di $\gamma$ è molto cicciona
* Con $\text{E}[K] = 1$ non provo nemmeno


Usando gli iperparametri del codice (che poi, non importa se siano stati usati o no nel codice, il punto è sempre confrontare tra qualcosa di largo e di stretto)

* Con $\text{E}[K] = 10$ e tutto fisso viene male, nel centro non riesco a riconoscere due cluster centrali
* Con $\text{E}[K] = 10$ e $\Lambda$ aleatorio viene sempre male
* Con $\text{E}[K] = 10$ e sia $\gamma$ che $\Lambda$ aleatorio viene sempre male
* Idem con $\text{E}[K] = 5$ 




# Annals - simulation study

## Data generation
```{r}
mycol = hcl.colors(n=3,palette = "Zissou1")


d = 1                   # number of groups
K = 7                   # number of global clusters
mu = c(-8,-5,0,5,7,11,15)   # vectors of means
sd = sqrt(c(0.8,0.5,2,0.25,0.05,1,1.25))         # vector of sd
n_j = c(200)#rep(200, d)        # set cardinality of the groups
mix_probs = matrix( c(0.1,0.1,0.15,0.125,0.125,0.2,0.2), nrow = d, ncol = K, byrow = T)
seed = 20051131

genD = simulate_data(d = d, K = K, p = mix_probs, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition

mycol_cluster = brewer.pal(n=K, name = "Dark2")
```



```{r}
idx_start = 1
idx_end = n_j[1]

par(mfrow = c(1,d), mar = c(2,2,1,1), bty = "l")
for(j in 1:d){
  mycol_cluster = brewer.pal(n=K, name = "Dark2")
  mycol_points  = as.factor(real_partition[idx_start:idx_end])
  levels(mycol_points) = unique(mycol_cluster)
  
  dj = density(data[j,1:n_j[j]])
  
  hist(data[j,1:n_j[j]], freq = F, breaks = n_j[j]/5,
       main = paste0("Level = ",j))
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points)
  lines(dj$x,dj$y, lwd = 2)
  
  idx_start = idx_end + 1
  if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}
```

## Hyperparameter setup:

### P0

Nel paper vengono riportati questi valori
```{r}
# Nel paper
mu0    = mean(data)
k0     = 1/( max(data) - min(data)  )^2
nu0    = 2
sigma0 = 0.5
```


```{r}
l_grid = 500
grid = seq(-15,20,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)


# Start plot
idx_start = 1
idx_end = n_j[1]

par(mfrow = c(1,d))
for(j in 1:d){
  mycol_cluster = brewer.pal(n=K, name = "Dark2")
  mycol_points  = as.factor(real_partition[idx_start:idx_end])
  levels(mycol_points) = unique(mycol_cluster)
  
  # plot histogram and data points
  par(mar = c(2,2,1,1))
  hist(data[j,1:n_j[j]], freq = F, breaks = n_j[j]/5,
       main = paste0("Level = ",j), ylim = c(0,0.3))
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points)
  # plot marginal 
  lines(x = grid, y = Prior_grid, type = "l", lwd = 2)
  
  idx_start = idx_end + 1
  if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}

```

### gamma, Lambda

```{r}
# I want to minimaze |f(gamma,lambda) - Kstar|
Kstar = 10
fExpVal <- function(x){
  abs( Expected_prior(n_j = n_j, 
                      gamma = x[1], 
                      type = "distinct", prior = "Poisson", 
                      lambda = x[2])$Mean 
       - Kstar
  )
}

optK = optim(par = rep(1,2), #initial points
             method = "L-BFGS-B", # that's the only method that takes bounds
             fn  = fExpVal, #function to be minimized
             lower = rep(0.001,2) #lower bound for variables
)
optK


Expected_prior(n_j = n_j, gamma = optK$par[1], 
                      type = "distinct", prior = "Poisson", 
                      lambda = optK$par[2])
```


```{r}
# gamma
mu_gamma  = 4#optK$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma
x_range_gamma = c(  max(0, mu_gamma-2*sqrt(var_gamma) ),
                    mu_gamma+2*sqrt(var_gamma) )
#lambda
mu_lambda  = 5#optK$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)
x_range_lambda = c(  max(0, mu_lambda-2*sqrt(var_lambda) ),
                     mu_lambda+2*sqrt(var_lambda) )


par(mfrow = c(1,2))
plot_density(shape = a_gamma, rate = b_gamma,
             dist = "gamma", main = "gamma prior", use_x11_device = F,
             xlim = x_range_gamma)
plot_density(shape = a_lambda, rate = b_lambda,
             dist = "gamma", main = "lambda prior", use_x11_device = F,
             xlim = x_range_lambda)
```



Con i parametri scelti, plotto un'approssimazione al $99.5\%$ della massa totale della distribuzione del numero di cluster
```{r}
prob_k = rep(0,sum(n_j))
threshold = 0.995
for(kk in 1:sum(n_j) ){
  prob_k[kk] = p_distinct_prior(k = kk, n_j = n_j, 
                                gamma = rep(mu_gamma,d),
                                prior = "Poisson", lambda = mu_lambda)
  if(sum(prob_k)>=threshold)
    break
}

plot(which(prob_k>0), prob_k[prob_k>0], 
     type = 'b', pch = 16, lty = 1, lwd = 3,
     main = "Prior number of clusters", xlab = "K", ylab = "prob")
```

## Run

```{r}
# Run  --------------------------------------------------------------------
niter  <- 5000
burnin <- 5000
thin   <- 10

#P0_hyparam = empirical_bayes_normalinvgamma(data = data)

# a_gamma <- 1; b_gamma <- 1
# a_lambda <- 10; b_lambda <- 2

option = set_options_marginal(
             "mu0" = mu0, "k0"= k0, "nu0"=nu0, "sigma0"= sigma0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = mu_gamma,"Lambda0" = mu_lambda, 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = seq(1,sum(n_j))
        )
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)

```

```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```


```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)

#mean( apply(part_matrix, 1, FUN = function(part_it){arandi(part_it,real_partition, adjust = T)}) )
```


```{r}
l_grid = 500
grid = seq(-15,20,length.out = l_grid)
# Predictive in all groups
Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)

# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]
par(mfrow = c(1,d))
for(j in 1:d){
  
  # set colors
  mycol_cluster = brewer.pal(n=K, name = "Dark2")
  mycol_points  = as.factor(real_partition[idx_start:idx_end])
  levels(mycol_points) = unique(mycol_cluster)
  
  # frequentist density
  dj = density(data[j,1:n_j[j]])
  
  # set axis limits
  yrange = range(Pred_all[[j]][3,]) + 0.1*max(Pred_all[[j]][3,])
  xrange = range( c(data[j,1:n_j[j]], grid) )
  
  # Open new empty plot so that I can add the grid below
  # the displayed figures
  par( mar = c(2,2,1,1), bty="l" )
  plot(0,0, type = "n",
       ylim = c(0,0.1), xlim = xrange,
       main = paste0("Level = ",j), cex.main = 0.9, 
       ylab = "y-axis", xlab = "x-axis")
  # add grid
  grid(lty = 1,lwd = 2,col = "gray90" )
  # add histogram
  hist(data[j,1:n_j[j]], 
       freq = F, 
       breaks = n_j[j]/6, 
       add = T )
  # add observed points
  points( x = data[j,1:n_j[j]], y = rep(0,n_j[j]), 
          pch = 16, col = mycol_points)
  # add frequentist density
  lines(dj$x,dj$y, lwd = 3)
  # add estimate density with credible band for estimated density
  polygon( c(grid, rev(grid)), 
           c(Pred_all[[j]][1,], rev(Pred_all[[j]][3,])),
           col = ACutils::t_col(mycol[1], percent = 45),
           border = ACutils::t_col(mycol[1], percent = 45) ) 
  lines(x = grid, y = Pred_all[[j]][2,], col = mycol[1], lwd = 3) #0.5
  legend("topleft", lty = 1, lwd = 2,
         legend = c("empricial", "bayesian"),
         col = c("black", mycol[1]))

  idx_start = idx_end + 1
    if(j<d){
    idx_end = idx_end + n_j[j+1]
  }
}

```


























