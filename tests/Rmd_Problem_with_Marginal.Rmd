---
title: "Marginal Sampler with 1 level only"
author: "Alessandro Colombi"
date: "30/8/2022"
output: html_document
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  out.width = "50%"
)
```


Testing Marginal sampler, when $d=1$. Density estimation is now implemented.


*Dati esperimento:*

* **K = 4**
* d = 1
* $n_1 = 200$
* $\mu_1 = -20$, $\mu_1 = -10$, $\mu_3 = -20$, $\mu_4 = -20$
* $\sigma_1 = \sigma_2 = \sigma_3 = \sigma_4 = 1$
* $\pi_1 = \pi_2 = \pi_3 = \pi_4 = 1/4$

# Partizione Fissata

```{r}
suppressWarnings(suppressPackageStartupMessages(library(GDFMM)))
suppressWarnings(suppressPackageStartupMessages(library(ACutils)))
suppressWarnings(suppressPackageStartupMessages(library(tidyverse)))
suppressWarnings(suppressPackageStartupMessages(library(RColorBrewer)))
suppressWarnings(suppressPackageStartupMessages(library(salso)))
suppressWarnings(suppressPackageStartupMessages(library(wesanderson)))
suppressWarnings(suppressPackageStartupMessages(library(mcclust.ext)))



# color palette -----------------------------------------------------------
mycol = hcl.colors(n=3,palette = "Zissou1")

# data generation ---------------------------------------------------------

d = 1                   # number of groups
K = 4                   # number of global clusters
mu = c(-20,-10,0, 10)   # vectors of means
sd = c(1,1,1,1)         # vector of sd
n_j = rep(200, d)        # set cardinality of the groups
seed = 20051131

#genD = generate_data(d = d, K = K, mu = mu, sd = sd, n_j = n_j, seed = seed)
genD = simulate_data(d = d, K = K, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition

mycol_cluster = brewer.pal(n=K, name = "Dark2")

```

```{r}
counter = 1
data_per_cluster = vector("list", length = K)
log_marginal_prior = vector("list", length = d)
for(j in 1:d){
  log_marginal_prior[[j]] = vector( length = n_j[j])
  for(i in 1:n_j[j]){
    c_ji = real_partition[counter]
    data_per_cluster[[c_ji]] = c(data_per_cluster[[c_ji]], data[j,i])
    log_marginal_prior[[j]][i] = log(ACutils::dnct(data[j,i], n0 = 10, mu0 = 1, gamma0 = sqrt(2)) )
    counter = counter + 1
  }
  
}
#data_per_cluster 
mean_data_per_cluster = lapply(data_per_cluster, mean)
var_data_per_cluster  = lapply(data_per_cluster, var)
#log_marginal_prior
```

## Iperparametri con empirical Bayes

In questo primo esperimento, setto i parametri relativi a $P_0$ con la funzione di empirical Bayes che abbiamo fatto.
Plotto i dati e la marginale del modello, che è 

$student-t(dof = \nu_0, loc = \mu_0, scale = \sqrt{\frac{k_0}{k_0+1}\sigma^2_0})$
```{r}
# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]

# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 1, varsig2 = 1)
mu0 = P0_hyparam$mu0;k0= P0_hyparam$k0;nu0=P0_hyparam$nu0;sigma0=P0_hyparam$sigma0

l_grid = 200
grid = seq(-25,15,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)

for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(  data = tibble(grid = grid, value = Prior_grid),
                color = 'black', aes(x=grid,y=value), size = 1.2,
                inherit.aes = F)  +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}
```
Sembra essere proprio la situazione che voglio, la prior abbraccia tutti e 4 i cluster


```{r}
# Run  --------------------------------------------------------------------
niter  <- 1000
burnin <- 1000
thin   <- 1

P0_hyparam = empirical_bayes_normalinvgamma(data = data)

option = set_options_marginal(
             "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = 1,"Lambda0" = 3, 
             "alpha_gamma"=5, "beta_gamma"=0.5, 
             "alpha_lambda"=1, "beta_lambda"=5,
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = real_partition
        )
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = T, option = option)

```




Analisi parte parametrica con partizione fissa
```{r}
mu_est = rep(0,K)
s2_est = rep(1,K)

for(it in 1:niter){
  mu_est = mu_est + GDFMM$mu[[it]]
  s2_est = s2_est + GDFMM$sigma[[it]]
}
mu_est = mu_est/niter
s2_est = s2_est/niter

mu_est
s2_est
```
Le stime fanno schifo, se le medie ancora ancora possono ricordare quelle vere, le varianze sono gigantesche

```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
#Pred_all = predictive_all_groups(grid = grid, fit = GDFMM, burnin = 1)

# Predictive in one group
Pred_one_group = predictive_marginal(idx_group = 1, grid = grid, fit = GDFMM, option = option)

j = 1
idx_start = 1
idx_end = n_j[1]
tibble(value = data[j,],
       true_clus = as.factor(real_partition[idx_start:idx_end])
       ) %>%
  ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
  geom_point(y = rep(0.005,length(data[j,])), size = 2) +
  geom_histogram(data = tibble(value = data[j,]),
                 aes(x=value, y = ..density..), 
                 color = 'black', alpha = 0.3,
                 binwidth = 0.5, inherit.aes = F) +
  scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
  scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] )  +
  geom_path( data = as_tibble( t(Pred_one_group) ),
             color = 'black', aes(x=grid,y=`50%`), size = 1.2,
             inherit.aes = F)  +
  labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
  geom_ribbon(data = as_tibble( t(Pred_one_group) ), fill = mycol[1],
              aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
              alpha = 0.5, inherit.aes = F)

```
La predittiva non c'entra niente con i dati. 


## Iperparametri di default

La partizione la tengo sempre fissa, questa volta però gli iperparametri non li scelgo con la funzione di empirical Bayes ma con le scelte di default che erano maturate con un po' di esperienza sul codice. In particolare:

$\mu_0 = 0;k_0= 1/10;\nu_0=10;\sigma^2_0=1$
```{r}
# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]

# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra

mu0 = 0;k0= 1/10;nu0=10;sigma0=1

l_grid = 200
grid = seq(-25,15,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)

for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(  data = tibble(grid = grid, value = Prior_grid),
                color = 'black', aes(x=grid,y=value), size = 1.2,
                inherit.aes = F)  +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}
```
Non sembra essere per niente una buona marginale (a priori) dei dati, è tutta schiacciata su un solo cluster 


```{r}
# Run  --------------------------------------------------------------------
niter  <- 1000
burnin <- 1000
thin   <- 1

P0_hyparam = empirical_bayes_normalinvgamma(data = data)

option = set_options_marginal(
             "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = 1,"Lambda0" = 3, 
             "alpha_gamma"=5, "beta_gamma"=0.5, 
             "alpha_lambda"=1, "beta_lambda"=5,
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = real_partition
        )
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = T, option = option)

```




Analisi parte parametrica con partizione fissa
```{r}
mu_est = rep(0,K)
s2_est = rep(1,K)

for(it in 1:niter){
  mu_est = mu_est + GDFMM$mu[[it]]
  s2_est = s2_est + GDFMM$sigma[[it]]
}
mu_est = mu_est/niter
s2_est = s2_est/niter

mu_est
s2_est
```
Le stime sono quasi perfette
```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
#Pred_all = predictive_all_groups(grid = grid, fit = GDFMM, burnin = 1)

# Predictive in one group
Pred_one_group = predictive_marginal(idx_group = 1, grid = grid, fit = GDFMM, 
                                     option = option, burnin = 0)

j = 1
idx_start = 1
idx_end = n_j[1]
tibble(value = data[j,],
       true_clus = as.factor(real_partition[idx_start:idx_end])
       ) %>%
  ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
  geom_point(y = rep(0.005,length(data[j,])), size = 2) +
  geom_histogram(data = tibble(value = data[j,]),
                 aes(x=value, y = ..density..), 
                 color = 'black', alpha = 0.3,
                 binwidth = 0.5, inherit.aes = F) +
  scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
  scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] )  +
  geom_path( data = as_tibble( t(Pred_one_group) ),
             color = 'black', aes(x=grid,y=`50%`), size = 1.2,
             inherit.aes = F)  +
  labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
  geom_ribbon(data = as_tibble( t(Pred_one_group) ), fill = mycol[1],
              aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
              alpha = 0.5, inherit.aes = F)

```
La predittiva è proprio dove dovrebbe essere 


Provo a spiegare l'evidenza osservata:

* Dal punto di vista della marginale $m(X)$, la $P_0$ scelta con iperparametri scelti con empirical Bayes sembra molto bella perché abbraccia bene i dati. Se scelgo invece "a caso" ho una $m(X)$ schiacciata solo sui dati vicini allo zero (perché metto $\mu_0 = 0$), quindi sembra piuttosto brutta. 

* Dal punto di vista delle medie a posteriori, abbiamo

$\mu_k | X_1,\dots,X_n \sim N(\frac{n_k}{n_k + k_0}\bar{X_k} + \frac{k_0}{n_k + k_0}\mu_0 )$

$\sigma^2_k | X_1,\dots,X_n \sim InvGamma(\frac{\nu_0 + n_k}{2}, \frac{1}{2}(\nu_0\sigma_0^2 + (n_k-1)V_k + \frac{n_k k_0}{n_k + k_0}(\bar{X}_k - \mu_0)^2))$

dove $n_k$ è la numerosità, $\bar{X}_k$ è la media campionaria e $V_k$ è la varianza campionaria del cluster $k$.
In particolare, con empircal bayes vengono dei valori molto alti di $k_0$ e quindi per esempio il peso legato alla prior nella media a posteriori è molto simile a quello dato all'evidenza dei dati $\frac{n_k}{n_k + k_0} \approx  \frac{k_0}{n_k + k_0}$. Per la varianza invece viene $\nu_0\sigma_0^2$ molto più alto degli altri termini.

* Quindi, dalla media a posteriori mi viene da dire che sarebbe meglio avere $k_0$ basso in modo da essere meno informativi sulla media
  - In Empirical Bayes mettiamo $k_0 = \frac{\bar{\sigma^2}}{\bar{V_\mu}}$, quindi è meglio aumentare $\bar{V_\mu}$. Per i      ragionamenti che ho fatto sulla varianza, vuol dire diminuire un po' la varianza di $X$. 
    Bello che $k_0$ sia l'unico valore a dipendere da $\bar{V_{\mu}}$, quindi posso regolarlo direttamente.
    

* Per $\nu_0$ viene scelto un valore enorme, tipo $2700$. Quindi in $\sigma_k$ conta solo la parte dalla prior
  - Per contenere $\nu$ posso aumentare $\bar{V_\sigma}$, ha anche un po' di impatto su $\sigma_0^2$. 
  - Per ridurre tutto, si può lavorare anche sul parametro di correzione: per fare matching dei momenti, metto
    $E[\sigma^2] = \bar{\sigma^2}/correction$. Seguendo il lavoro di Raf avevo sempre considerato $correction = 3$ 
    ma penso sia meglio alzarlo.
  
* La contraddizione però resta, tutti questi interventi hanno reso la $m(X)$ molto più concentrata invece che diffusa

## Iperparametri con empirical Bayes - Modificati

Faccio un terzo esperimento settando i parametri con empirical bayes seguendo le intuizioni elencate al punto precedente
```{r}
# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]

# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 100, varsig2 = 100, correction = 10)
mu0 = P0_hyparam$mu0;k0= P0_hyparam$k0;nu0=P0_hyparam$nu0;sigma0=P0_hyparam$sigma0

l_grid = 200
grid = seq(-25,15,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)

for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(  data = tibble(grid = grid, value = Prior_grid),
                color = 'black', aes(x=grid,y=value), size = 1.2,
                inherit.aes = F)  +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}
```
Un po' più larga che con i valori a caso ma molto più stretta rispetto a prima.


```{r}
# Run  --------------------------------------------------------------------
niter  <-  1000
burnin <-  1000
thin   <- 1

P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 1000, varsig2 = 1000, correction = 10)

option = set_options_marginal(
             "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = 1,"Lambda0" = 3, 
             "alpha_gamma"=5, "beta_gamma"=0.5, 
             "alpha_lambda"=1, "beta_lambda"=5,
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = real_partition
        )
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = T, option = option)

```




Analisi parte parametrica con partizione fissa
```{r}
mu_est = rep(0,K)
s2_est = rep(0,K)

for(it in 1:niter){
  mu_est = mu_est + GDFMM$mu[[it]]
  s2_est = s2_est + GDFMM$sigma[[it]]
}
mu_est = mu_est/niter
s2_est = s2_est/niter

mu_est
s2_est
```
Ora le stime tornano ad essere piuttosto buone

```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
#Pred_all = predictive_all_groups(grid = grid, fit = GDFMM, burnin = 1)

# Predictive in one group
Pred_one_group = predictive_marginal(idx_group = 1, grid = grid, fit = GDFMM, option = option)

j = 1
idx_start = 1
idx_end = n_j[1]
tibble(value = data[j,],
       true_clus = as.factor(real_partition[idx_start:idx_end])
       ) %>%
  ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
  geom_point(y = rep(0.005,length(data[j,])), size = 2) +
  geom_histogram(data = tibble(value = data[j,]),
                 aes(x=value, y = ..density..), 
                 color = 'black', alpha = 0.3,
                 binwidth = 0.5, inherit.aes = F) +
  scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
  scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] )  +
  geom_path( data = as_tibble( t(Pred_one_group) ),
             color = 'black', aes(x=grid,y=`50%`), size = 1.2,
             inherit.aes = F)  +
  labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
  geom_ribbon(data = as_tibble( t(Pred_one_group) ), fill = mycol[1],
              aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
              alpha = 0.5, inherit.aes = F)

```
Anche la predittiva è dove dovrebbe essere



# Partizione aleatoria

Rispetto alla prime sezione, ora lascio variare anche la partizione. Quindi tutti i parametri entrano in gioco e devono essere aggiornati. Come prima, analizzerò 3 modi per settare gli iperparametri di $P_0$: con empirical Bayes, con le scelte di default e con empirical Bayes modificato. Vedremo che con la prima scelta, ancora una volta non abbiamo risultati accettabili. Con gli altri due si, dunque per questi due casi analizzo 3 situazioni diverse sulla partizione iniziale: quella vera, quella dove tutti sono in un unico cluster e quella in cui ognuno fa un cluster a sé.

Inolte, discuto anche una procedura su come scegliere i valori iniziali e gli iperparametri per $\gamma$ e $\Lambda$.

```{r}
suppressWarnings(suppressPackageStartupMessages(library(GDFMM)))
suppressWarnings(suppressPackageStartupMessages(library(ACutils)))
suppressWarnings(suppressPackageStartupMessages(library(tidyverse)))
suppressWarnings(suppressPackageStartupMessages(library(RColorBrewer)))
suppressWarnings(suppressPackageStartupMessages(library(salso)))
suppressWarnings(suppressPackageStartupMessages(library(wesanderson)))
suppressWarnings(suppressPackageStartupMessages(library(mcclust.ext)))



# color palette -----------------------------------------------------------
mycol = hcl.colors(n=3,palette = "Zissou1")

# data generation ---------------------------------------------------------

d = 1                   # number of groups
K = 4                   # number of global clusters
mu = c(-20,-10,0, 10)   # vectors of means
sd = c(1,1,1,1)         # vector of sd
n_j = rep(200, d)        # set cardinality of the groups
seed = 2352571

#genD = generate_data(d = d, K = K, mu = mu, sd = sd, n_j = n_j, seed = seed)
genD = simulate_data(d = d, K = K, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition

mycol_cluster = brewer.pal(n=K, name = "Dark2")

table(real_partition)
```


```{r}
counter = 1
data_per_cluster = vector("list", length = K)
log_marginal_prior = vector("list", length = d)
for(j in 1:d){
  log_marginal_prior[[j]] = vector( length = n_j[j])
  for(i in 1:n_j[j]){
    c_ji = real_partition[counter]
    data_per_cluster[[c_ji]] = c(data_per_cluster[[c_ji]], data[j,i])
    log_marginal_prior[[j]][i] = log(ACutils::dnct(data[j,i], n0 = 10, mu0 = 1, gamma0 = sqrt(2)) )
    counter = counter + 1
  }
  
}
#data_per_cluster 
mean_data_per_cluster = lapply(data_per_cluster, mean)
var_data_per_cluster  = lapply(data_per_cluster, var)

mean_data_per_cluster

var_data_per_cluster
#log_marginal_prior
```


Provo a fissare $\gamma$ e $\Lambda$ in modo intelligente.
Voglio che la probabilità a priori di avere $K=4$ cluster sia più alta possibile.


```{r}
# f è la funzione da ottimizzare
f <- function(x){
  -p_distinct_prior(k = K, n_j = c(n_j), 
                   gamma = c(x[1]), 
                   prior = "Poisson", lambda = x[2] )
}

opt = optim(par = c(0.1,0.1), #initial points
            method = "L-BFGS-B", # that's the only method that takes bounds
            fn  = f, #function to be minimized
            lower = c(0.001,0.001) #lower bound for variables
            )
opt

```

Parametrizzo la gamma secondo media e varianza. Questo perché ho trovato dei valori di $\gamma$ e $\Lambda$ che a priori mi piacciono, quindi voglio settare delle prior centrate su quei valori e regolare la varianza attorno ad essi.

Se $X \sim gamma(a,b)$, allora $E[X] = a/b$ e $V(X) = a/b^2$.

Invece, se $X \sim gamma(\mu_x,V_x)$, allora $a = \frac{\mu_x^2}{V_x}$ e $b = \frac{\mu_x}{V_x}$

```{r}
# gamma
mu_gamma  = opt$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma
x_range_gamma = c(  max(0, mu_gamma-2*sqrt(var_gamma) ),
                    mu_gamma+2*sqrt(var_gamma) )
#lambda
mu_lambda  = opt$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)
x_range_lambda = c(  max(0, mu_lambda-2*sqrt(var_lambda) ),
                     mu_lambda+2*sqrt(var_lambda) )


par(mfrow = c(1,2))
plot_density(shape = a_gamma, rate = b_gamma,
             dist = "gamma", main = "gamma prior", use_x11_device = F,
             xlim = x_range_gamma)
plot_density(shape = a_lambda, rate = b_lambda,
             dist = "gamma", main = "lambda prior", use_x11_device = F,
             xlim = x_range_lambda)
```


## Iperparametri con empirical Bayes

In questo primo esperimento, setto i parametri relativi a $P_0$ con la funzione di empirical Bayes che abbiamo fatto.
Plotto i dati e la marginale del modello, che è 

$student-t(dof = \nu_0, loc = \mu_0, scale = \sqrt{\frac{k_0}{k_0+1}\sigma^2_0})$
```{r}
# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]

# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 1, varsig2 = 1)
mu0 = P0_hyparam$mu0;k0= P0_hyparam$k0;nu0=P0_hyparam$nu0;sigma0=P0_hyparam$sigma0

l_grid = 200
grid = seq(-25,15,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)

for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(  data = tibble(grid = grid, value = Prior_grid),
                color = 'black', aes(x=grid,y=value), size = 1.2,
                inherit.aes = F)  +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}
```
Sembra essere proprio la situazione che voglio, la prior abbraccia tutti e 4 i cluster

*Parto dalla partizione vera*
```{r}
# Run  --------------------------------------------------------------------
niter  <-  1000
burnin <-  1000
thin   <- 1

# tau 
P0_hyparam = empirical_bayes_normalinvgamma(data = data)

# gamma
mu_gamma  = opt$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma

#lambda
mu_lambda  = opt$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)


option = set_options_marginal(
             "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = opt$par[1],"Lambda0" = opt$par[2], 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = real_partition
        )

# P0 Hyperparameters:
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,

# Process parameters
# old set:  "gamma0" = 1,"Lambda0" = 3, 
#           "alpha_gamma"=5, "beta_gamma"=0.5, "alpha_lambda"=1, "beta_lambda"=5,


#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```


```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```

```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)
```
Clustering osceno, è sempre su 2 cluster ma non va per niente bene. Inoltre la $U$ e la $\gamma$ hanno traceplot bruttissimi.

Le stime dei parametri $\mu$ e $\sigma$ sono praticamente uguali al caso in cui la partizione viene tenuta fissa (in particolare le varianze sono grandissime). Ovviamente la predittiva fa schifo tanto quanto l'altro caso
```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
#Pred_all = predictive_all_groups(grid = grid, fit = GDFMM, burnin = 1)

# Predictive in one group
Pred_one_group = predictive_marginal(idx_group = 1, grid = grid, fit = GDFMM, 
                                     option = option, burnin = 0)

j = 1
idx_start = 1
idx_end = n_j[1]
tibble(value = data[j,],
       true_clus = as.factor(real_partition[idx_start:idx_end])
       ) %>%
  ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
  geom_point(y = rep(0.005,length(data[j,])), size = 2) +
  geom_histogram(data = tibble(value = data[j,]),
                 aes(x=value, y = ..density..), 
                 color = 'black', alpha = 0.3,
                 binwidth = 0.5, inherit.aes = F) +
  scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
  scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] )  +
  geom_path( data = as_tibble( t(Pred_one_group) ),
             color = 'black', aes(x=grid,y=`50%`), size = 1.2,
             inherit.aes = F)  +
  labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
  geom_ribbon(data = as_tibble( t(Pred_one_group) ), fill = mycol[1],
              aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
              alpha = 0.5, inherit.aes = F)

```




## Iperparametri di default

Faccio come prima ma gli iperparametri gli scelgo "a caso", con dei valori selezionati senza guardare i dati
```{r}
# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]

# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra

mu0 = 0;k0= 1/10;nu0=10;sigma0=1

l_grid = 200
grid = seq(-25,15,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)

for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(  data = tibble(grid = grid, value = Prior_grid),
                color = 'black', aes(x=grid,y=value), size = 1.2,
                inherit.aes = F)  +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}
```

```{r}
# gamma
mu_gamma  = opt$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma
x_range_gamma = c(  max(0, mu_gamma-2*sqrt(var_gamma) ),
                    mu_gamma+2*sqrt(var_gamma) )
#lambda
mu_lambda  = opt$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)
x_range_lambda = c(  max(0, mu_lambda-2*sqrt(var_lambda) ),
                     mu_lambda+2*sqrt(var_lambda) )


par(mfrow = c(1,2))
plot_density(shape = a_gamma, rate = b_gamma,
             dist = "gamma", main = "gamma prior", use_x11_device = F,
             xlim = x_range_gamma)
plot_density(shape = a_lambda, rate = b_lambda,
             dist = "gamma", main = "lambda prior", use_x11_device = F,
             xlim = x_range_lambda)
```


### Parto da partizione vera
```{r}
# Run  --------------------------------------------------------------------
niter  <-  1000
burnin <-  1000
thin   <- 1

# tau 
P0_hyparam = empirical_bayes_normalinvgamma(data = data)

# gamma
mu_gamma  = opt$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma

#lambda
mu_lambda  = opt$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)


option = set_options_marginal(
             "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = opt$par[1],"Lambda0" = opt$par[2], 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = real_partition
        )

# P0 Hyperparameters:
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,

# Process parameters
# old set:  "gamma0" = 1,"Lambda0" = 3, 
#           "alpha_gamma"=5, "beta_gamma"=0.5, "alpha_lambda"=1, "beta_lambda"=5,


#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```

```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```

```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)
```



```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
#Pred_all = predictive_all_groups(grid = grid, fit = GDFMM, burnin = 1)

# Predictive in one group
Pred_one_group = predictive_marginal(idx_group = 1, grid = grid, fit = GDFMM, 
                                     option = option, burnin = 0)

j = 1
idx_start = 1
idx_end = n_j[1]
tibble(value = data[j,],
       true_clus = as.factor(real_partition[idx_start:idx_end])
       ) %>%
  ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
  geom_point(y = rep(0.005,length(data[j,])), size = 2) +
  geom_histogram(data = tibble(value = data[j,]),
                 aes(x=value, y = ..density..), 
                 color = 'black', alpha = 0.3,
                 binwidth = 0.5, inherit.aes = F) +
  scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
  scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] )  +
  geom_path( data = as_tibble( t(Pred_one_group) ),
             color = 'black', aes(x=grid,y=`50%`), size = 1.2,
             inherit.aes = F)  +
  labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
  geom_ribbon(data = as_tibble( t(Pred_one_group) ), fill = mycol[1],
              aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
              alpha = 0.5, inherit.aes = F)

```


Riassunto:

* $\Lambda$ è molto stabile sui valori scelti dalla prior
* $\gamma$ e $U$ hanno delle brutte catene, sembrano molto autocorrelate.
* Sensibilità rispetto al parametro MALA $s_p$: se aumenta ($0.1$), la catena di $U$ esplora anche valori molto più alti mentre per quanto riguarda $\gamma$ non ci sono cambiamenti significativi. Se diminuisce ($0.001$) sembra ci voglia di più per la convergenza, $\gamma$ diminuisce e $U$ non esplode su valori grandi. Il clustering si assesta su partizioni più parsimoniose ma a nei risultati finali non ci sono differenze.  
* $K$ numero di cluster *NON* è quello corretto, è più alto ma si muove tanto, non rimane fisso. Sembra mixxare molto.
* A posteriori però ritrovo il clustering esatto 
* La stima della densità è come vorrei

### Parto da ognuno fa cluster da se
```{r}
# Run  --------------------------------------------------------------------
niter  <-  1000
burnin <-  2000
thin   <- 1

# tau 
P0_hyparam = empirical_bayes_normalinvgamma(data = data)

# gamma
mu_gamma  = opt$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma

#lambda
mu_lambda  = opt$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)


option = set_options_marginal(
             "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = opt$par[1],"Lambda0" = opt$par[2], 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = seq(1:length(real_partition))#real_partition
        )

# P0 Hyperparameters:
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,

# Process parameters
# old set:  "gamma0" = 1,"Lambda0" = 3, 
#           "alpha_gamma"=5, "beta_gamma"=0.5, "alpha_lambda"=1, "beta_lambda"=5,


#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```

```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```

```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)
```



```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
#Pred_all = predictive_all_groups(grid = grid, fit = GDFMM, burnin = 1)

# Predictive in one group
Pred_one_group = predictive_marginal(idx_group = 1, grid = grid, fit = GDFMM, 
                                     option = option, burnin = 0)

j = 1
idx_start = 1
idx_end = n_j[1]
tibble(value = data[j,],
       true_clus = as.factor(real_partition[idx_start:idx_end])
       ) %>%
  ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
  geom_point(y = rep(0.005,length(data[j,])), size = 2) +
  geom_histogram(data = tibble(value = data[j,]),
                 aes(x=value, y = ..density..), 
                 color = 'black', alpha = 0.3,
                 binwidth = 0.5, inherit.aes = F) +
  scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
  scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] )  +
  geom_path( data = as_tibble( t(Pred_one_group) ),
             color = 'black', aes(x=grid,y=`50%`), size = 1.2,
             inherit.aes = F)  +
  labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
  geom_ribbon(data = as_tibble( t(Pred_one_group) ), fill = mycol[1],
              aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
              alpha = 0.5, inherit.aes = F)

```

Riassunto:

* Ho aumentato il numero di iterazioni perché $\gamma$ non sembrava per niente a convergenza.
* Per il resto, le conclusioni sono tali e quali al caso in cui partivo dalla partizione vera.

### Parto da tutti in un unico cluster
```{r}
# Run  --------------------------------------------------------------------
niter  <-  2000
burnin <-  3000
thin   <- 1

# tau 
P0_hyparam = empirical_bayes_normalinvgamma(data = data)

# gamma
mu_gamma  = opt$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma

#lambda
mu_lambda  = opt$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)


option = set_options_marginal(
             "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = opt$par[1],"Lambda0" = opt$par[2], 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = NULL#real_partition
        )

# P0 Hyperparameters:
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,

# Process parameters
# old set:  "gamma0" = 1,"Lambda0" = 3, 
#           "alpha_gamma"=5, "beta_gamma"=0.5, "alpha_lambda"=1, "beta_lambda"=5,


#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```

```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```

```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)
```



```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
#Pred_all = predictive_all_groups(grid = grid, fit = GDFMM, burnin = 1)

# Predictive in one group
Pred_one_group = predictive_marginal(idx_group = 1, grid = grid, fit = GDFMM, 
                                     option = option, burnin = 0)

j = 1
idx_start = 1
idx_end = n_j[1]
tibble(value = data[j,],
       true_clus = as.factor(real_partition[idx_start:idx_end])
       ) %>%
  ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
  geom_point(y = rep(0.005,length(data[j,])), size = 2) +
  geom_histogram(data = tibble(value = data[j,]),
                 aes(x=value, y = ..density..), 
                 color = 'black', alpha = 0.3,
                 binwidth = 0.5, inherit.aes = F) +
  scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
  scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] )  +
  geom_path( data = as_tibble( t(Pred_one_group) ),
             color = 'black', aes(x=grid,y=`50%`), size = 1.2,
             inherit.aes = F)  +
  labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
  geom_ribbon(data = as_tibble( t(Pred_one_group) ), fill = mycol[1],
              aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
              alpha = 0.5, inherit.aes = F)

```

Riassunto:

* Questo è il caso più problematico, ho notato che cambiando il seed con cui genero i dati, a volte va bene e a volte no



## Iperparametri con empirical Bayes - Modificati

Seguo le indicazioni della sezione precedente
```{r}
# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]

# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 100, varsig2 = 100, correction = 10)
mu0 = P0_hyparam$mu0;k0= P0_hyparam$k0;nu0=P0_hyparam$nu0;sigma0=P0_hyparam$sigma0

l_grid = 200
grid = seq(-25,15,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)

for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(  data = tibble(grid = grid, value = Prior_grid),
                color = 'black', aes(x=grid,y=value), size = 1.2,
                inherit.aes = F)  +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}
```

```{r}
# gamma
mu_gamma  = opt$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma
x_range_gamma = c(  max(0, mu_gamma-2*sqrt(var_gamma) ),
                    mu_gamma+2*sqrt(var_gamma) )
#lambda
mu_lambda  = opt$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)
x_range_lambda = c(  max(0, mu_lambda-2*sqrt(var_lambda) ),
                     mu_lambda+2*sqrt(var_lambda) )


par(mfrow = c(1,2))
plot_density(shape = a_gamma, rate = b_gamma,
             dist = "gamma", main = "gamma prior", use_x11_device = F,
             xlim = x_range_gamma)
plot_density(shape = a_lambda, rate = b_lambda,
             dist = "gamma", main = "lambda prior", use_x11_device = F,
             xlim = x_range_lambda)
```


### Parto da partizione vera
```{r}
# Run  --------------------------------------------------------------------
niter  <-  1000
burnin <-  1000
thin   <- 1


# tau 
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 100, varsig2 = 100, correction = 10)

# gamma
mu_gamma  = opt$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma

#lambda
mu_lambda  = opt$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)


option = set_options_marginal(
             "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = opt$par[1],"Lambda0" = opt$par[2], 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = real_partition
        )

# P0 Hyperparameters:
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,

# Process parameters
# old set:  "gamma0" = 1,"Lambda0" = 3, 
#           "alpha_gamma"=5, "beta_gamma"=0.5, "alpha_lambda"=1, "beta_lambda"=5,


#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```

```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```

```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)
```



```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
#Pred_all = predictive_all_groups(grid = grid, fit = GDFMM, burnin = 1)

# Predictive in one group
Pred_one_group = predictive_marginal(idx_group = 1, grid = grid, fit = GDFMM, 
                                     option = option, burnin = 0)

j = 1
idx_start = 1
idx_end = n_j[1]
tibble(value = data[j,],
       true_clus = as.factor(real_partition[idx_start:idx_end])
       ) %>%
  ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
  geom_point(y = rep(0.005,length(data[j,])), size = 2) +
  geom_histogram(data = tibble(value = data[j,]),
                 aes(x=value, y = ..density..), 
                 color = 'black', alpha = 0.3,
                 binwidth = 0.5, inherit.aes = F) +
  scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
  scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] )  +
  geom_path( data = as_tibble( t(Pred_one_group) ),
             color = 'black', aes(x=grid,y=`50%`), size = 1.2,
             inherit.aes = F)  +
  labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
  geom_ribbon(data = as_tibble( t(Pred_one_group) ), fill = mycol[1],
              aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
              alpha = 0.5, inherit.aes = F)

```


Riassunto:

* $K$ numero di cluster mi piace molto, perché come valore più basso ha 4 che è il numero di cluster veri, e poi esplora un po' cosa succede per $K$ maggiori
* Questa volta anche la matrice di similarity è praticamente perfetta
* A posteriori ritrovo il clustering esatto
* La stima della densità è come vorrei

### Parto da ognuno fa cluster da se
```{r}
# Run  --------------------------------------------------------------------
niter  <-  2000
burnin <-  2000
thin   <- 1

# tau 
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 100, varsig2 = 100, correction = 10)


# gamma
mu_gamma  = opt$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma

#lambda
mu_lambda  = opt$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)


option = set_options_marginal(
             "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = opt$par[1],"Lambda0" = opt$par[2], 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = seq(1:length(real_partition))#real_partition
        )

# P0 Hyperparameters:
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,

# Process parameters
# old set:  "gamma0" = 1,"Lambda0" = 3, 
#           "alpha_gamma"=5, "beta_gamma"=0.5, "alpha_lambda"=1, "beta_lambda"=5,


#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```

```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```

```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)
```



```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
#Pred_all = predictive_all_groups(grid = grid, fit = GDFMM, burnin = 1)

# Predictive in one group
Pred_one_group = predictive_marginal(idx_group = 1, grid = grid, fit = GDFMM, 
                                     option = option, burnin = 0)

j = 1
idx_start = 1
idx_end = n_j[1]
tibble(value = data[j,],
       true_clus = as.factor(real_partition[idx_start:idx_end])
       ) %>%
  ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
  geom_point(y = rep(0.005,length(data[j,])), size = 2) +
  geom_histogram(data = tibble(value = data[j,]),
                 aes(x=value, y = ..density..), 
                 color = 'black', alpha = 0.3,
                 binwidth = 0.5, inherit.aes = F) +
  scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
  scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] )  +
  geom_path( data = as_tibble( t(Pred_one_group) ),
             color = 'black', aes(x=grid,y=`50%`), size = 1.2,
             inherit.aes = F)  +
  labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
  geom_ribbon(data = as_tibble( t(Pred_one_group) ), fill = mycol[1],
              aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
              alpha = 0.5, inherit.aes = F)

```

Riassunto:

* Mi piace, il comportamento è molto simile al caso degli iperparametri settati di default ma c'è da dire che la matrice di similarity ora è quasi perfetta. Infatti si arriva alla partizione corretta anche usando la Binder loss, cosa che per il caso di default non avveniva, bisognava guardare solo la VI. 
* Anche la convergenza mi piace, ho aumentato il numero di iterazioni ma non era cosi necessario come nel caso di prima


### Parto da tutti in un unico cluster
```{r}
# Run  --------------------------------------------------------------------
niter  <-  2000
burnin <-  2000
thin   <- 1

# tau 
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 100, varsig2 = 100, correction = 10)

# gamma
mu_gamma  = opt$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma

#lambda
mu_lambda  = opt$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)


option = set_options_marginal(
             "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = opt$par[1],"Lambda0" = opt$par[2], 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = NULL#real_partition
        )

# P0 Hyperparameters:
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,

# Process parameters
# old set:  "gamma0" = 1,"Lambda0" = 3, 
#           "alpha_gamma"=5, "beta_gamma"=0.5, "alpha_lambda"=1, "beta_lambda"=5,


#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```

```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```

```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)
```



```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
#Pred_all = predictive_all_groups(grid = grid, fit = GDFMM, burnin = 1)

# Predictive in one group
Pred_one_group = predictive_marginal(idx_group = 1, grid = grid, fit = GDFMM, 
                                     option = option, burnin = 0)

j = 1
idx_start = 1
idx_end = n_j[1]
tibble(value = data[j,],
       true_clus = as.factor(real_partition[idx_start:idx_end])
       ) %>%
  ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
  geom_point(y = rep(0.005,length(data[j,])), size = 2) +
  geom_histogram(data = tibble(value = data[j,]),
                 aes(x=value, y = ..density..), 
                 color = 'black', alpha = 0.3,
                 binwidth = 0.5, inherit.aes = F) +
  scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
  scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] )  +
  geom_path( data = as_tibble( t(Pred_one_group) ),
             color = 'black', aes(x=grid,y=`50%`), size = 1.2,
             inherit.aes = F)  +
  labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
  geom_ribbon(data = as_tibble( t(Pred_one_group) ), fill = mycol[1],
              aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
              alpha = 0.5, inherit.aes = F)

```

Riassunto:

Ancora tutto bene

# CONCLUSIONE
* Settare una $P_0$ più stretta rispetto ai dati ma comunque scelta usando il metodo empirical Bayes sembra essere la scelta più conveniente in termini di mixing, rapidità di convergenza (anche se le catene di $\gamma$ e $\U$ continuano ad essere molto autocorrelate) e come similarity matrix. 
* Le catene di $U$ e $\gamma$ dipendono chiaramente dalla scelta di parametro MALA $s_p$. In questo caso semplice in cui $d=1$ non sembra esserci niente di problematico, tuttavia bisogna vedere se questo crea problemi quando $d>1$
* *NON* riesco proprio a capire come mai la scelta di iperparametri fatta con empirical Bayes che ci porta a delle marginali a priori che abbracciano molto bene i dati non funzioni. Se guardo come vengono calcolate le stime a posteriori è chiaro che non stiamo facendo una buona scelta di iperparametri. Però graficamente mi sembrerebbe davvero un'ottima scelta. Non me lo riesco a spiegare.
















