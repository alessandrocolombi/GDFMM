---
title: "Marginal Sampler with 2 levels"
author: "Alessandro Colombi"
date: "30/8/2022"
output: html_document
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  out.width = "50%"
)
```


Given the simulation study with $d=1$, I now proceede to analyze the behaviour for $d=2$

*Dati esperimento:*

* **K = 4**, di cui due condivisi e due livello specifici
* d = 2
* $n_1 = 200$, $n_2 = 200$
* $\mu_1 = -20$, $\mu_1 = -10$, $\mu_3 = 0$, $\mu_4 = 15$
* $\sigma_1 = \sigma_2 = \sigma_3 = \sigma_4 = 1$
* $\pi_{11} = \pi_{12} = \pi_{14} = 1/3, \pi_{13} = 0$
* $\pi_{21} = \pi_{23} = \pi_{24} = 1/3, \pi_{22} = 0$

# Partizione Fissata

```{r}
suppressWarnings(suppressPackageStartupMessages(library(GDFMM)))
suppressWarnings(suppressPackageStartupMessages(library(ACutils)))
suppressWarnings(suppressPackageStartupMessages(library(tidyverse)))
suppressWarnings(suppressPackageStartupMessages(library(RColorBrewer)))
suppressWarnings(suppressPackageStartupMessages(library(salso)))
suppressWarnings(suppressPackageStartupMessages(library(wesanderson)))
suppressWarnings(suppressPackageStartupMessages(library(mcclust.ext)))
```


```{r}
# color palette -----------------------------------------------------------
mycol = hcl.colors(n=3,palette = "Zissou1")

# data generation ---------------------------------------------------------

d = 2                   # number of groups
K = 4                   # number of global clusters
mu = c(-20,-10,0, 15)   # vectors of means
sd = c(1,1,1,1)         # vector of sd
n_j = rep(200, d)        # set cardinality of the groups
mix_probs = matrix(c(1/3,1/3,0,1/3,
                     1/3,0,1/3,1/3), nrow = d, ncol = K, byrow = T)
seed = 20051131

genD = simulate_data(d = d, K = K, p = mix_probs, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition

mycol_cluster = brewer.pal(n=K, name = "Dark2")

```

```{r}
counter = 1
data_per_cluster = vector("list", length = K)
log_marginal_prior = vector("list", length = d)
for(j in 1:d){
  log_marginal_prior[[j]] = vector( length = n_j[j])
  for(i in 1:n_j[j]){
    c_ji = real_partition[counter]
    data_per_cluster[[c_ji]] = c(data_per_cluster[[c_ji]], data[j,i])
    log_marginal_prior[[j]][i] = log(ACutils::dnct(data[j,i], n0 = 10, mu0 = 1, gamma0 = sqrt(2)) )
    counter = counter + 1
  }
  
}
#data_per_cluster 
mean_data_per_cluster = lapply(data_per_cluster, mean)
mean_data_per_cluster
var_data_per_cluster  = lapply(data_per_cluster, var)
var_data_per_cluster
#log_marginal_prior
```

## Iperparametri con empirical Bayes

In questo primo esperimento, setto i parametri relativi a $P_0$ con la funzione di empirical Bayes che abbiamo fatto.
Plotto i dati e la marginale del modello, che è 

$student-t(dof = \nu_0, loc = \mu_0, scale = \sqrt{\frac{k_0}{k_0+1}\sigma^2_0})$
```{r}
# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]

# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 1, varsig2 = 1)
mu0 = P0_hyparam$mu0;k0= P0_hyparam$k0;nu0=P0_hyparam$nu0;sigma0=P0_hyparam$sigma0

l_grid = 200
grid = seq(-25,15,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)

for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(  data = tibble(grid = grid, value = Prior_grid),
                color = 'black', aes(x=grid,y=value), size = 1.2,
                inherit.aes = F)  +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}
```
Molto larga, abbraccia tutti e 4 i cluster


```{r}
# Run  --------------------------------------------------------------------
niter  <- 1000
burnin <- 1000
thin   <- 1

P0_hyparam = empirical_bayes_normalinvgamma(data = data)

option = set_options_marginal(
             "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = 1,"Lambda0" = 3, 
             "alpha_gamma"=5, "beta_gamma"=0.5, 
             "alpha_lambda"=1, "beta_lambda"=5,
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = real_partition
        )
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = T, option = option)

```




Analisi parte parametrica con partizione fissa
```{r}
mu_est = rep(0,K)
s2_est = rep(1,K)

for(it in 1:niter){
  mu_est = mu_est + GDFMM$mu[[it]]
  s2_est = s2_est + GDFMM$sigma[[it]]
}
mu_est = mu_est/niter
s2_est = s2_est/niter

mu_est
s2_est
```
Le stime fanno schifo, se le medie ancora ancora possono ricordare quelle vere, le varianze sono gigantesche
```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)

# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]
for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(data = as_tibble(t(Pred_all[[j]])), color = 'black', aes(x=grid,y=`50%`), size = 1.2,
              inherit.aes = F) +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
    geom_ribbon(data = as_tibble(t(Pred_all[[j]])), fill = mycol[1],
                aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
                alpha = 0.5, inherit.aes = F)
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}

```

La predittiva non c'entra niente con i dati. 


## Iperparametri di default

La partizione la tengo sempre fissa, questa volta però gli iperparametri non li scelgo con la funzione di empirical Bayes ma con le scelte di default che erano maturate con un po' di esperienza sul codice. In particolare:

$\mu_0 = 0;k_0= 1/10;\nu_0=10;\sigma^2_0=1$
```{r}
# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]

# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra

mu0 = 0;k0= 1/10;nu0=10;sigma0=1

l_grid = 200
grid = seq(-25,15,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)

for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(  data = tibble(grid = grid, value = Prior_grid),
                color = 'black', aes(x=grid,y=value), size = 1.2,
                inherit.aes = F)  +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}
```
Non sembra essere per niente una buona marginale (a priori) dei dati, nel secondo livello è centrato in un punto in cui non ci sono dati


```{r}
# Run  --------------------------------------------------------------------
niter  <- 1000
burnin <- 1000
thin   <- 1

P0_hyparam = empirical_bayes_normalinvgamma(data = data)

option = set_options_marginal(
             "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = 1,"Lambda0" = 3, 
             "alpha_gamma"=5, "beta_gamma"=0.5, 
             "alpha_lambda"=1, "beta_lambda"=5,
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = real_partition
        )
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = T, option = option)

```




Analisi parte parametrica con partizione fissa
```{r}
mu_est = rep(0,K)
s2_est = rep(1,K)

for(it in 1:niter){
  mu_est = mu_est + GDFMM$mu[[it]]
  s2_est = s2_est + GDFMM$sigma[[it]]
}
mu_est = mu_est/niter
s2_est = s2_est/niter

mu_est
s2_est
```
Le stime sono quasi perfette, si vede la gobbetta che viene dalla scambio di informazione tra i livelli: anche se non ci sono dati, la preditivva mette anche lì perché nell'altro livello invece c'è un cluster
```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)

# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]
for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(data = as_tibble(t(Pred_all[[j]])), color = 'black', aes(x=grid,y=`50%`), size = 1.2,
              inherit.aes = F) +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
    geom_ribbon(data = as_tibble(t(Pred_all[[j]])), fill = mycol[1],
                aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
                alpha = 0.5, inherit.aes = F)
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}

```

Fin ora, sembra tutto come nel caso di un solo livello, $d=1$

## Iperparametri con empirical Bayes - Modificati

Faccio un terzo esperimento settando i parametri con empirical bayes seguendo le intuizioni elencate nel report precedente. Nota, usando gli stessi parametri e le stesse correzioni di prima, ora la marginale a priori risulta più larga di quanto non lo fosse nel caso con $d=1$
```{r}
# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]

# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 100, varsig2 = 100, correction = 10)
mu0 = P0_hyparam$mu0;k0= P0_hyparam$k0;nu0=P0_hyparam$nu0;sigma0=P0_hyparam$sigma0

l_grid = 200
grid = seq(-25,15,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)

for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(  data = tibble(grid = grid, value = Prior_grid),
                color = 'black', aes(x=grid,y=value), size = 1.2,
                inherit.aes = F)  +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}
```

Un po' più larga che con i valori a caso ma molto più stretta rispetto a quella iniziale. 


```{r}
# Run  --------------------------------------------------------------------
niter  <-  1000
burnin <-  1000
thin   <- 1

P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 1000, varsig2 = 1000, correction = 10)

option = set_options_marginal(
             "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = 1,"Lambda0" = 3, 
             "alpha_gamma"=5, "beta_gamma"=0.5, 
             "alpha_lambda"=1, "beta_lambda"=5,
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = real_partition
        )
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = T, option = option)

```




Analisi parte parametrica con partizione fissa
```{r}
mu_est = rep(0,K)
s2_est = rep(0,K)

for(it in 1:niter){
  mu_est = mu_est + GDFMM$mu[[it]]
  s2_est = s2_est + GDFMM$sigma[[it]]
}
mu_est = mu_est/niter
s2_est = s2_est/niter

mu_est
s2_est
```

Ora le stime tornano ad essere piuttosto buone, idem per la predittiva che anche in questo caso mostra la gobbetta dove c'è il cluster assente

```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)

# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]
for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(data = as_tibble(t(Pred_all[[j]])), color = 'black', aes(x=grid,y=`50%`), size = 1.2,
              inherit.aes = F) +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
    geom_ribbon(data = as_tibble(t(Pred_all[[j]])), fill = mycol[1],
                aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
                alpha = 0.5, inherit.aes = F)
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}

```



# Partizione aleatoria

Rispetto alla prime sezione, ora lascio variare anche la partizione. Quindi tutti i parametri entrano in gioco e devono essere aggiornati. Come prima, analizzerò 3 modi per settare gli iperparametri di $P_0$: con empirical Bayes, con le scelte di default e con empirical Bayes modificato. 

Vedremo che con la prima scelta, ancora una volta non abbiamo risultati accettabili. Con gli altri due si, dunque per questi due casi analizzo 3 situazioni diverse sulla partizione iniziale: quella vera, quella dove tutti sono in un unico cluster e quella in cui ognuno fa un cluster a sé.

Inolte, discuto anche una procedura su come scegliere i valori iniziali e gli iperparametri per $\gamma$ e $\Lambda$.

```{r}
suppressWarnings(suppressPackageStartupMessages(library(GDFMM)))
suppressWarnings(suppressPackageStartupMessages(library(ACutils)))
suppressWarnings(suppressPackageStartupMessages(library(tidyverse)))
suppressWarnings(suppressPackageStartupMessages(library(RColorBrewer)))
suppressWarnings(suppressPackageStartupMessages(library(salso)))
suppressWarnings(suppressPackageStartupMessages(library(wesanderson)))
suppressWarnings(suppressPackageStartupMessages(library(mcclust.ext)))
```


```{r}
# color palette -----------------------------------------------------------
mycol = hcl.colors(n=3,palette = "Zissou1")

# data generation ---------------------------------------------------------

d = 2                   # number of groups
K = 4                   # number of global clusters
mu = c(-20,-10,0, 15)   # vectors of means
sd = c(1,1,1,1)         # vector of sd
n_j = rep(200, d)        # set cardinality of the groups
mix_probs = matrix(c(1/3,1/3,0,1/3,
                     1/3,0,1/3,1/3), nrow = d, ncol = K, byrow = T)
seed = 20051131

genD = simulate_data(d = d, K = K, p = mix_probs, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition

mycol_cluster = brewer.pal(n=K, name = "Dark2")

```

```{r}
counter = 1
data_per_cluster = vector("list", length = K)
log_marginal_prior = vector("list", length = d)
for(j in 1:d){
  log_marginal_prior[[j]] = vector( length = n_j[j])
  for(i in 1:n_j[j]){
    c_ji = real_partition[counter]
    data_per_cluster[[c_ji]] = c(data_per_cluster[[c_ji]], data[j,i])
    log_marginal_prior[[j]][i] = log(ACutils::dnct(data[j,i], n0 = 10, mu0 = 1, gamma0 = sqrt(2)) )
    counter = counter + 1
  }
  
}
#data_per_cluster 
mean_data_per_cluster = lapply(data_per_cluster, mean)
mean_data_per_cluster
var_data_per_cluster  = lapply(data_per_cluster, var)
var_data_per_cluster
#log_marginal_prior
```


Provo a fissare $\gamma$ e $\Lambda$ in modo intelligente.
Ora vorrei sia che la probabilità a priori di avere $K=4$ cluster sia più alta possibile, ma anche che la probabilità di avere $S = 3$ cluster condivisi sia più alta possibile. 

* **p_shared_prior** non riesco ad usarla dentro ad un'ottimizzazione, è troppo lenta
* Un altro problema è che l'ottimizzazione viene fatta su tre parametri ($\gamma_1,\gamma_2,\Lambda$) ma poi nel modello stiamo assumendo le $\gamma_j$ iid da un'unica distribuzione. 
  - Un'idea potrebbe essere ottimizzare con il vincolo che $\gamma_1 = \gamma_2$

```{r}
#esempio
p_distinct_prior(k = K, n_j = n_j, prior = "Poisson",
                 gamma = c(0.5,0.5), lambda = 6 )
p_shared_prior(s=3,n_j = n_j, gamma = c(0.5,0.5),prior = "Poisson", lambda = 6) #much slower


# fK è la funzione da ottimizzare: now it takes 3 parameters, gamma_1, gamma_2 and Lambda
fK <- function(x){
  -p_distinct_prior(k = K, n_j = n_j, 
                   gamma = c(x[1],x[2]), 
                   prior = "Poisson", lambda = x[3] )
}



optK = optim(par = c(0.1,0.1,0.1), #initial points
            method = "L-BFGS-B", # that's the only method that takes bounds
            fn  = fK, #function to be minimized
            lower = c(0.001,0.001,0.001) #lower bound for variables
            )
optK

p_shared_prior(s=3,n_j = n_j, gamma = c(optK$par[1],optK$par[2]),prior = "Poisson", lambda = optK$par[3]) 

```
Con i parametri che sceglie, **K=4** è molto probabile perché ha più di $1/4$ della massa totale ma la probabilità di avere **S=3** è praticamente nulla.


Parametrizzo la gamma secondo media e varianza. Questo perché ho trovato dei valori di $\gamma$ e $\Lambda$ che a priori mi piacciono, quindi voglio settare delle prior centrate su quei valori e regolare la varianza attorno ad essi.

Se $X \sim gamma(a,b)$, allora $E[X] = a/b$ e $V(X) = a/b^2$.

Invece, se $X \sim gamma(\mu_x,V_x)$, allora $a = \frac{\mu_x^2}{V_x}$ e $b = \frac{\mu_x}{V_x}$


```{r}
# fK è la funzione da ottimizzare: now it takes 3 parameters, gamma_1, gamma_2 and Lambda
fK <- function(x){
  -p_distinct_prior(k = K, n_j = n_j, 
                    gamma = c(x[1],x[1]), 
                    prior = "Poisson", lambda = x[2] )
}



optK = optim(par = c(0.1,0.1), #initial points
             method = "L-BFGS-B", # that's the only method that takes bounds
             fn  = fK, #function to be minimized
             lower = c(0.001,0.001) #lower bound for variables
)
optK
```


```{r}
# gamma
mu_gamma  = optK$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma
x_range_gamma = c(  max(0, mu_gamma-2*sqrt(var_gamma) ),
                    mu_gamma+2*sqrt(var_gamma) )
#lambda
mu_lambda  = optK$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)
x_range_lambda = c(  max(0, mu_lambda-2*sqrt(var_lambda) ),
                     mu_lambda+2*sqrt(var_lambda) )


par(mfrow = c(1,2))
plot_density(shape = a_gamma, rate = b_gamma,
             dist = "gamma", main = "gamma prior", use_x11_device = F,
             xlim = x_range_gamma)
plot_density(shape = a_lambda, rate = b_lambda,
             dist = "gamma", main = "lambda prior", use_x11_device = F,
             xlim = x_range_lambda)
```

Da notare quanto $\gamma$ sia schiacciatissima su 0

## Iperparametri con empirical Bayes

In questo primo esperimento, setto i parametri relativi a $P_0$ con la funzione di empirical Bayes che abbiamo fatto.
Plotto i dati e la marginale del modello, che è 

$student-t(dof = \nu_0, loc = \mu_0, scale = \sqrt{\frac{k_0}{k_0+1}\sigma^2_0})$
```{r}
# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]

# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 1, varsig2 = 1)
mu0 = P0_hyparam$mu0;k0= P0_hyparam$k0;nu0=P0_hyparam$nu0;sigma0=P0_hyparam$sigma0

l_grid = 200
grid = seq(-25,15,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)

for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(  data = tibble(grid = grid, value = Prior_grid),
                color = 'black', aes(x=grid,y=value), size = 1.2,
                inherit.aes = F)  +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}
```


*Parto dalla partizione vera*
```{r}
# Run  --------------------------------------------------------------------
niter  <-  1000
burnin <-  1000
thin   <- 1

# tau 
P0_hyparam = empirical_bayes_normalinvgamma(data = data)

# gamma
mu_gamma  = optK$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma

#lambda
mu_lambda  = optK$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)


option = set_options_marginal(
             "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = optK$par[1],"Lambda0" = optK$par[2], 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = unlist(mean_data_per_cluster), 
             "init_var_cluster" = unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = real_partition
        )

# P0 Hyperparameters:
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,

# Process parameters
# old set:  "gamma0" = 1,"Lambda0" = 3, 
#           "alpha_gamma"=5, "beta_gamma"=0.5, "alpha_lambda"=1, "beta_lambda"=5,


#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```


```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```

Come al solito, le catene di $U$ e $\gamma$ sono pessime. Quella di $K$ non è male ma è troppo concentrata su due soli cluster

```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)
```
Clustering osceno, forse la catena si muove un po' più del caso $d=1$ però. Comunque la partizione stimata alla fine non va bene

Le stime dei parametri $\mu$ e $\sigma$ sono praticamente uguali al caso in cui la partizione viene tenuta fissa (in particolare le varianze sono grandissime). Ovviamente la predittiva fa schifo tanto quanto l'altro caso

```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)

# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]
for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(data = as_tibble(t(Pred_all[[j]])), color = 'black', aes(x=grid,y=`50%`), size = 1.2,
              inherit.aes = F) +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
    geom_ribbon(data = as_tibble(t(Pred_all[[j]])), fill = mycol[1],
                aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
                alpha = 0.5, inherit.aes = F)
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}

```




## Iperparametri di default

Faccio come prima ma gli iperparametri gli scelgo "a caso", con dei valori selezionati senza guardare i dati
```{r}
# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]

# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra

mu0 = 0;k0= 1/10;nu0=10;sigma0=1

l_grid = 200
grid = seq(-25,15,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)

for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(  data = tibble(grid = grid, value = Prior_grid),
                color = 'black', aes(x=grid,y=value), size = 1.2,
                inherit.aes = F)  +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}
```

```{r}
# gamma
mu_gamma  = optK$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma
x_range_gamma = c(  max(0, mu_gamma-2*sqrt(var_gamma) ),
                    mu_gamma+2*sqrt(var_gamma) )
#lambda
mu_lambda  = optK$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)
x_range_lambda = c(  max(0, mu_lambda-2*sqrt(var_lambda) ),
                     mu_lambda+2*sqrt(var_lambda) )


par(mfrow = c(1,2))
plot_density(shape = a_gamma, rate = b_gamma,
             dist = "gamma", main = "gamma prior", use_x11_device = F,
             xlim = x_range_gamma)
plot_density(shape = a_lambda, rate = b_lambda,
             dist = "gamma", main = "lambda prior", use_x11_device = F,
             xlim = x_range_lambda)
```


### Parto da partizione vera
```{r}
# Run  --------------------------------------------------------------------
niter  <-  1000
burnin <-  1000
thin   <- 1

# tau 
P0_hyparam = empirical_bayes_normalinvgamma(data = data)

# gamma
mu_gamma  = optK$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma

#lambda
mu_lambda  = optK$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)


option = set_options_marginal(
             "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = optK$par[1],"Lambda0" = optK$par[2], 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = unlist(mean_data_per_cluster), 
             "init_var_cluster" = unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = real_partition
        )

# P0 Hyperparameters:
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,

# Process parameters
# old set:  "gamma0" = 1,"Lambda0" = 3, 
#           "alpha_gamma"=5, "beta_gamma"=0.5, "alpha_lambda"=1, "beta_lambda"=5,


#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```

```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```



```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)
```



```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)

# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]
for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(data = as_tibble(t(Pred_all[[j]])), color = 'black', aes(x=grid,y=`50%`), size = 1.2,
              inherit.aes = F) +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
    geom_ribbon(data = as_tibble(t(Pred_all[[j]])), fill = mycol[1],
                aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
                alpha = 0.5, inherit.aes = F)
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}

```


**Riassunto**

* Clustering e predittiva sono ottimi
* $\Lambda$ anche, rimane molto vicina ai valori scelti dalla prior
* $U$ e $\gamma$ hanno catene veramente brutte
* Di $K$ mi piace che la catena sembra muoversi abbastanza, tuttavia ha come moda valori maggiori di quello vero e infatti con la Binder loss trovo troppi cluster. Il numero minimo di cluster che esplora è quello vero, che tutto sommato può essere ragionevole

### Parto da ognuno fa cluster da se
```{r}
# Run  --------------------------------------------------------------------
niter  <-  1000
burnin <-  2000
thin   <- 1

# tau 
P0_hyparam = empirical_bayes_normalinvgamma(data = data)

# gamma
mu_gamma  = optK$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma

#lambda
mu_lambda  = optK$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)


option = set_options_marginal(
             "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = optK$par[1],"Lambda0" = optK$par[2], 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = seq(1:length(real_partition))#real_partition
        )

# P0 Hyperparameters:
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,

# Process parameters
# old set:  "gamma0" = 1,"Lambda0" = 3, 
#           "alpha_gamma"=5, "beta_gamma"=0.5, "alpha_lambda"=1, "beta_lambda"=5,


#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```

```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```

```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)
```

```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)

# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]
for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(data = as_tibble(t(Pred_all[[j]])), color = 'black', aes(x=grid,y=`50%`), size = 1.2,
              inherit.aes = F) +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
    geom_ribbon(data = as_tibble(t(Pred_all[[j]])), fill = mycol[1],
                aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
                alpha = 0.5, inherit.aes = F)
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}

```

**Riassunto**

* Clustering e predittiva sono ottimi
* $\Lambda$ anche, rimane molto vicina ai valori scelti dalla prior
* $U$ e $\gamma$ hanno catene veramente brutte
* Ora $K$ si assesta su valori più piccoli rispetto al caso di prima, forse si muove meno ma esplora di più la zona corretta. Infatti con la binder loss trovo un clustering più sensato, anche se non è ancora quello vero


### Parto da tutti in un unico cluster
```{r}
# Run  --------------------------------------------------------------------
niter  <-  2000
burnin <-  3000
thin   <- 1

# tau 
P0_hyparam = empirical_bayes_normalinvgamma(data = data)

# gamma
mu_gamma  = optK$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma

#lambda
mu_lambda  = optK$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)


option = set_options_marginal(
             "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = optK$par[1],"Lambda0" = optK$par[2], 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = NULL#real_partition
        )

# P0 Hyperparameters:
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,

# Process parameters
# old set:  "gamma0" = 1,"Lambda0" = 3, 
#           "alpha_gamma"=5, "beta_gamma"=0.5, "alpha_lambda"=1, "beta_lambda"=5,


#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```

```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```

```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)
```



```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)

# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]
for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(data = as_tibble(t(Pred_all[[j]])), color = 'black', aes(x=grid,y=`50%`), size = 1.2,
              inherit.aes = F) +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
    geom_ribbon(data = as_tibble(t(Pred_all[[j]])), fill = mycol[1],
                aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
                alpha = 0.5, inherit.aes = F)
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}

```

**Riassunto**

* Clustering e predittiva sono ottimi
* $\Lambda$ anche, rimane molto vicina ai valori scelti dalla prior
* $U$ e $\gamma$ hanno catene veramente brutte
* $K$ si muove tantissimo ma su valori più grandi


## Iperparametri con empirical Bayes - Modificati

Seguo le indicazioni della sezione precedente
```{r}
# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]

# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 100, varsig2 = 100, correction = 10)
mu0 = P0_hyparam$mu0;k0= P0_hyparam$k0;nu0=P0_hyparam$nu0;sigma0=P0_hyparam$sigma0

l_grid = 200
grid = seq(-25,15,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)

for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(  data = tibble(grid = grid, value = Prior_grid),
                color = 'black', aes(x=grid,y=value), size = 1.2,
                inherit.aes = F)  +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}
```

```{r}
# gamma
mu_gamma  = optK$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma
x_range_gamma = c(  max(0, mu_gamma-2*sqrt(var_gamma) ),
                    mu_gamma+2*sqrt(var_gamma) )
#lambda
mu_lambda  = optK$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)
x_range_lambda = c(  max(0, mu_lambda-2*sqrt(var_lambda) ),
                     mu_lambda+2*sqrt(var_lambda) )


par(mfrow = c(1,2))
plot_density(shape = a_gamma, rate = b_gamma,
             dist = "gamma", main = "gamma prior", use_x11_device = F,
             xlim = x_range_gamma)
plot_density(shape = a_lambda, rate = b_lambda,
             dist = "gamma", main = "lambda prior", use_x11_device = F,
             xlim = x_range_lambda)
```


### Parto da partizione vera
```{r}
# Run  --------------------------------------------------------------------
niter  <-  1000
burnin <-  1000
thin   <- 1


# tau 
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 100, varsig2 = 100, correction = 10)

# gamma
mu_gamma  = optK$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma

#lambda
mu_lambda  = optK$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)


option = set_options_marginal(
             "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = optK$par[1],"Lambda0" = optK$par[2], 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = real_partition
        )

# P0 Hyperparameters:
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,

# Process parameters
# old set:  "gamma0" = 1,"Lambda0" = 3, 
#           "alpha_gamma"=5, "beta_gamma"=0.5, "alpha_lambda"=1, "beta_lambda"=5,


#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```

```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```

```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)
```



```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)

# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]
for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(data = as_tibble(t(Pred_all[[j]])), color = 'black', aes(x=grid,y=`50%`), size = 1.2,
              inherit.aes = F) +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
    geom_ribbon(data = as_tibble(t(Pred_all[[j]])), fill = mycol[1],
                aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
                alpha = 0.5, inherit.aes = F)
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}

```

**Riassunto**

* Su questo caso ho sentimenti constrastanti. Il clustering è nettamente il migliore, la catena di $K$ si muove meno ma rimane molto più vicina ai valori corretti. 
  - Tuttavia le predittive sono più basse di quanto vorrei, secondo me è dovuto a come sto settando gli iperparameteri e basta
* $\Lambda$ anche, rimane molto vicina ai valori scelti dalla prior
* $U$ e $\gamma$ hanno catene veramente brutte



### Parto da ognuno fa cluster da se
```{r}
# Run  --------------------------------------------------------------------
niter  <-  2000
burnin <-  2000
thin   <- 1

# tau 
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 100, varsig2 = 100, correction = 10)


# gamma
mu_gamma  = optK$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma

#lambda
mu_lambda  = optK$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)


option = set_options_marginal(
             "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = optK$par[1],"Lambda0" = optK$par[2], 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = seq(1:length(real_partition))#real_partition
        )

# P0 Hyperparameters:
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,

# Process parameters
# old set:  "gamma0" = 1,"Lambda0" = 3, 
#           "alpha_gamma"=5, "beta_gamma"=0.5, "alpha_lambda"=1, "beta_lambda"=5,


#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```

```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```

```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)
```


```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)

# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]
for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(data = as_tibble(t(Pred_all[[j]])), color = 'black', aes(x=grid,y=`50%`), size = 1.2,
              inherit.aes = F) +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
    geom_ribbon(data = as_tibble(t(Pred_all[[j]])), fill = mycol[1],
                aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
                alpha = 0.5, inherit.aes = F)
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}

```

**Riassunto**

* Su questo caso ho sentimenti constrastanti. Il clustering è nettamente il migliore, la catena di $K$ si muove meno ma rimane molto più vicina ai valori corretti. 
  - Tuttavia le predittive sono più basse di quanto vorrei, secondo me è dovuto a come sto settando gli iperparameteri e basta
* $\Lambda$ anche, rimane molto vicina ai valori scelti dalla prior
* $U$ e $\gamma$ hanno catene veramente brutte


### Parto da tutti in un unico cluster
```{r}
# Run  --------------------------------------------------------------------
niter  <-  2000
burnin <-  2000
thin   <- 1

# tau 
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 100, varsig2 = 100, correction = 10)

# gamma
mu_gamma  = optK$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma

#lambda
mu_lambda  = optK$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)


option = set_options_marginal(
             "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = optK$par[1],"Lambda0" = optK$par[2], 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = NULL#real_partition
        )

# P0 Hyperparameters:
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,

# Process parameters
# old set:  "gamma0" = 1,"Lambda0" = 3, 
#           "alpha_gamma"=5, "beta_gamma"=0.5, "alpha_lambda"=1, "beta_lambda"=5,


#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```

```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```

```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)
```

```{r}
l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)

# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]
for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(data = as_tibble(t(Pred_all[[j]])), color = 'black', aes(x=grid,y=`50%`), size = 1.2,
              inherit.aes = F) +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
    geom_ribbon(data = as_tibble(t(Pred_all[[j]])), fill = mycol[1],
                aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
                alpha = 0.5, inherit.aes = F)
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}

```


**Riassunto**

* Su questo caso ho sentimenti constrastanti. Il clustering è nettamente il migliore, la catena di $K$ si muove meno ma rimane molto più vicina ai valori corretti. 
  - Tuttavia le predittive sono più basse di quanto vorrei, secondo me è dovuto a come sto settando gli iperparameteri e basta
* $\Lambda$ anche, rimane molto vicina ai valori scelti dalla prior
* $U$ e $\gamma$ hanno catene veramente brutte



# Due livelli - esempio difficile


```{r}
# color palette -----------------------------------------------------------
mycol = hcl.colors(n=3,palette = "Zissou1")

# data generation ---------------------------------------------------------

d = 2               # number of groups
K = 3               # number of global clusters
mu = c(-3,0,1)      # vectors of means
sd = c(sqrt(0.1), sqrt(0.5), sqrt(1.5) )     # vector of sd
prob <- matrix(c(0.2,0.8,  0,
                   0,0.2,0.8),nrow = d, ncol = K, byrow = T)
  
n_j = rep(200, d)  # set cardinality of the groups
seed = 20051131

#genD = generate_data_prob(d=d,p=prob, K=K, mu = mu, sd = sd, n_j = n_j, seed = seed)
genD = simulate_data(d=d,prob=prob, K=K, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition


table(real_partition)
table(real_partition[1:n_j[1]])
table(real_partition[(n_j[1]+1):(n_j[1] + n_j[2])])


mycol_cluster = brewer.pal(n=K, name = "Accent")
```


## Default iperparametri

Data plot with prior marginal

```{r}
# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]

# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra

mu0 = 0;k0= 1/10;nu0=10;sigma0=1

l_grid = 200
grid = seq(-5,5,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)

for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(  data = tibble(grid = grid, value = Prior_grid),
                color = 'black', aes(x=grid,y=value), size = 1.2,
                inherit.aes = F)  +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}
```

Set process hyperparameters
```{r}
# fK è la funzione da ottimizzare: now it takes 3 parameters, gamma_1, gamma_2 and Lambda
fK <- function(x){
  -p_distinct_prior(k = K, n_j = n_j, 
                    gamma = c(x[1],x[1]), 
                    prior = "Poisson", lambda = x[2] )
}



optK = optim(par = c(0.1,0.1), #initial points
             method = "L-BFGS-B", # that's the only method that takes bounds
             fn  = fK, #function to be minimized
             lower = c(0.001,0.001) #lower bound for variables
)
optK

# gamma
mu_gamma  = optK$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma
x_range_gamma = c(  max(0, mu_gamma-2*sqrt(var_gamma) ),
                    mu_gamma+2*sqrt(var_gamma) )
#lambda
mu_lambda  = optK$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)
x_range_lambda = c(  max(0, mu_lambda-2*sqrt(var_lambda) ),
                     mu_lambda+2*sqrt(var_lambda) )


par(mfrow = c(1,2))
plot_density(shape = a_gamma, rate = b_gamma,
             dist = "gamma", main = "gamma prior", use_x11_device = F,
             xlim = x_range_gamma)
plot_density(shape = a_lambda, rate = b_lambda,
             dist = "gamma", main = "lambda prior", use_x11_device = F,
             xlim = x_range_lambda)
```
Parto dalla partizione in cui ognuno fa cluster a sé
```{r}
## Run

niter  <-  3000
burnin <-  3000
thin   <- 1

# gamma
mu_gamma  = optK$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma

#lambda
mu_lambda  = optK$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)


option = set_options_marginal(
             "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = optK$par[1],"Lambda0" = optK$par[2], 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = seq(1:length(real_partition))#real_partition
        )

# P0 Hyperparameters:
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,

# Process parameters
# old set:  "gamma0" = 1,"Lambda0" = 3, 
#           "alpha_gamma"=5, "beta_gamma"=0.5, "alpha_lambda"=1, "beta_lambda"=5,


#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```

```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```

```{r}
l_grid = 200
grid = seq(-5,5,length.out = l_grid)

# Predictive in all groups
Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)

# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]
for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(data = as_tibble(t(Pred_all[[j]])), color = 'black', aes(x=grid,y=`50%`), size = 1.2,
              inherit.aes = F) +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
    geom_ribbon(data = as_tibble(t(Pred_all[[j]])), fill = mycol[1],
                aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
                alpha = 0.5, inherit.aes = F)
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}

```
Global Analysis
```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)
```

Analisi per gruppo:

Primo livello
```{r}
part_matrix_lv1 = part_matrix[,1:n_j[1]]
# Compute similarity matrix
sim_matrix_lv1 <- psm(part_matrix_lv1)


heatmap(sim_matrix_lv1)


binder_sara_lv1 = minbinder(sim_matrix_lv1)
VI_sara_lv1 = minVI(sim_matrix_lv1)

table(binder_sara_lv1$cl)
table(VI_sara_lv1$cl)
table(real_partition[1:n_j[1]])

cred_ball_lv1 = credibleball(VI_sara_lv1$cl, sim_matrix_lv1, alpha = 0.05 )
cred_ball_lv1$dist.uppervert
```

Secondo livello
```{r}
part_matrix_lv2 = part_matrix[,(n_j[1]+1):sum(n_j)]
# Compute similarity matrix
sim_matrix_lv2 <- psm(part_matrix_lv2)


heatmap(sim_matrix_lv2)


binder_sara_lv2 = minbinder(sim_matrix_lv2)
VI_sara_lv2 = minVI(sim_matrix_lv2)

table(binder_sara_lv2$cl)
table(VI_sara_lv2$cl)
table(real_partition[(n_j[1]+1):sum(n_j)])

#cred_ball_lv2 = credibleball(VI_sara_lv2$cl, sim_matrix_lv2, alpha = 0.05 )
#cred_ball_lv2$dist.uppervert

idx_special_data = which(real_partition[(n_j[1]+1):sum(n_j)] == 2)
idx_special_data
idx_bin = which(binder_sara_lv2$cl==2)
idx_bin
```

## Modified Bayes iperparametri

Data plot with prior marginal

```{r}
# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]

# Calcolo anche la marginale trovata con i valori di Empirical Bayes e la plotto sopra
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 10, varsig2 = 10, correction = 10)
mu0 = P0_hyparam$mu0;k0= P0_hyparam$k0;nu0=P0_hyparam$nu0;sigma0=P0_hyparam$sigma0

l_grid = 200
grid = seq(-5,5,length.out = l_grid)
scale = sqrt( k0/(k0 + 1) * sigma0 )
mean_marginal = mu0
var_marginal  = nu0/(nu0-2) * scale^2

Prior_grid = GDFMM:::dnct(x = grid, n0 = nu0, mu0 = mu0, gamma0 = scale)

for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(  data = tibble(grid = grid, value = Prior_grid),
                color = 'black', aes(x=grid,y=value), size = 1.2,
                inherit.aes = F)  +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}
```

Set process hyperparameters
```{r}
# fK è la funzione da ottimizzare: now it takes 3 parameters, gamma_1, gamma_2 and Lambda
fK <- function(x){
  -p_distinct_prior(k = K, n_j = n_j, 
                    gamma = c(x[1],x[1]), 
                    prior = "Poisson", lambda = x[2] )
}

fS <- function(x){
  -p_shared_prior(s = 1, n_j = n_j, 
                  gamma = c(x[1],x[1]), 
                  prior = "Poisson", lambda = x[2] )
}


optK = optim(par = c(0.1,0.1), #initial points
             method = "L-BFGS-B", # that's the only method that takes bounds
             fn  = fK, #function to be minimized
             lower = c(0.001,0.001), #lower bound for variables
             control = list(maxit = 4)
)
optK

# non buono questo
p_shared_prior(s=1,n_j = n_j, gamma = c(optK$par[1],optK$par[2]), 
               prior = "Poisson", lambda = optK$par[3])

# gamma
mu_gamma  = optK$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma
x_range_gamma = c(  max(0, mu_gamma-2*sqrt(var_gamma) ),
                    mu_gamma+2*sqrt(var_gamma) )
#lambda
mu_lambda  = optK$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)
x_range_lambda = c(  max(0, mu_lambda-2*sqrt(var_lambda) ),
                     mu_lambda+2*sqrt(var_lambda) )


par(mfrow = c(1,2))
plot_density(shape = a_gamma, rate = b_gamma,
             dist = "gamma", main = "gamma prior", use_x11_device = F,
             xlim = x_range_gamma)
plot_density(shape = a_lambda, rate = b_lambda,
             dist = "gamma", main = "lambda prior", use_x11_device = F,
             xlim = x_range_lambda)
```
Parto dalla partizione in cui ognuno fa cluster a sé
```{r}
## Run

niter  <-  3000
burnin <-  3000
thin   <- 1

# tau 
P0_hyparam = empirical_bayes_normalinvgamma(data = data, varmu = 10, varsig2 = 10, correction = 10)


# gamma
mu_gamma  = optK$par[1]
var_gamma = 0.1
b_gamma = mu_gamma/(var_gamma)
a_gamma = mu_gamma * b_gamma

#lambda
mu_lambda  = optK$par[2]
var_lambda = 0.1
a_lambda = (mu_lambda*mu_lambda)/var_lambda
b_lambda = mu_lambda/(var_lambda)


option = set_options_marginal(
             "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, 
             "sp_mala_U" = 0.01, "sp_mala_gamma"=0.01,
             "gamma0" = optK$par[1],"Lambda0" = optK$par[2], 
             "alpha_gamma"=a_gamma, "beta_gamma"=b_gamma, 
             "alpha_lambda"=a_lambda, "beta_lambda"=b_lambda,
             "init_mean_cluster" = NULL,#unlist(mean_data_per_cluster), 
             "init_var_cluster" = NULL,#unlist(var_data_per_cluster),
             "UpdateU" = T, "UpdateGamma" = T, "UpdateTau" = T, "UpdateLambda" = T, 
             "partition" = seq(1:length(real_partition))#real_partition
        )

# P0 Hyperparameters:
# old set: "mu0" = 0,"sigma0"= 1, "k0"= 1/10, "nu0"=10,
# new set: "mu0" = P0_hyparam$mu0, "k0"= P0_hyparam$k0, "nu0"=P0_hyparam$nu0, "sigma0"= P0_hyparam$sigma0,

# Process parameters
# old set:  "gamma0" = 1,"Lambda0" = 3, 
#           "alpha_gamma"=5, "beta_gamma"=0.5, "alpha_lambda"=1, "beta_lambda"=5,


#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_marginal_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```

```{r, echo = F}
#Lambda
plot(GDFMM$lambda, type = 'l', main = expression(Lambda))
hist(GDFMM$lambda, main = expression(Lambda))

# gamma
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$gamma[j,], type = 'l', main = paste0(expression(gamma), "_",j))
}

# U
par(mfrow = c(1,d))
for(j in 1:d){
  plot(GDFMM$U[j,], type = 'l', main = paste0("U_",j))
}

# K
par(mfrow = c(1,2))
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

```

```{r}
l_grid = 200
grid = seq(-5,5,length.out = l_grid)

# Predictive in all groups
Pred_all = predictive_marginal_all_groups(grid = grid, fit = GDFMM, burnin = 0, option = option)

# In questo plot sotto colori i pallini secondo il vero cluster.
idx_start = 1
idx_end = n_j[1]
for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(data = as_tibble(t(Pred_all[[j]])), color = 'black', aes(x=grid,y=`50%`), size = 1.2,
              inherit.aes = F) +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
    geom_ribbon(data = as_tibble(t(Pred_all[[j]])), fill = mycol[1],
                aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
                alpha = 0.5, inherit.aes = F)
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}

```
Global Analysis
```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION 

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition #GDFMM$Partition is a (n_iter x n_data) matrix

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)
```

Analisi per gruppo:

Primo livello
```{r}
part_matrix_lv1 = part_matrix[,1:n_j[1]]
# Compute similarity matrix
sim_matrix_lv1 <- psm(part_matrix_lv1)


heatmap(sim_matrix_lv1)


binder_sara_lv1 = minbinder(sim_matrix_lv1)
VI_sara_lv1 = minVI(sim_matrix_lv1)

table(binder_sara_lv1$cl)
table(VI_sara_lv1$cl)
table(real_partition[1:n_j[1]])

cred_ball_lv1 = credibleball(VI_sara_lv1$cl, sim_matrix_lv1, alpha = 0.05 )
cred_ball_lv1$dist.uppervert
```

Secondo livello
```{r}
part_matrix_lv2 = part_matrix[,(n_j[1]+1):sum(n_j)]
# Compute similarity matrix
sim_matrix_lv2 <- psm(part_matrix_lv2)


heatmap(sim_matrix_lv2)


binder_sara_lv2 = minbinder(sim_matrix_lv2)
VI_sara_lv2 = minVI(sim_matrix_lv2)

table(binder_sara_lv2$cl)
table(VI_sara_lv2$cl)
table(real_partition[(n_j[1]+1):sum(n_j)])

#cred_ball_lv2 = credibleball(VI_sara_lv2$cl, sim_matrix_lv2, alpha = 0.05 )
#cred_ball_lv2$dist.uppervert

idx_special_data = which(real_partition[(n_j[1]+1):sum(n_j)] == 2)
idx_special_data
idx_bin = which(binder_sara_lv2$cl==2)
idx_bin
```

















