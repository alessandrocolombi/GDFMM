---
title: "Rmd_Clustering_part1"
author: "Alessandro Colombi"
date: "20/5/2022"
output: pdf_document

---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  out.width = "50%"
)
```

Inizio a fare dei test sul clustering. 

## d=1

**Primo caso, K = 2**

Caso non simmetrico.
```{r, results='hide'}
suppressWarnings(suppressPackageStartupMessages(library(GDFMM)))
suppressWarnings(suppressPackageStartupMessages(library(ACutils)))
suppressWarnings(suppressPackageStartupMessages(library(tidyverse)))
suppressWarnings(suppressPackageStartupMessages(library(RColorBrewer)))
suppressWarnings(suppressPackageStartupMessages(library(salso)))
suppressWarnings(suppressPackageStartupMessages(library(wesanderson)))
suppressWarnings(suppressPackageStartupMessages(library(mcclust.ext)))

# color palette -----------------------------------------------------------
mycol = hcl.colors(n=3,palette = "Zissou1")
# data generation ---------------------------------------------------------

d = 1               # number of groups
K = 2               # number of global clusters
mu = c(-10,0)   # vectors of means
sd = c(1,1)      # vector of sd
n_j = rep(200, d)  # set cardinality of the groups
seed = 1243
mycol_cluster = brewer.pal(n=K, name = "Dark2")

genD = generate_data(d=d, K=K, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition

# Run  --------------------------------------------------------------------


niter  <- 1000
burnin <- 1000
thin   <- 1

option<-set_options("nu" = 1, "Mstar0" = 2, "Lambda0" = 3, "mu0" = 0,"sigma0"= 1, "gamma0" = 1,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, "Adapt_MH_power_lim"=10, "Adapt_MH_var0"=1,
             "k0"= 1/10, "nu0"=10, "alpha_gamma"=1,
             "beta_gamma"=1, "alpha_lambda"=1, "beta_lambda"=1,
             "UpdateU" = T, "UpdateM" = T, "UpdateGamma" = T, "UpdateS" = T,
             "UpdateTau" = T, "UpdateLambda" = T
)

#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```
```{r, echo = F}
# Analisi output ----------------------------------------------------------

#K
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

#Mstar
plot(GDFMM$Mstar, type = 'l', main = "Mstar")


# Predictive --------------------------------------------------------------

l_grid = 200
grid = seq(-15,15,length.out = l_grid)

# Predictive in all groups
Pred_all = predictive_all_groups(grid = grid, fit = GDFMM)

for(j in 1:d){
print(
  tibble(value = data[j,]) %>%
    ggplot(aes(x=value)) +
    geom_histogram(aes(y = ..density..), color = 'black', fill = 'darkred', alpha = 0.3,
                   binwidth = 0.5) +
    geom_point(y = rep(0,length(data[j,])), col = 'darkred') +
    geom_path(data = as_tibble(t(Pred_all[[j]])), color = 'black', aes(x=grid,y=`50%`), size = 1.2 ) +
    geom_ribbon(data = as_tibble(t(Pred_all[[j]])), fill = mycol[1],
                aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
                alpha = 0.5) +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +  theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
)
}

```
Controllo la qualit√† del clustering

- `part_matrix`: This is a `(n_iter x n)` matrix, the j-th column contains all the labels assign to the j-th data point for all saved iterations.
- `sim_matrix`: This is a `(n x n)` matrix. Element (i,j) contains the probability that data points i and j are clustered together.
- `estimate_partition`: this is a vector of length `n` containing the estimated partition.
```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION -------------------

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)

# Get estimated partition according to binder (or VI) loss functions.
# VI_dahl <- dlso(matr, loss = 'VI', estimate=NULL)
# binder_dahl <- dlso(part_matrix, loss = 'binder', estimate = sim_matrix)
# estimate_partition = as.vector(binder_dahl)
# 
# # Get quality indicies
# Kest = length(unique(estimate_partition))
# cat('\n Estimated number of clusters \n')
# Kest
# Binder_loss = salso::binder(truth = real_partition, estimate = estimate_partition)
# cat('\n Binder loss function \n')
# Binder_loss
# Tab = table(real_partition,estimate_partition)
# cat('\n Miss classification table \n')
# Tab

```

Caso simmetrico
```{r, results='hide'}
suppressWarnings(suppressPackageStartupMessages(library(GDFMM)))
suppressWarnings(suppressPackageStartupMessages(library(ACutils)))
suppressWarnings(suppressPackageStartupMessages(library(tidyverse)))
suppressWarnings(suppressPackageStartupMessages(library(RColorBrewer)))

# color palette -----------------------------------------------------------
mycol = hcl.colors(n=3,palette = "Zissou1")
# data generation ---------------------------------------------------------

d = 1               # number of groups
K = 2               # number of global clusters
mu = c(-2,2)   # vectors of means
sd = c(1,1)      # vector of sd
n_j = rep(200, d)  # set cardinality of the groups
seed = 32156
mycol_cluster = brewer.pal(n=K, name = "Dark2")

genD = generate_data(d=d, K=K, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition
# Run  --------------------------------------------------------------------


niter  <- 1000
burnin <- 1000
thin   <- 1

option<-set_options("nu" = 1, "Mstar0" = 2, "Lambda0" = 3, "mu0" = 0,"sigma0"= 1, "gamma0" = 1,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, "Adapt_MH_power_lim"=10, "Adapt_MH_var0"=1,
             "k0"= 1/10, "nu0"=10, "alpha_gamma"=1,
             "beta_gamma"=1, "alpha_lambda"=1, "beta_lambda"=1,
             "UpdateU" = T, "UpdateM" = T, "UpdateGamma" = T, "UpdateS" = T,
             "UpdateTau" = T, "UpdateLambda" = T
)

#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```
```{r, echo = F}
# Analisi output ----------------------------------------------------------

#K
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

#Mstar
plot(GDFMM$Mstar, type = 'l', main = "Mstar")


# Predictive --------------------------------------------------------------

l_grid = 200
grid = seq(-5,5,length.out = l_grid)

# Predictive in all groups
Pred_all = predictive_all_groups(grid = grid, fit = GDFMM)

for(j in 1:d){
print(
  tibble(value = data[j,]) %>%
    ggplot(aes(x=value)) +
    geom_histogram(aes(y = ..density..), color = 'black', fill = 'darkred', alpha = 0.3,
                   binwidth = 0.5) +
    geom_point(y = rep(0,length(data[j,])), col = 'darkred') +
    geom_path(data = as_tibble(t(Pred_all[[j]])), color = 'black', aes(x=grid,y=`50%`), size = 1.2 ) +
    geom_ribbon(data = as_tibble(t(Pred_all[[j]])), fill = mycol[1],
                aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
                alpha = 0.5) +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +  theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
)
}

```

```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION -------------------

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition

# Compute similarity matrix
sim_matrix <- psm(part_matrix)

# Get estimated partition according to binder (or VI) loss functions.
# VI_dahl <- dlso(matr, loss = 'VI', estimate=NULL)
binder_dahl <- dlso(part_matrix, loss = 'binder', estimate = sim_matrix)
estimate_partition = as.vector(binder_dahl)

# Get quality indicies
Kest = length(unique(estimate_partition))
cat('\n Estimated number of clusters \n')
Kest
Binder_loss = salso::binder(truth = real_partition, estimate = estimate_partition)
cat('\n Binder loss function \n')
Binder_loss
Tab = table(real_partition,estimate_partition)
cat('\n Miss classification table \n')
Tab
```

**Secondo caso, K = 3**

Caso simmetrico.
```{r, results='hide'}
suppressWarnings(suppressPackageStartupMessages(library(GDFMM)))
suppressWarnings(suppressPackageStartupMessages(library(ACutils)))
suppressWarnings(suppressPackageStartupMessages(library(tidyverse)))
suppressWarnings(suppressPackageStartupMessages(library(RColorBrewer)))
# data generation ---------------------------------------------------------

d = 1               # number of groups
K = 3               # number of global clusters
mu = c(-20,0, 20)   # vectors of means
sd = c(1,1,1)      # vector of sd
n_j = rep(200, d)  # set cardinality of the groups
seed = 124123
mycol_cluster = brewer.pal(n=K, name = "Dark2")

genD = generate_data(d=d, K=K, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition

# Run  --------------------------------------------------------------------


niter  <- 1000
burnin <- 1000
thin   <- 1

option<-set_options("nu" = 1, "Mstar0" = 2, "Lambda0" = 3, "mu0" = 0,"sigma0"= 1, "gamma0" = 1,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, "Adapt_MH_power_lim"=10, "Adapt_MH_var0"=1,
             "k0"= 1/10, "nu0"=10, "alpha_gamma"=1,
             "beta_gamma"=1, "alpha_lambda"=1, "beta_lambda"=1,
             "UpdateU" = T, "UpdateM" = T, "UpdateGamma" = T, "UpdateS" = T,
             "UpdateTau" = T, "UpdateLambda" = T
)

#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```

```{r, echo = F}
# Analisi output ----------------------------------------------------------

#K
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

#Mstar
plot(GDFMM$Mstar, type = 'l', main = "Mstar")


# Predictive --------------------------------------------------------------

l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
Pred_all = predictive_all_groups(grid = grid, fit = GDFMM)

for(j in 1:d){
print(
  tibble(value = data[j,]) %>%
    ggplot(aes(x=value)) +
    geom_histogram(aes(y = ..density..), color = 'black', fill = 'darkred', alpha = 0.3,
                   binwidth = 0.5) +
    geom_point(y = rep(0,length(data[j,])), col = 'darkred') +
    geom_path(data = as_tibble(t(Pred_all[[j]])), color = 'black', aes(x=grid,y=`50%`), size = 1.2 ) +
    geom_ribbon(data = as_tibble(t(Pred_all[[j]])), fill = mycol[1],
                aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
                alpha = 0.5) +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +  theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
)
}

```
```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION -------------------

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)

# Get estimated partition according to binder (or VI) loss functions.
# VI_dahl <- dlso(matr, loss = 'VI', estimate=NULL)
# binder_dahl <- dlso(part_matrix, loss = 'binder', estimate = sim_matrix)
# estimate_partition = as.vector(binder_dahl)
# 
# # Get quality indicies
# Kest = length(unique(estimate_partition))
# cat('\n Estimated number of clusters \n')
# Kest
# Binder_loss = salso::binder(truth = real_partition, estimate = estimate_partition)
# cat('\n Binder loss function \n')
# Binder_loss
# Tab = table(real_partition,estimate_partition)
# cat('\n Miss classification table \n')
# Tab

```


Caso NON simmetrico.
```{r, results='hide'}
suppressWarnings(suppressPackageStartupMessages(library(GDFMM)))
suppressWarnings(suppressPackageStartupMessages(library(ACutils)))
suppressWarnings(suppressPackageStartupMessages(library(tidyverse)))
suppressWarnings(suppressPackageStartupMessages(library(RColorBrewer)))
# data generation ---------------------------------------------------------

d = 1               # number of groups
K = 3               # number of global clusters
mu = c(-20,0, 5)   # vectors of means
sd = c(1,1,1)      # vector of sd
n_j = rep(200, d)  # set cardinality of the groups
seed = 124123
mycol_cluster = brewer.pal(n=K, name = "Dark2")

genD = generate_data(d=d, K=K, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition

# Run  --------------------------------------------------------------------


niter  <- 1000
burnin <- 1000
thin   <- 1

option<-set_options("nu" = 1, "Mstar0" = 2, "Lambda0" = 3, "mu0" = 0,"sigma0"= 1, "gamma0" = 1,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, "Adapt_MH_power_lim"=10, "Adapt_MH_var0"=1,
             "k0"= 1/10, "nu0"=10, "alpha_gamma"=1,
             "beta_gamma"=1, "alpha_lambda"=1, "beta_lambda"=1,
             "UpdateU" = T, "UpdateM" = T, "UpdateGamma" = T, "UpdateS" = T,
             "UpdateTau" = T, "UpdateLambda" = T
)

#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```

```{r, echo = F}
# Analisi output ----------------------------------------------------------

#K
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

#Mstar
plot(GDFMM$Mstar, type = 'l', main = "Mstar")


# Predictive --------------------------------------------------------------

l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
Pred_all = predictive_all_groups(grid = grid, fit = GDFMM)

for(j in 1:d){
print(
  tibble(value = data[j,]) %>%
    ggplot(aes(x=value)) +
    geom_histogram(aes(y = ..density..), color = 'black', fill = 'darkred', alpha = 0.3,
                   binwidth = 0.5) +
    geom_point(y = rep(0,length(data[j,])), col = 'darkred') +
    geom_path(data = as_tibble(t(Pred_all[[j]])), color = 'black', aes(x=grid,y=`50%`), size = 1.2 ) +
    geom_ribbon(data = as_tibble(t(Pred_all[[j]])), fill = mycol[1],
                aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
                alpha = 0.5) +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +  theme(plot.title = element_text(hjust = 0.5))  # set title in center and set the
)
}

```

```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION -------------------

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)

# Get estimated partition according to binder (or VI) loss functions.
# VI_dahl <- dlso(matr, loss = 'VI', estimate=NULL)
# binder_dahl <- dlso(part_matrix, loss = 'binder', estimate = sim_matrix)
# estimate_partition = as.vector(binder_dahl)
# 
# # Get quality indicies
# Kest = length(unique(estimate_partition))
# cat('\n Estimated number of clusters \n')
# Kest
# Binder_loss = salso::binder(truth = real_partition, estimate = estimate_partition)
# cat('\n Binder loss function \n')
# Binder_loss
# Tab = table(real_partition,estimate_partition)
# cat('\n Miss classification table \n')
# Tab

```


## Caso d = 2

**Fisso K = 4**
Caso NON simmetrico.



```{r, results='hide'}
suppressWarnings(suppressPackageStartupMessages(library(GDFMM)))
suppressWarnings(suppressPackageStartupMessages(library(ACutils)))
suppressWarnings(suppressPackageStartupMessages(library(tidyverse)))
suppressWarnings(suppressPackageStartupMessages(library(RColorBrewer)))

# color palette -----------------------------------------------------------
mycol = hcl.colors(n=3,palette = "Zissou1")
# data generation ---------------------------------------------------------

d = 2               # number of groups
K = 4               # number of global clusters
mu = c(-20,-10,0, 5)   # vectors of means
sd = c(1,1,1,1)      # vector of sd
n_j = rep(200, d)  # set cardinality of the groups
seed = 20051131
mycol_cluster = brewer.pal(n=K, name = "Dark2")

genD = generate_data(d=d, K=K, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition

# Run  --------------------------------------------------------------------


niter  <- 1000
burnin <- 1000
thin   <- 1

option<-set_options("nu" = 1, "Mstar0" = 2, "Lambda0" = 3, "mu0" = 0,"sigma0"= 1, "gamma0" = 1,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, "Adapt_MH_power_lim"=10, "Adapt_MH_var0"=1,
             "k0"= 1/10, "nu0"=10, "alpha_gamma"=1,
             "beta_gamma"=1, "alpha_lambda"=1, "beta_lambda"=1,
             "UpdateU" = T, "UpdateM" = T, "UpdateGamma" = T, "UpdateS" = T,
             "UpdateTau" = T, "UpdateLambda" = T
)

#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)
```


```{r, echo = F}
# Analisi output ----------------------------------------------------------

#K
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

#Mstar
plot(GDFMM$Mstar, type = 'l', main = "Mstar")


# Predictive --------------------------------------------------------------

l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
Pred_all = predictive_all_groups(grid = grid, fit = GDFMM)

# In questo plot sotto colori i pallini secondo il vero cluster.
# Assurdo ma gli istogrammi non funzionano pi√π, evidentemente raggruppa in modo diverso
idx_start = 1
idx_end = n_j[1]
for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(data = as_tibble(t(Pred_all[[j]])), color = 'black', aes(x=grid,y=`50%`), size = 1.2,
              inherit.aes = F) +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
    geom_ribbon(data = as_tibble(t(Pred_all[[j]])), fill = mycol[1],
                aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
                alpha = 0.5, inherit.aes = F)
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}


```


```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION -------------------

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)

# Get estimated partition according to binder (or VI) loss functions.
# VI_dahl <- dlso(matr, loss = 'VI', estimate=NULL)
# binder_dahl <- dlso(part_matrix, loss = 'binder', estimate = sim_matrix)
# estimate_partition = as.vector(binder_dahl)
# 
# # Get quality indicies
# Kest = length(unique(estimate_partition))
# cat('\n Estimated number of clusters \n')
# Kest
# Binder_loss = salso::binder(truth = real_partition, estimate = estimate_partition)
# cat('\n Binder loss function \n')
# Binder_loss
# Tab = table(real_partition,estimate_partition)
# cat('\n Miss classification table \n')
# Tab

```


## d=3

**K = 4**

Primo caso, default come gli altri.
```{r, results='hide'}
suppressWarnings(suppressPackageStartupMessages(library(GDFMM)))
suppressWarnings(suppressPackageStartupMessages(library(ACutils)))
suppressWarnings(suppressPackageStartupMessages(library(tidyverse)))
suppressWarnings(suppressPackageStartupMessages(library(RColorBrewer)))
suppressWarnings(suppressPackageStartupMessages(library(salso)))
suppressWarnings(suppressPackageStartupMessages(library(wesanderson)))



# color palette -----------------------------------------------------------
mycol = hcl.colors(n=3,palette = "Zissou1")

# data generation ---------------------------------------------------------

d = 3               # number of groups
K = 4               # number of global clusters
mu = c(-20,-10,0, 10)   # vectors of means
sd = c(1,1,1,1)      # vector of sd
n_j = rep(200, d)  # set cardinality of the groups
seed = 20051131

genD = generate_data(d=d, K=K, mu = mu, sd = sd, n_j = n_j, seed = seed)
data = genD$data
real_partition = genD$real_partition


mycol_cluster = brewer.pal(n=K, name = "Dark2")
# Run  --------------------------------------------------------------------


niter  <- 1000
burnin <- 1000
thin   <- 1

option<-set_options("nu" = 1, "Mstar0" = 2, "Lambda0" = 3, "mu0" = 0,"sigma0"= 1, "gamma0" = 1,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, "Adapt_MH_power_lim"=10, "Adapt_MH_var0"=1,
             "k0"= 1/10, "nu0"=10, "alpha_gamma"=1,
             "beta_gamma"=1, "alpha_lambda"=1, "beta_lambda"=1,
             "UpdateU" = T, "UpdateM" = T, "UpdateGamma" = T, "UpdateS" = T,
             "UpdateTau" = T, "UpdateLambda" = T
)

#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)

```

```{r, echo = F}
# Analisi output ----------------------------------------------------------

#K
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

#Mstar
plot(GDFMM$Mstar, type = 'l', main = "Mstar")


# Predictive --------------------------------------------------------------

l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
Pred_all = predictive_all_groups(grid = grid, fit = GDFMM)

# In questo plot sotto colori i pallini secondo il vero cluster.
# Assurdo ma gli istogrammi non funzionano pi√π, evidentemente raggruppa in modo diverso
idx_start = 1
idx_end = n_j[1]
for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(data = as_tibble(t(Pred_all[[j]])), color = 'black', aes(x=grid,y=`50%`), size = 1.2,
              inherit.aes = F) +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
    geom_ribbon(data = as_tibble(t(Pred_all[[j]])), fill = mycol[1],
                aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
                alpha = 0.5, inherit.aes = F)
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}


```



```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION -------------------

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)

# Get estimated partition according to binder (or VI) loss functions.
# VI_dahl <- dlso(matr, loss = 'VI', estimate=NULL)
# binder_dahl <- dlso(part_matrix, loss = 'binder', estimate = sim_matrix)
# estimate_partition = as.vector(binder_dahl)
# 
# # Get quality indicies
# Kest = length(unique(estimate_partition))
# cat('\n Estimated number of clusters \n')
# Kest
# Binder_loss = salso::binder(truth = real_partition, estimate = estimate_partition)
# cat('\n Binder loss function \n')
# Binder_loss
# Tab = table(real_partition,estimate_partition)
# cat('\n Miss classification table \n')
# Tab

```


Secondo caso, fisso `Mstar = 10`. Mixxa molto meglio.

```{r, results='hide'}
# Run  --------------------------------------------------------------------


niter  <- 1000
burnin <- 1000
thin   <- 1

option<-set_options("nu" = 1, "Mstar0" = 10, "Lambda0" = 3, "mu0" = 0,"sigma0"= 1, "gamma0" = 1,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, "Adapt_MH_power_lim"=10, "Adapt_MH_var0"=1,
             "k0"= 1/10, "nu0"=10, "alpha_gamma"=1,
             "beta_gamma"=1, "alpha_lambda"=1, "beta_lambda"=1,
             "UpdateU" = T, "UpdateM" = F, "UpdateGamma" = T, "UpdateS" = T,
             "UpdateTau" = T, "UpdateLambda" = T
)

#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)

```

```{r, echo = F}
# Analisi output ----------------------------------------------------------

#K
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

#Mstar
plot(GDFMM$Mstar, type = 'l', main = "Mstar")


# Predictive --------------------------------------------------------------

l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
Pred_all = predictive_all_groups(grid = grid, fit = GDFMM)

# In questo plot sotto colori i pallini secondo il vero cluster.
# Assurdo ma gli istogrammi non funzionano pi√π, evidentemente raggruppa in modo diverso
idx_start = 1
idx_end = n_j[1]
for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(data = as_tibble(t(Pred_all[[j]])), color = 'black', aes(x=grid,y=`50%`), size = 1.2,
              inherit.aes = F) +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
    geom_ribbon(data = as_tibble(t(Pred_all[[j]])), fill = mycol[1],
                aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
                alpha = 0.5, inherit.aes = F)
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}


```


```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION -------------------

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)

# Get estimated partition according to binder (or VI) loss functions.
# VI_dahl <- dlso(matr, loss = 'VI', estimate=NULL)
# binder_dahl <- dlso(part_matrix, loss = 'binder', estimate = sim_matrix)
# estimate_partition = as.vector(binder_dahl)
# 
# # Get quality indicies
# Kest = length(unique(estimate_partition))
# cat('\n Estimated number of clusters \n')
# Kest
# Binder_loss = salso::binder(truth = real_partition, estimate = estimate_partition)
# cat('\n Binder loss function \n')
# Binder_loss
# Tab = table(real_partition,estimate_partition)
# cat('\n Miss classification table \n')
# Tab

```


Terzo caso, `Mstar` random ma aumento prior su numbero di componenti. 

Visto che $\Lambda \sim Gamma(a_{\Lambda}, b_{\Lambda})$, metto $a_{\Lambda} = 15$.

```{r, results='hide'}
# Run  --------------------------------------------------------------------


niter  <- 1000
burnin <- 1000
thin   <- 1

option<-set_options("nu" = 1, "Mstar0" = 10, "Lambda0" = 3, "mu0" = 0,"sigma0"= 1, "gamma0" = 1,
             "Adapt_MH_hyp1"= 0.7,"Adapt_MH_hyp2"= 0.234, "Adapt_MH_power_lim"=10, "Adapt_MH_var0"=1,
             "k0"= 1/10, "nu0"=10, "alpha_gamma"=1,
             "beta_gamma"=1, "alpha_lambda"=15, "beta_lambda"=1,
             "UpdateU" = T, "UpdateM" = T, "UpdateGamma" = T, "UpdateS" = T,
             "UpdateTau" = T, "UpdateLambda" = T
)

#GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, option = option)
GDFMM = GDFMM_sampler(data, niter, burnin, thin, seed = 123, FixPartition = F, option = option)

```

```{r, echo = F}
# Analisi output ----------------------------------------------------------

#K
plot(GDFMM$K, type = 'l', main = "K")
hist(GDFMM$K, main = "K")

#Mstar
plot(GDFMM$Mstar, type = 'l', main = "Mstar")


# Predictive --------------------------------------------------------------

l_grid = 200
grid = seq(-25,25,length.out = l_grid)

# Predictive in all groups
Pred_all = predictive_all_groups(grid = grid, fit = GDFMM)

# In questo plot sotto colori i pallini secondo il vero cluster.
# Assurdo ma gli istogrammi non funzionano pi√π, evidentemente raggruppa in modo diverso
idx_start = 1
idx_end = n_j[1]
for(j in 1:d){

  print(
  tibble(value = data[j,],
         true_clus = as.factor(real_partition[idx_start:idx_end])
         ) %>%
    ggplot(aes(x=value, col = true_clus, fill = true_clus)) +
    geom_point(y = rep(0.005,length(data[j,])), size = 2) +
    geom_histogram(data = tibble(value = data[j,]),
                   aes(x=value, y = ..density..), 
                   color = 'black', alpha = 0.3,
                   binwidth = 0.5, inherit.aes = F) +
    scale_color_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    scale_fill_manual(values = mycol_cluster[ sort(unique(real_partition[idx_start:idx_end])) ] ) +
    geom_path(data = as_tibble(t(Pred_all[[j]])), color = 'black', aes(x=grid,y=`50%`), size = 1.2,
              inherit.aes = F) +
    labs(title=paste0("level = ",j), x = "x-axis", y = "y-axis") + theme_bw() +
    theme(plot.title = element_text(hjust = 0.5)) + # set title in center and set the
    geom_ribbon(data = as_tibble(t(Pred_all[[j]])), fill = mycol[1],
                aes(x = grid, ymin=`2.5%`, ymax=`97.5%`, y=`50%`, fill = "band"),
                alpha = 0.5, inherit.aes = F)
  )

  idx_start = idx_end + 1
  idx_end = idx_end + n_j[j]
}


```


```{r}
# COMPUTE BINDER LOSS FUNCTION TO SELECT BEST PARTITION -------------------

# Get labels for each iterations for each data point 
part_matrix <- GDFMM$Partition

# Compute similarity matrix
sim_matrix <- psm(part_matrix)


heatmap(sim_matrix)


binder_sara = minbinder(sim_matrix)
VI_sara = minVI(sim_matrix)

table(binder_sara$cl)
table(VI_sara$cl)
table(real_partition)

# Get estimated partition according to binder (or VI) loss functions.
# VI_dahl <- dlso(matr, loss = 'VI', estimate=NULL)
# binder_dahl <- dlso(part_matrix, loss = 'binder', estimate = sim_matrix)
# estimate_partition = as.vector(binder_dahl)
# 
# # Get quality indicies
# Kest = length(unique(estimate_partition))
# cat('\n Estimated number of clusters \n')
# Kest
# Binder_loss = salso::binder(truth = real_partition, estimate = estimate_partition)
# cat('\n Binder loss function \n')
# Binder_loss
# Tab = table(real_partition,estimate_partition)
# cat('\n Miss classification table \n')
# Tab

```


